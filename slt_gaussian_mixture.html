<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.27">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Illustrating Watanabe’s Singular Learning Theory with a Gaussian Mixture Model – Watanabe's Singular Learning Theory</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-ed96de9b727972fe78a7b5d16c58bf87.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-690d2c05533395d17c836f89f1947714.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Watanabe’s Singular Learning Theory</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="./index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link active" href="./slt_gaussian_mixture.html" aria-current="page"> 
<span class="menu-text">SLT Gaussian Mixture</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./regular_and_singular_models.html"> 
<span class="menu-text">Regular and Singular Models</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./cheat_sheet.html"> 
<span class="menu-text">Cheat Sheet</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction-the-model-and-the-true-distribution-chapter-1" id="toc-introduction-the-model-and-the-true-distribution-chapter-1" class="nav-link active" data-scroll-target="#introduction-the-model-and-the-true-distribution-chapter-1">1. Introduction: The Model and the True Distribution (Chapter 1)</a>
  <ul class="collapse">
  <li><a href="#the-two-component-gaussian-mixture" id="toc-the-two-component-gaussian-mixture" class="nav-link" data-scroll-target="#the-two-component-gaussian-mixture">The Two-Component Gaussian Mixture</a></li>
  <li><a href="#the-set-of-true-parameters" id="toc-the-set-of-true-parameters" class="nav-link" data-scroll-target="#the-set-of-true-parameters">The Set of True Parameters</a></li>
  <li><a href="#the-kullback-leibler-divergence" id="toc-the-kullback-leibler-divergence" class="nav-link" data-scroll-target="#the-kullback-leibler-divergence">The Kullback-Leibler Divergence</a></li>
  </ul></li>
  <li><a href="#resolution-of-singularities-chapter-2" id="toc-resolution-of-singularities-chapter-2" class="nav-link" data-scroll-target="#resolution-of-singularities-chapter-2">2. Resolution of Singularities (Chapter 2)</a>
  <ul class="collapse">
  <li><a href="#the-blow-up-transformation" id="toc-the-blow-up-transformation" class="nav-link" data-scroll-target="#the-blow-up-transformation">The Blow-Up Transformation</a></li>
  </ul></li>
  <li><a href="#standard-form-and-real-log-canonical-threshold-chapter-3" id="toc-standard-form-and-real-log-canonical-threshold-chapter-3" class="nav-link" data-scroll-target="#standard-form-and-real-log-canonical-threshold-chapter-3">3. Standard Form and Real Log Canonical Threshold (Chapter 3)</a></li>
  <li><a href="#singular-fluctuation-and-free-energy-chapter-4" id="toc-singular-fluctuation-and-free-energy-chapter-4" class="nav-link" data-scroll-target="#singular-fluctuation-and-free-energy-chapter-4">4. Singular Fluctuation and Free Energy (Chapter 4)</a></li>
  <li><a href="#generalization-and-training-errors-chapter-5" id="toc-generalization-and-training-errors-chapter-5" class="nav-link" data-scroll-target="#generalization-and-training-errors-chapter-5">5. Generalization and Training Errors (Chapter 5)</a></li>
  <li><a href="#asymptotic-expansion-and-waic-chapter-6" id="toc-asymptotic-expansion-and-waic-chapter-6" class="nav-link" data-scroll-target="#asymptotic-expansion-and-waic-chapter-6">6. Asymptotic Expansion and WAIC (Chapter 6)</a>
  <ul class="collapse">
  <li><a href="#calculating-waic-in-practice" id="toc-calculating-waic-in-practice" class="nav-link" data-scroll-target="#calculating-waic-in-practice">Calculating WAIC in Practice</a></li>
  <li><a href="#visualizing-the-posterior" id="toc-visualizing-the-posterior" class="nav-link" data-scroll-target="#visualizing-the-posterior">Visualizing the Posterior</a></li>
  <li><a href="#contrast-asymptotic-regularity-vs.-finite-sample-singularity" id="toc-contrast-asymptotic-regularity-vs.-finite-sample-singularity" class="nav-link" data-scroll-target="#contrast-asymptotic-regularity-vs.-finite-sample-singularity">Contrast: Asymptotic Regularity vs.&nbsp;Finite-Sample Singularity</a></li>
  <li><a href="#is-waic-still-good-for-small-samples" id="toc-is-waic-still-good-for-small-samples" class="nav-link" data-scroll-target="#is-waic-still-good-for-small-samples">Is WAIC Still Good for Small Samples?</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Illustrating Watanabe’s Singular Learning Theory with a Gaussian Mixture Model</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>This document provides a walk-through of Sumio Watanabe’s <em>Singular Learning Theory</em> (up to Chapter 6), using a two-component Gaussian Mixture Model (GMM) as a concrete pedagogical example.</p>
<section id="introduction-the-model-and-the-true-distribution-chapter-1" class="level2">
<h2 class="anchored" data-anchor-id="introduction-the-model-and-the-true-distribution-chapter-1">1. Introduction: The Model and the True Distribution (Chapter 1)</h2>
<p>In statistical formulation, we consider a learning machine defined by a parametric statistical model <span class="math inline">\(p(x|w)\)</span> and a true data-generating distribution <span class="math inline">\(q(x)\)</span>.</p>
<section id="the-two-component-gaussian-mixture" class="level3">
<h3 class="anchored" data-anchor-id="the-two-component-gaussian-mixture">The Two-Component Gaussian Mixture</h3>
<p>Let’s define a simple 2-parameter Gaussian Mixture Model where one component is fixed at the origin: <span class="math display">\[ p(x|w) = (1-a) \mathcal{N}(x|0, 1) + a \mathcal{N}(x|\mu, 1) \]</span> Here, the parameter vector is <span class="math inline">\(w = (a, \mu) \in W = [0, 1] \times [-c, c]\)</span>.</p>
<p>Suppose the <strong>true distribution</strong> is simply a standard normal distribution: <span class="math display">\[ q(x) = \mathcal{N}(x|0, 1) \]</span></p>
<div id="cell-fig-distributions" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">400</span>)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>q <span class="op">=</span> np.exp(<span class="op">-</span>x<span class="op">**</span><span class="dv">2</span><span class="op">/</span><span class="dv">2</span>)<span class="op">/</span>np.sqrt(<span class="dv">2</span><span class="op">*</span>np.pi)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> <span class="fl">0.7</span> <span class="op">*</span> np.exp(<span class="op">-</span>x<span class="op">**</span><span class="dv">2</span><span class="op">/</span><span class="dv">2</span>)<span class="op">/</span>np.sqrt(<span class="dv">2</span><span class="op">*</span>np.pi) <span class="op">+</span> <span class="fl">0.3</span> <span class="op">*</span> np.exp(<span class="op">-</span>(x<span class="op">-</span><span class="fl">1.5</span>)<span class="op">**</span><span class="dv">2</span><span class="op">/</span><span class="dv">2</span>)<span class="op">/</span>np.sqrt(<span class="dv">2</span><span class="op">*</span>np.pi)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">5</span>))</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>plt.plot(x, q, label<span class="op">=</span><span class="vs">r'True Distribution </span><span class="dv">$</span><span class="vs">q</span><span class="kw">(</span><span class="vs">x</span><span class="kw">)</span><span class="vs"> = </span><span class="dv">\m</span><span class="vs">athcal{N}</span><span class="kw">(</span><span class="vs">0,1</span><span class="kw">)</span><span class="dv">$</span><span class="vs">'</span>, color<span class="op">=</span><span class="st">'blue'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>plt.plot(x, p, label<span class="op">=</span><span class="vs">r'Model </span><span class="dv">$</span><span class="vs">p</span><span class="kw">(</span><span class="vs">x</span><span class="cf">|</span><span class="vs">w</span><span class="kw">)</span><span class="dv">$</span><span class="vs"> with </span><span class="dv">$</span><span class="vs">a=0</span><span class="dv">.</span><span class="vs">3, </span><span class="dv">\m</span><span class="vs">u=1</span><span class="dv">.</span><span class="vs">5</span><span class="dv">$</span><span class="vs">'</span>, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Gaussian Mixture Model vs True Distribution"</span>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"x"</span>)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Density"</span>)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-distributions" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-distributions-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="slt_gaussian_mixture_files/figure-html/fig-distributions-output-1.png" width="758" height="470" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-distributions-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: True Distribution vs Model
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="the-set-of-true-parameters" class="level3">
<h3 class="anchored" data-anchor-id="the-set-of-true-parameters">The Set of True Parameters</h3>
<p>The true distribution is realized by the model whenever <span class="math inline">\(p(x|w) = q(x)\)</span>. By inspecting the equation: <span class="math display">\[ (1-a) \mathcal{N}(x|0, 1) + a \mathcal{N}(x|\mu, 1) = \mathcal{N}(x|0, 1) \]</span> <span class="math display">\[ a (\mathcal{N}(x|\mu, 1) - \mathcal{N}(x|0, 1)) = 0 \]</span></p>
<p>This holds true if and only if <strong><span class="math inline">\(a = 0\)</span></strong> (the mixing proportion is zero) OR <strong><span class="math inline">\(\mu = 0\)</span></strong> (both components are identical). Thus, the set of true parameters <span class="math inline">\(W_0\)</span> is: <span class="math display">\[ W_0 = \{ (a, \mu) \in W : a = 0 \text{ or } \mu = 0 \} \]</span></p>
<p>This means that <span class="math inline">\(W_0\)</span> is not a single point, but the <strong>union of two intersecting lines</strong>. In classical (regular) statistical theory, <span class="math inline">\(W_0\)</span> is assumed to be a single point, and the Fisher Information Matrix is positive definite. Here, the Fisher Information Matrix degenerates on <span class="math inline">\(W_0\)</span>, making this a <strong>singular model</strong>.</p>
</section>
<section id="the-kullback-leibler-divergence" class="level3">
<h3 class="anchored" data-anchor-id="the-kullback-leibler-divergence">The Kullback-Leibler Divergence</h3>
<p>The log-likelihood ratio (empirical loss) is driven by the Kullback-Leibler (KL) divergence from <span class="math inline">\(q(x)\)</span> to <span class="math inline">\(p(x|w)\)</span>: <span class="math display">\[ K(w) = \int q(x) \log \frac{q(x)}{p(x|w)} dx \]</span></p>
<p>Using a Taylor expansion for small <span class="math inline">\(a\)</span> and <span class="math inline">\(\mu\)</span>, we can approximate <span class="math inline">\(p(x|w)\)</span>: <span class="math display">\[ p(x|w) = \mathcal{N}(x|0, 1) \left[ 1 + a (e^{\mu x - \mu^2/2} - 1) \right] \approx \mathcal{N}(x|0, 1) \left[ 1 + a\left(\mu x + \frac{1}{2}\mu^2(x^2 - 1)\right) \right] \]</span></p>
<p>Plugging this into the KL divergence and using <span class="math inline">\(-\log(1+z) \approx -z + z^2/2\)</span>, the linear terms integrate to 0 under <span class="math inline">\(q(x)\)</span>, leaving the leading non-zero term: <span class="math display">\[ K(w) \approx \frac{1}{2} a^2 \mu^2 \]</span></p>
<div id="cell-fig-kl-divergence" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">400</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>M <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">400</span>)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>AA, MM <span class="op">=</span> np.meshgrid(A, M)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>K <span class="op">=</span> <span class="fl">0.5</span> <span class="op">*</span> (AA<span class="op">**</span><span class="dv">2</span>) <span class="op">*</span> (MM<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>contour <span class="op">=</span> plt.contourf(AA, MM, K, levels<span class="op">=</span><span class="dv">20</span>, cmap<span class="op">=</span><span class="st">'viridis_r'</span>)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>plt.colorbar(contour, label<span class="op">=</span><span class="vs">r'KL Divergence </span><span class="dv">$</span><span class="vs">K</span><span class="kw">(</span><span class="vs">a, </span><span class="dv">\m</span><span class="vs">u</span><span class="kw">)</span><span class="dv">$</span><span class="vs">'</span>)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>plt.axhline(<span class="dv">0</span>, color<span class="op">=</span><span class="st">'red'</span>, linewidth<span class="op">=</span><span class="dv">3</span>, label<span class="op">=</span><span class="st">'True Parameters ($a=0$)'</span>)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>plt.axvline(<span class="dv">0</span>, color<span class="op">=</span><span class="st">'red'</span>, linewidth<span class="op">=</span><span class="dv">3</span>, label<span class="op">=</span><span class="vs">r'True Parameters </span><span class="kw">(</span><span class="dv">$\m</span><span class="vs">u=0</span><span class="dv">$</span><span class="kw">)</span><span class="vs">'</span>)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Parameter Space and KL Divergence"</span>)</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Parameter $a$"</span>)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r"Parameter </span><span class="dv">$\m</span><span class="vs">u</span><span class="dv">$</span><span class="vs">"</span>)</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-kl-divergence" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-kl-divergence-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="slt_gaussian_mixture_files/figure-html/fig-kl-divergence-output-1.png" width="731" height="566" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-kl-divergence-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Parameter Space and KL Divergence. Notice how the valley of <span class="math inline">\(K(w)=0\)</span> forms a cross at <span class="math inline">\(a=0\)</span> and <span class="math inline">\(\mu=0\)</span>.
</figcaption>
</figure>
</div>
</div>
</div>
<hr>
</section>
</section>
<section id="resolution-of-singularities-chapter-2" class="level2">
<h2 class="anchored" data-anchor-id="resolution-of-singularities-chapter-2">2. Resolution of Singularities (Chapter 2)</h2>
<p>Because the set of true parameters <span class="math inline">\(W_0\)</span> has a singularity (an intersection forming a cross), standard asymptotic expansions (like the Laplace approximation) fail. Watanabe employs <strong>Hironaka’s Theorem on the Resolution of Singularities</strong> from algebraic geometry to resolve this.</p>
<p>The theorem states that there exists a real analytic manifold <span class="math inline">\(\mathcal{M}\)</span> and a proper analytic map <span class="math inline">\(g: \mathcal{M} \to W\)</span> (a “blow-up”) such that the composition <span class="math inline">\(K(g(u))\)</span> has a simple normal crossing form.</p>
<section id="the-blow-up-transformation" class="level3">
<h3 class="anchored" data-anchor-id="the-blow-up-transformation">The Blow-Up Transformation</h3>
<p>For the approximation <span class="math inline">\(K(w) \approx \frac{1}{2} a^2 \mu^2\)</span>, the true parameters <span class="math inline">\(W_0\)</span> correspond to the crossing lines <span class="math inline">\(a=0\)</span> and <span class="math inline">\(\mu=0\)</span>. To resolve this intersection, we apply a “blow-up” transformation. A blow-up geometrically replaces the problematic intersection point (the origin) with an entire line (called the exceptional divisor), separating the paths that cross there.</p>
<p>We can reparameterize the space by keeping track of the <em>slope</em> of lines passing through the origin. We define a local coordinate chart (a directional blow-up) as: <span class="math display">\[ a = u_1 \]</span> <span class="math display">\[ \mu = u_1 u_2 \]</span></p>
<p>Here, <span class="math inline">\(u_1\)</span> simply represents the original <span class="math inline">\(a\)</span> coordinate, while <span class="math inline">\(u_2 = \mu / a\)</span> represents the slope of a line passing through the origin in the <span class="math inline">\((a, \mu)\)</span> parameter space.</p>
<ul>
<li>A single point in the original space—the origin <span class="math inline">\((a=0, \mu=0)\)</span>—now corresponds to the entire line <span class="math inline">\(u_1 = 0\)</span> for any value of <span class="math inline">\(u_2\)</span> in the new space.</li>
<li>The crossing lines in the <span class="math inline">\((a, \mu)\)</span> space have been pulled apart. Actually, our new coordinate system does not extend to the line <span class="math inline">\(a=0\)</span>, but we can get arbitrarily close to it. We would need another patch with coordinates <span class="math inline">\(\mu, a/\mu\)</span> to represent the line <span class="math inline">\(a=0\)</span>.</li>
</ul>
<p>The KL divergence in these new coordinates <span class="math inline">\(u = (u_1, u_2)\)</span> is <span class="math display">\[ K(g(u)) \approx \frac{1}{2} (u_1)^2 (u_1 u_2)^2 = \frac{1}{2} u_1^4 u_2^2 \]</span></p>
<p>We must also account for the distortion of the volume measure, dictated by the Jacobian of <span class="math inline">\(g\)</span>: <span class="math display">\[ dw = |g'(u)| du = \left| \det \begin{pmatrix} 1 &amp; 0 \\ u_2 &amp; u_1 \end{pmatrix} \right| du_1 du_2 = |u_1| du_1 du_2 \]</span></p>
<hr>
</section>
</section>
<section id="standard-form-and-real-log-canonical-threshold-chapter-3" class="level2">
<h2 class="anchored" data-anchor-id="standard-form-and-real-log-canonical-threshold-chapter-3">3. Standard Form and Real Log Canonical Threshold (Chapter 3)</h2>
<p>In Chapter 3, Watanabe introduces the concept of the <strong>Real Log Canonical Threshold (RLCT)</strong>, denoted by <span class="math inline">\(\lambda\)</span>, and its multiplicity <span class="math inline">\(m\)</span>. These two algebraic invariants completely govern the asymptotic behavior of the learning machine.</p>
<p>To understand why the blow-up was a strictly necessary algebraic maneuver, we have to look at how <span class="math inline">\(\lambda\)</span> is formally calculated. By definition, the RLCT is found by examining the analytic continuation of the <strong>zeta function</strong> of the statistical model, given by the integral: <span class="math display">\[ \zeta(z) = \int (K(w))^z \varphi(w) dw \]</span> where <span class="math inline">\(\varphi(w)\)</span> is the prior distribution and <span class="math inline">\(z \in \mathbb{C}\)</span> (<span class="math inline">\(\Re(z) &gt; 0\)</span>). The RLCT <span class="math inline">\(\lambda\)</span> is defined such that <span class="math inline">\(-\lambda\)</span> is the largest (closest to zero) real pole of this function, and its multiplicity <span class="math inline">\(m\)</span> is the order of this pole.</p>
<p>Without the blow-up, evaluating this integral and finding its poles is mathematically intractable. The true KL divergence <span class="math inline">\(K(w)\)</span> isn’t just a simple polynomial like <span class="math inline">\(a^2\mu^2\)</span>; it contains an infinite series of higher-order terms from the Taylor expansion. Because the variables are coupled in a highly non-linear way at the singularity (the cross <span class="math inline">\(a=0, \mu=0\)</span>), you cannot separate the variables to evaluate the integral.</p>
<p><strong>This is where the blow-up of the singularity resolves the integration problem.</strong> Hironaka’s Theorem guarantees that after passing to the resolved coordinates <span class="math inline">\(u\)</span>, the fully complex divergence <span class="math inline">\(K(g(u))\)</span> perfectly factors into a single monomial multiplied by a non-vanishing positive analytic function <span class="math inline">\(b(u) &gt; 0\)</span>. The prior measure and Jacobian of the blow-up also become a simple monomial multiplied by a strictly positive function <span class="math inline">\(c(u) &gt; 0\)</span>.</p>
<p>When we substitute this <strong>Standard Form</strong> into the zeta function integral using our resolved coordinates <span class="math inline">\(u\)</span>, the variables completely decouple near the origin: <span class="math display">\[ \zeta(z) = \int \left( u_1^{2k_{1}} u_2^{2k_{2}} b(u) \right)^z \left( u_1^{h_1} u_2^{h_2} c(u) \right) du_1 du_2 \]</span> <span class="math display">\[ \zeta(z) \approx C \left( \int u_1^{2k_1 z + h_1} du_1 \right) \left( \int u_2^{2k_2 z + h_2} du_2 \right) \]</span></p>
<p>Evaluating these independent 1D integrals yields formulas of the type: <span class="math display">\[ \int_0^\epsilon u^{2k z + h} du = \frac{\epsilon^{2k z + h + 1}}{2k z + h + 1} \]</span> This expression clearly has a pole exactly when the denominator is zero, i.e., at <span class="math inline">\(z = -\frac{h + 1}{2k}\)</span>.</p>
<p>By separating the variables, the blow-up perfectly isolates the poles of the zeta function!</p>
<p>Using our specific factors from the GMM blow-up: 1. Divergence function: <span class="math inline">\(K(u) = u_1^4 u_2^2\)</span> (so <span class="math inline">\(k_1 = 2\)</span>, <span class="math inline">\(k_2 = 1\)</span>) 2. Prior measure / Jacobian: <span class="math inline">\(\Phi(u) = u_1^1 u_2^0\)</span> (so <span class="math inline">\(h_1 = 1\)</span>, <span class="math inline">\(h_2 = 0\)</span>)</p>
<p>The candidate poles <span class="math inline">\(z = -\lambda_j\)</span> along each coordinate axis <span class="math inline">\(j\)</span> give us: <span class="math display">\[ \lambda_j = \frac{h_j + 1}{2k_j} \]</span></p>
<p>For our <span class="math inline">\(u_1\)</span> and <span class="math inline">\(u_2\)</span>: - <span class="math inline">\(\lambda_1 = \frac{1 + 1}{4} = \frac{1}{2}\)</span> - <span class="math inline">\(\lambda_2 = \frac{0 + 1}{2} = \frac{1}{2}\)</span></p>
<p>The overall RLCT <span class="math inline">\(\lambda\)</span> is dictated by the pole closest to zero, which is the minimum of these values: <span class="math display">\[ \lambda = \min(\lambda_1, \lambda_2) = \min(1/2, 1/2) = \frac{1}{2} \]</span></p>
<p>The <strong>multiplicity</strong> <span class="math inline">\(m\)</span> is the order of this leading pole, which equals the number of coordinate indices <span class="math inline">\(j\)</span> that achieve this minimum. Here, both <span class="math inline">\(\lambda_1\)</span> and <span class="math inline">\(\lambda_2\)</span> equal <span class="math inline">\(1/2\)</span>. Thus: <span class="math display">\[ m = 2 \]</span></p>
<p><em>Note: In a regular model with parameter dimension <span class="math inline">\(d=2\)</span>, the RLCT is always <span class="math inline">\(\lambda = d/2 = 1\)</span>. Our singular GMM has <span class="math inline">\(\lambda = 1/2 &lt; 1\)</span>, showcasing the mathematical definition of a singular learning machine.</em></p>
<hr>
</section>
<section id="singular-fluctuation-and-free-energy-chapter-4" class="level2">
<h2 class="anchored" data-anchor-id="singular-fluctuation-and-free-energy-chapter-4">4. Singular Fluctuation and Free Energy (Chapter 4)</h2>
<p>In Bayesian evaluation, the <strong>Stochastic Complexity</strong> or <strong>Free Energy</strong> <span class="math inline">\(F_n\)</span> represents the negative log-marginal likelihood (evidence) of the data <span class="math inline">\(X^n\)</span>: <span class="math display">\[ F_n = -\log Z_n = -\log \int e^{-n L_n(w)} \varphi(w) dw \]</span> where <span class="math inline">\(L_n(w) = -\frac{1}{n} \sum \log p(X_i|w)\)</span> is the empirical loss.</p>
<p>Chapter 4 defines how the parameter posterior behaves under singular fluctuations. Using the algebraic invariants we just found, SLT proves that the free energy asymptotically expands as:</p>
<p><span class="math display">\[ F_n \approx n L_n(w_0) + \lambda \log n - (m-1) \log \log n + O_p(1) \]</span></p>
<p>For our two-component Gaussian Mixture Model: <span class="math display">\[ F_n \approx n L_n(w_0) + \frac{1}{2} \log n - \log \log n + O_p(1) \]</span></p>
<div id="cell-fig-free-energy" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> np.logspace(<span class="dv">1</span>, <span class="dv">4</span>, <span class="dv">100</span>)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>F_reg <span class="op">=</span> <span class="fl">1.0</span> <span class="op">*</span> np.log(n)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>F_sing <span class="op">=</span> <span class="fl">0.5</span> <span class="op">*</span> np.log(n) <span class="op">-</span> <span class="fl">1.0</span> <span class="op">*</span> np.log(np.log(n <span class="op">+</span> <span class="fl">1.1</span>))</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">5</span>))</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>plt.plot(n, F_reg, label<span class="op">=</span><span class="vs">r'Regular Model </span><span class="kw">(</span><span class="dv">$</span><span class="vs">d=2 </span><span class="er">\</span><span class="vs">Rightarrow </span><span class="er">\</span><span class="vs">lambda=1</span><span class="dv">$</span><span class="kw">)</span><span class="vs">'</span>, color<span class="op">=</span><span class="st">'red'</span>)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>plt.plot(n, F_sing, label<span class="op">=</span><span class="vs">r'Singular Model </span><span class="kw">(</span><span class="vs">GMM, </span><span class="dv">$</span><span class="er">\</span><span class="vs">lambda=0</span><span class="dv">.</span><span class="vs">5, m=2</span><span class="dv">$</span><span class="kw">)</span><span class="vs">'</span>, color<span class="op">=</span><span class="st">'blue'</span>)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Asymptotic Free Energy Penalty vs Sample Size"</span>)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Sample Size $n$ (log scale)"</span>)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Penalty Term"</span>)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>plt.xscale(<span class="st">'log'</span>)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-free-energy" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-free-energy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="slt_gaussian_mixture_files/figure-html/fig-free-energy-output-1.png" width="758" height="466" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-free-energy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Asymptotic Free Energy. The penalty term for the singular model is significantly smaller than for a regular model, making singular models heavily preferred by the marginal likelihood.
</figcaption>
</figure>
</div>
</div>
</div>
<p>This result breaks the classical Bayesian Information Criterion (BIC), which assumes a penalty of <span class="math inline">\(\frac{d}{2} \log n\)</span>. BIC would have penalized this 2-parameter model by <span class="math inline">\(1.0 \log n\)</span>, entirely missing the true Bayesian Occam’s Razor effect on the singular manifold.</p>
<hr>
</section>
<section id="generalization-and-training-errors-chapter-5" class="level2">
<h2 class="anchored" data-anchor-id="generalization-and-training-errors-chapter-5">5. Generalization and Training Errors (Chapter 5)</h2>
<p>Chapter 5 links the geometry of the parameter space to the average errors. Let <span class="math inline">\(G_n\)</span> be the <strong>Generalization Error</strong> (expected loss on new data) and <span class="math inline">\(T_n\)</span> be the <strong>Training Error</strong> (empirical loss on the training set).</p>
<p>In traditional regular statistics (like AIC), the expectation of these errors relies on <span class="math inline">\(d\)</span>: <span class="math display">\[ \mathbb{E}[G_n] = L(w_0) + \frac{d}{2n} \]</span> <span class="math display">\[ \mathbb{E}[T_n] = L(w_0) - \frac{d}{2n} \]</span></p>
<p>In Singular Learning Theory, Watanabe elegantly proves a symmetry relation using <span class="math inline">\(\lambda\)</span>: <span class="math display">\[ \mathbb{E}[G_n] = L(w_0) + \frac{\lambda}{n} \]</span> <span class="math display">\[ \mathbb{E}[T_n] = L(w_0) - \frac{\lambda}{n} \]</span></p>
<p>For our continuous GMM, <span class="math inline">\(\lambda = 1/2\)</span>. Therefore: - Expected Generalization Error converges with rate <span class="math inline">\(\frac{0.5}{n}\)</span>. - Expected Training Error converges with rate <span class="math inline">\(-\frac{0.5}{n}\)</span>.</p>
<p>This explains why heavily overparameterized singular models (like deep neural networks and complex mixtures) often generalize better than their parameter count <span class="math inline">\(d\)</span> would imply; their learning dynamics are constrained by the smaller geometric invariant <span class="math inline">\(\lambda &lt; d/2\)</span>.</p>
<hr>
</section>
<section id="asymptotic-expansion-and-waic-chapter-6" class="level2">
<h2 class="anchored" data-anchor-id="asymptotic-expansion-and-waic-chapter-6">6. Asymptotic Expansion and WAIC (Chapter 6)</h2>
<p>Finally, Chapter 6 resolves a crucial practical problem. Since calculating <span class="math inline">\(\lambda\)</span> analytically requires algebraic blow-ups (which is impossible for massive modern models like LLMs), we cannot use it directly to estimate the generalization error. Furthermore, cross-validation can be unstable in singular models.</p>
<p>Watanabe introduces the <strong>Widely Applicable Information Criterion (WAIC)</strong>: <span class="math display">\[ \text{WAIC} = T_n + \frac{1}{n} \sum_{i=1}^n V_w \left( \log p(X_i | w) \right) \]</span> Where <span class="math inline">\(V_w\)</span> is the posterior variance of the log-likelihood for data point <span class="math inline">\(X_i\)</span>.</p>
<p>The core theorem of Chapter 6 proves that WAIC is an asymptotically unbiased estimator of the generalization error, <strong>even for singular models</strong>: <span class="math display">\[ \mathbb{E}[\text{WAIC}] \approx \mathbb{E}[G_n] \]</span></p>
<p>In the context of our GMM, evaluating the posterior variance computationally via Markov Chain Monte Carlo (MCMC) allows us to estimate the true hold-out performance without needing to analytically find <span class="math inline">\(\lambda = 1/2\)</span> or <span class="math inline">\(m=2\)</span>, proving WAIC’s universal applicability in modern deep learning and mixture modeling.</p>
<section id="calculating-waic-in-practice" class="level3">
<h3 class="anchored" data-anchor-id="calculating-waic-in-practice">Calculating WAIC in Practice</h3>
<p>To make this concrete, let’s generate a synthetic dataset from the true distribution <span class="math inline">\(q(x) = \mathcal{N}(0, 1)\)</span> and compute the WAIC for our toy GMM. By using a fine grid covering our parameter space <span class="math inline">\(W = [0, 1] \times [-2, 2]\)</span>, we can exactly compute the posterior and WAIC without relying on intricate MCMC setups.</p>
<div id="waic-calculation" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Generate synthetic data from true distribution q(x) = N(0,1)</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="dv">1</span>, n)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Define parameter grid for evaluating the posterior</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>a_vals <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">100</span>)</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>mu_vals <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">100</span>)</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>A, MU <span class="op">=</span> np.meshgrid(a_vals, mu_vals)</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="co"># p(x|a, mu) = (1-a)*N(x|0,1) + a*N(x|mu,1)</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> norm_pdf(x, m, s):</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.exp(<span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> ((x <span class="op">-</span> m) <span class="op">/</span> s)<span class="op">**</span><span class="dv">2</span>) <span class="op">/</span> (np.sqrt(<span class="dv">2</span> <span class="op">*</span> np.pi) <span class="op">*</span> s)</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Compute log-likelihood for each parameter pair and each data point</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>log_p <span class="op">=</span> np.zeros((n, <span class="bu">len</span>(mu_vals), <span class="bu">len</span>(a_vals)))</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, x <span class="kw">in</span> <span class="bu">enumerate</span>(X):</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>    p_x_w <span class="op">=</span> (<span class="dv">1</span> <span class="op">-</span> A) <span class="op">*</span> norm_pdf(x, <span class="dv">0</span>, <span class="dv">1</span>) <span class="op">+</span> A <span class="op">*</span> norm_pdf(x, MU, <span class="dv">1</span>)</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add a small epsilon to prevent log(0)</span></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>    log_p[i] <span class="op">=</span> np.log(p_x_w <span class="op">+</span> <span class="fl">1e-12</span>)</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. Compute unnormalized posterior probabilities (assuming uniform prior)</span></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Sum over all data points to get the full log-likelihood for each parameter pair</span></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>total_log_likelihood <span class="op">=</span> np.<span class="bu">sum</span>(log_p, axis<span class="op">=</span><span class="dv">0</span>) </span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Subtract maximum for numerical stability before exponentiating</span></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>log_posterior <span class="op">=</span> total_log_likelihood <span class="op">-</span> np.<span class="bu">max</span>(total_log_likelihood)</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>posterior <span class="op">=</span> np.exp(log_posterior)</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Normalize to create a valid probability distribution over the grid</span></span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>posterior <span class="op">/=</span> np.<span class="bu">sum</span>(posterior) </span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a><span class="co"># 5. Calculate WAIC</span></span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>predictive_density <span class="op">=</span> np.zeros(n)</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>V_w <span class="op">=</span> np.zeros(n)</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Expected volume: E_w[p(X_i|w)]</span></span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a>    predictive_density[i] <span class="op">=</span> np.<span class="bu">sum</span>(np.exp(log_p[i]) <span class="op">*</span> posterior)</span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Posterior variance of the log-likelihood: V_w(log p(X_i|w))</span></span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a>    expected_log_p <span class="op">=</span> np.<span class="bu">sum</span>(log_p[i] <span class="op">*</span> posterior)</span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a>    expected_log_p_sq <span class="op">=</span> np.<span class="bu">sum</span>((log_p[i]<span class="op">**</span><span class="dv">2</span>) <span class="op">*</span> posterior)</span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a>    V_w[i] <span class="op">=</span> expected_log_p_sq <span class="op">-</span> expected_log_p<span class="op">**</span><span class="dv">2</span></span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a><span class="co"># T_n: Empirical training loss of the Bayes predictive distribution</span></span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a>T_n <span class="op">=</span> <span class="op">-</span><span class="fl">1.0</span> <span class="op">/</span> n <span class="op">*</span> np.<span class="bu">sum</span>(np.log(predictive_density <span class="op">+</span> <span class="fl">1e-12</span>))</span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a><span class="co"># Functional variance penalty term</span></span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a><span class="co"># This corresponds to (1/n) * sum(V_w)</span></span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a>penalty <span class="op">=</span> np.mean(V_w)</span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-53"><a href="#cb4-53" aria-hidden="true" tabindex="-1"></a>waic <span class="op">=</span> T_n <span class="op">+</span> penalty</span>
<span id="cb4-54"><a href="#cb4-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-55"><a href="#cb4-55" aria-hidden="true" tabindex="-1"></a><span class="co"># 6. Calculate True Generalization Error on massive holdout</span></span>
<span id="cb4-56"><a href="#cb4-56" aria-hidden="true" tabindex="-1"></a>n_test <span class="op">=</span> <span class="dv">20000</span></span>
<span id="cb4-57"><a href="#cb4-57" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="dv">1</span>, n_test)</span>
<span id="cb4-58"><a href="#cb4-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-59"><a href="#cb4-59" aria-hidden="true" tabindex="-1"></a>pred_dens_test <span class="op">=</span> np.zeros(n_test)</span>
<span id="cb4-60"><a href="#cb4-60" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">2000</span></span>
<span id="cb4-61"><a href="#cb4-61" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> b <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, n_test, batch_size):</span>
<span id="cb4-62"><a href="#cb4-62" aria-hidden="true" tabindex="-1"></a>    end_idx <span class="op">=</span> <span class="bu">min</span>(b <span class="op">+</span> batch_size, n_test)</span>
<span id="cb4-63"><a href="#cb4-63" aria-hidden="true" tabindex="-1"></a>    X_batch <span class="op">=</span> X_test[b:end_idx]</span>
<span id="cb4-64"><a href="#cb4-64" aria-hidden="true" tabindex="-1"></a>    X_expanded <span class="op">=</span> X_batch[:, np.newaxis, np.newaxis]</span>
<span id="cb4-65"><a href="#cb4-65" aria-hidden="true" tabindex="-1"></a>    p_batch <span class="op">=</span> (<span class="dv">1</span> <span class="op">-</span> A) <span class="op">*</span> norm_pdf(X_expanded, <span class="dv">0</span>, <span class="dv">1</span>) <span class="op">+</span> A <span class="op">*</span> norm_pdf(X_expanded, MU, <span class="dv">1</span>)</span>
<span id="cb4-66"><a href="#cb4-66" aria-hidden="true" tabindex="-1"></a>    expected_p <span class="op">=</span> np.<span class="bu">sum</span>(p_batch <span class="op">*</span> posterior, axis<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb4-67"><a href="#cb4-67" aria-hidden="true" tabindex="-1"></a>    pred_dens_test[b:end_idx] <span class="op">=</span> expected_p</span>
<span id="cb4-68"><a href="#cb4-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-69"><a href="#cb4-69" aria-hidden="true" tabindex="-1"></a>gen_error <span class="op">=</span> <span class="op">-</span><span class="fl">1.0</span> <span class="op">/</span> n_test <span class="op">*</span> np.<span class="bu">sum</span>(np.log(pred_dens_test <span class="op">+</span> <span class="fl">1e-12</span>))</span>
<span id="cb4-70"><a href="#cb4-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-71"><a href="#cb4-71" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Sample size n = </span><span class="sc">{</span>n<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb4-72"><a href="#cb4-72" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Training Error (T_n) = </span><span class="sc">{</span>T_n<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb4-73"><a href="#cb4-73" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Penalty Term (V / n) = </span><span class="sc">{</span>penalty<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb4-74"><a href="#cb4-74" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"WAIC = </span><span class="sc">{</span>waic<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb4-75"><a href="#cb4-75" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Generalization Error = </span><span class="sc">{</span>gen_error<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Sample size n = 100
Training Error (T_n) = 1.3310
Penalty Term (V / n) = 0.0057
WAIC = 1.3367
Generalization Error = 1.4243</code></pre>
</div>
</div>
</section>
<section id="visualizing-the-posterior" class="level3">
<h3 class="anchored" data-anchor-id="visualizing-the-posterior">Visualizing the Posterior</h3>
<p>We can also visualize the posterior distribution over our parameter grid to see how it concentrates around the true parameters. Because this is a singular model, the asymptotic distribution does not look like a neat Gaussian blob—instead, it concentrates along the non-identifiable manifold (the cross at <span class="math inline">\(a=0\)</span> or <span class="math inline">\(\mu=0\)</span>).</p>
<div id="cell-fig-posterior" class="cell" data-execution_count="5">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>contour <span class="op">=</span> plt.contourf(A, MU, posterior, levels<span class="op">=</span><span class="dv">20</span>, cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>plt.colorbar(contour, label<span class="op">=</span><span class="st">'Posterior Probability'</span>)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>plt.axhline(<span class="dv">0</span>, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="st">'True Parameters ($a=0$)'</span>)</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>plt.axvline(<span class="dv">0</span>, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="vs">r'True Parameters </span><span class="kw">(</span><span class="dv">$\m</span><span class="vs">u=0</span><span class="dv">$</span><span class="kw">)</span><span class="vs">'</span>)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Posterior Distribution of GMM Parameters"</span>)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Parameter $a$"</span>)</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r"Parameter </span><span class="dv">$\m</span><span class="vs">u</span><span class="dv">$</span><span class="vs">"</span>)</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-posterior" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-posterior-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="slt_gaussian_mixture_files/figure-html/fig-posterior-output-1.png" width="749" height="566" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-posterior-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Posterior Distribution of GMM Parameters. Notice how the probability mass is drawn out along the axes representing the true parameters.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="contrast-asymptotic-regularity-vs.-finite-sample-singularity" class="level3">
<h3 class="anchored" data-anchor-id="contrast-asymptotic-regularity-vs.-finite-sample-singularity">Contrast: Asymptotic Regularity vs.&nbsp;Finite-Sample Singularity</h3>
<p>What happens if the true distribution <span class="math inline">\(q(x)\)</span> is actually drawn from a genuine two-component mixture with <strong>both <span class="math inline">\(a \neq 0\)</span> and <span class="math inline">\(\mu \neq 0\)</span></strong>? For example, setting <span class="math inline">\(a=0.5\)</span> and <span class="math inline">\(\mu=0.3\)</span>.</p>
<p>In this case, the true parameters <span class="math inline">\((a_0, \mu_0)\)</span> lie in the interior of the parameter space, and there is exactly one parameter combination that matches the distribution. The true parameter space shrinks to a single, unique point. Because the true parameters are uniquely identifiable, the log-likelihood function has a strict minimum at <span class="math inline">\((a_0, \mu_0)\)</span>, and the Hessian (Fisher Information Matrix) becomes positive definite. <strong>Asymptotically (<span class="math inline">\(n \to \infty\)</span>), the model behaves as a standard regular model</strong>.</p>
<p>In this theoretical asymptotic scenario: 1. The Real Log Canonical Threshold becomes <span class="math inline">\(\lambda = d/2 = 2/2 = 1.0\)</span>. 2. The multiplicity is <span class="math inline">\(m = 1\)</span>. 3. The Bayesian Occam’s Razor penalty term in WAIC should converge to roughly <span class="math inline">\(1.0\times (\log n)/n\)</span> for free energy, or <span class="math inline">\(1.0/n\)</span> for generalization error. We expect the functional variance <span class="math inline">\(V\)</span> to approximate this <span class="math inline">\(1.0\)</span> factor (rather than <span class="math inline">\(0.5\)</span>).</p>
<p>Let’s test this by running our WAIC code again, but generating the data from <span class="math inline">\(a=0.5\)</span> and <span class="math inline">\(\mu=0.3\)</span>:</p>
<div id="cell-waic-regular-case" class="cell" data-execution_count="6">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Generate synthetic data from a true mixture model</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>n_reg <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>a_true <span class="op">=</span> <span class="fl">0.5</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>mu_true <span class="op">=</span> <span class="fl">0.3</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> np.random.binomial(<span class="dv">1</span>, a_true, n_reg)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>X_reg <span class="op">=</span> np.zeros(n_reg)</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>X_reg[z <span class="op">==</span> <span class="dv">0</span>] <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="dv">1</span>, np.<span class="bu">sum</span>(z <span class="op">==</span> <span class="dv">0</span>))</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>X_reg[z <span class="op">==</span> <span class="dv">1</span>] <span class="op">=</span> np.random.normal(mu_true, <span class="dv">1</span>, np.<span class="bu">sum</span>(z <span class="op">==</span> <span class="dv">1</span>))</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Compute log-likelihood</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>log_p_reg <span class="op">=</span> np.zeros((n_reg, <span class="bu">len</span>(mu_vals), <span class="bu">len</span>(a_vals)))</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, x <span class="kw">in</span> <span class="bu">enumerate</span>(X_reg):</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>    p_x_w <span class="op">=</span> (<span class="dv">1</span> <span class="op">-</span> A) <span class="op">*</span> norm_pdf(x, <span class="dv">0</span>, <span class="dv">1</span>) <span class="op">+</span> A <span class="op">*</span> norm_pdf(x, MU, <span class="dv">1</span>)</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>    log_p_reg[i] <span class="op">=</span> np.log(p_x_w <span class="op">+</span> <span class="fl">1e-12</span>)</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Compute posterior</span></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>total_log_likelihood_reg <span class="op">=</span> np.<span class="bu">sum</span>(log_p_reg, axis<span class="op">=</span><span class="dv">0</span>) </span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>log_posterior_reg <span class="op">=</span> total_log_likelihood_reg <span class="op">-</span> np.<span class="bu">max</span>(total_log_likelihood_reg)</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>posterior_reg <span class="op">=</span> np.exp(log_posterior_reg)</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>posterior_reg <span class="op">/=</span> np.<span class="bu">sum</span>(posterior_reg) </span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. Calculate WAIC penalty</span></span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>predictive_density_reg <span class="op">=</span> np.zeros(n_reg)</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>V_w_reg <span class="op">=</span> np.zeros(n_reg)</span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_reg):</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>    predictive_density_reg[i] <span class="op">=</span> np.<span class="bu">sum</span>(np.exp(log_p_reg[i]) <span class="op">*</span> posterior_reg)</span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>    expected_log_p <span class="op">=</span> np.<span class="bu">sum</span>(log_p_reg[i] <span class="op">*</span> posterior_reg)</span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>    expected_log_p_sq <span class="op">=</span> np.<span class="bu">sum</span>((log_p_reg[i]<span class="op">**</span><span class="dv">2</span>) <span class="op">*</span> posterior_reg)</span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a>    V_w_reg[i] <span class="op">=</span> expected_log_p_sq <span class="op">-</span> expected_log_p<span class="op">**</span><span class="dv">2</span></span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a>T_n_reg <span class="op">=</span> <span class="op">-</span><span class="fl">1.0</span> <span class="op">/</span> n_reg <span class="op">*</span> np.<span class="bu">sum</span>(np.log(predictive_density_reg <span class="op">+</span> <span class="fl">1e-12</span>))</span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a>penalty_reg <span class="op">=</span> np.mean(V_w_reg)</span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a>waic_reg <span class="op">=</span> T_n_reg <span class="op">+</span> penalty_reg</span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a><span class="co"># 5. Calculate True Generalization Error</span></span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a>n_test <span class="op">=</span> <span class="dv">20000</span></span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a>z_test <span class="op">=</span> np.random.binomial(<span class="dv">1</span>, a_true, n_test)</span>
<span id="cb7-40"><a href="#cb7-40" aria-hidden="true" tabindex="-1"></a>X_test_reg <span class="op">=</span> np.zeros(n_test)</span>
<span id="cb7-41"><a href="#cb7-41" aria-hidden="true" tabindex="-1"></a>X_test_reg[z_test <span class="op">==</span> <span class="dv">0</span>] <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="dv">1</span>, np.<span class="bu">sum</span>(z_test <span class="op">==</span> <span class="dv">0</span>))</span>
<span id="cb7-42"><a href="#cb7-42" aria-hidden="true" tabindex="-1"></a>X_test_reg[z_test <span class="op">==</span> <span class="dv">1</span>] <span class="op">=</span> np.random.normal(mu_true, <span class="dv">1</span>, np.<span class="bu">sum</span>(z_test <span class="op">==</span> <span class="dv">1</span>))</span>
<span id="cb7-43"><a href="#cb7-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-44"><a href="#cb7-44" aria-hidden="true" tabindex="-1"></a>pred_dens_test_reg <span class="op">=</span> np.zeros(n_test)</span>
<span id="cb7-45"><a href="#cb7-45" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">2000</span></span>
<span id="cb7-46"><a href="#cb7-46" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> b <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, n_test, batch_size):</span>
<span id="cb7-47"><a href="#cb7-47" aria-hidden="true" tabindex="-1"></a>    end_idx <span class="op">=</span> <span class="bu">min</span>(b <span class="op">+</span> batch_size, n_test)</span>
<span id="cb7-48"><a href="#cb7-48" aria-hidden="true" tabindex="-1"></a>    X_batch <span class="op">=</span> X_test_reg[b:end_idx]</span>
<span id="cb7-49"><a href="#cb7-49" aria-hidden="true" tabindex="-1"></a>    X_expanded <span class="op">=</span> X_batch[:, np.newaxis, np.newaxis]</span>
<span id="cb7-50"><a href="#cb7-50" aria-hidden="true" tabindex="-1"></a>    p_batch <span class="op">=</span> (<span class="dv">1</span> <span class="op">-</span> A) <span class="op">*</span> norm_pdf(X_expanded, <span class="dv">0</span>, <span class="dv">1</span>) <span class="op">+</span> A <span class="op">*</span> norm_pdf(X_expanded, MU, <span class="dv">1</span>)</span>
<span id="cb7-51"><a href="#cb7-51" aria-hidden="true" tabindex="-1"></a>    expected_p <span class="op">=</span> np.<span class="bu">sum</span>(p_batch <span class="op">*</span> posterior_reg, axis<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb7-52"><a href="#cb7-52" aria-hidden="true" tabindex="-1"></a>    pred_dens_test_reg[b:end_idx] <span class="op">=</span> expected_p</span>
<span id="cb7-53"><a href="#cb7-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-54"><a href="#cb7-54" aria-hidden="true" tabindex="-1"></a>gen_error_reg <span class="op">=</span> <span class="op">-</span><span class="fl">1.0</span> <span class="op">/</span> n_test <span class="op">*</span> np.<span class="bu">sum</span>(np.log(pred_dens_test_reg <span class="op">+</span> <span class="fl">1e-12</span>))</span>
<span id="cb7-55"><a href="#cb7-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-56"><a href="#cb7-56" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Sample size n = </span><span class="sc">{</span>n_reg<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb7-57"><a href="#cb7-57" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Training Error (T_n) = </span><span class="sc">{</span>T_n_reg<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb7-58"><a href="#cb7-58" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Regular Penalty Term (V / n) = </span><span class="sc">{</span>penalty_reg<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb7-59"><a href="#cb7-59" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"WAIC = </span><span class="sc">{</span>waic_reg<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb7-60"><a href="#cb7-60" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Generalization Error = </span><span class="sc">{</span>gen_error_reg<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb7-61"><a href="#cb7-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-62"><a href="#cb7-62" aria-hidden="true" tabindex="-1"></a><span class="co"># 6. Plot the new regular posterior</span></span>
<span id="cb7-63"><a href="#cb7-63" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">5</span>))</span>
<span id="cb7-64"><a href="#cb7-64" aria-hidden="true" tabindex="-1"></a>contour3 <span class="op">=</span> plt.contourf(A, MU, posterior_reg, levels<span class="op">=</span><span class="dv">20</span>, cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb7-65"><a href="#cb7-65" aria-hidden="true" tabindex="-1"></a>plt.colorbar(contour3, label<span class="op">=</span><span class="st">'Posterior Probability'</span>)</span>
<span id="cb7-66"><a href="#cb7-66" aria-hidden="true" tabindex="-1"></a>plt.scatter([a_true], [mu_true], color<span class="op">=</span><span class="st">'red'</span>, marker<span class="op">=</span><span class="st">'x'</span>, s<span class="op">=</span><span class="dv">100</span>, linewidths<span class="op">=</span><span class="dv">3</span>, label<span class="op">=</span><span class="st">'True Parameters'</span>)</span>
<span id="cb7-67"><a href="#cb7-67" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Posterior Distribution (Regular Model)"</span>)</span>
<span id="cb7-68"><a href="#cb7-68" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Parameter $a$"</span>)</span>
<span id="cb7-69"><a href="#cb7-69" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r"Parameter </span><span class="dv">$\m</span><span class="vs">u</span><span class="dv">$</span><span class="vs">"</span>)</span>
<span id="cb7-70"><a href="#cb7-70" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb7-71"><a href="#cb7-71" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb7-72"><a href="#cb7-72" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Sample size n = 100
Training Error (T_n) = 1.3483
Regular Penalty Term (V / n) = 0.0073
WAIC = 1.3556
Generalization Error = 1.4317</code></pre>
</div>
<div class="cell-output cell-output-display">
<div id="waic-regular-case" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="slt_gaussian_mixture_files/figure-html/waic-regular-case-output-2.png" width="656" height="470" class="figure-img"></p>
<figcaption>Posterior Distribution for the ‘Regular’ Case with Finite Sample Size. Because the true parameter <span class="math inline">\(\mu=0.3\)</span> is very close to the singularity <span class="math inline">\(\mu=0\)</span>, the sample size <span class="math inline">\(n=100\)</span> is not large enough to resolve the parameters perfectly. The posterior is gravitationally pulled into the singularity, maintaining the shape and WAIC penalty of a singular model.</figcaption>
</figure>
</div>
</div>
</div>
<section id="the-gravitational-pull-of-the-singularity" class="level4">
<h4 class="anchored" data-anchor-id="the-gravitational-pull-of-the-singularity">The Gravitational Pull of the Singularity</h4>
<p>Notice what happened in the plot and output! Despite the true model being theoretically regular, the penalty term <span class="math inline">\(V \approx 0.73\)</span> is far closer to the singular penalty <span class="math inline">\(0.5\)</span> than the regular penalty <span class="math inline">\(1.0\)</span>. And the posterior clearly does not look like a neat Gaussian blob; it is smeared out along the <span class="math inline">\(\mu=0\)</span> axis exactly like the singular case.</p>
<p>Why? Because the true parameter <span class="math inline">\(\mu=0.3\)</span> is <strong>extremely close to the singular manifold</strong> (<span class="math inline">\(\mu=0\)</span>). Watanabe’s theory is asymptotic (<span class="math inline">\(n \to \infty\)</span>). For a finite sample size of <span class="math inline">\(n=100\)</span>, the data simply does not have enough resolution to confidently distinguish the true distribution <span class="math inline">\(\mu=0.3\)</span> from the singularity at <span class="math inline">\(\mu=0\)</span>. The likelihood spills over into the singularity, and the posterior is “gravitationally pulled” into the non-identifiable manifold, forcing the model to behave like a singular machine. It would take a much larger sample size (e.g., <span class="math inline">\(n=10,000\)</span>) for the regular asymptotic theory to overcome the geometry of the singularity locally.</p>
<p>This is a profound realization from Singular Learning Theory: singularities govern the learning dynamics of models in practice, <strong>even when the true distribution is slightly off the singularity</strong>, because finite samples cannot perfectly resolve the identifiability.</p>
<p>The mathematical beauty of Chapter 6 is that <strong>WAIC works uniformly in both these cases without adjustment</strong>. The functional variance inherently adapts to the finite-sample geometry of the posterior, providing an accurate estimator of the true generalization error regardless of whether the model is trapped in a singularity or has broken free!</p>
</section>
</section>
<section id="is-waic-still-good-for-small-samples" class="level3">
<h3 class="anchored" data-anchor-id="is-waic-still-good-for-small-samples">Is WAIC Still Good for Small Samples?</h3>
<p>A natural follow-up question is whether WAIC remains a good estimator of generalization error when the sample size <span class="math inline">\(n\)</span> is very small. <strong>The short answer is yes</strong>, and this is exactly where WAIC shines compared to classical criteria like the Deviance Information Criterion (DIC).</p>
<p>Classical criteria like DIC rely on a <em>point estimate</em> of the parameter (usually the posterior mean or mode). In a singular model, the posterior is highly non-Gaussian and often stretched along a non-identifiable manifold (like the cross at <span class="math inline">\(\mu=0\)</span> or <span class="math inline">\(a=0\)</span>). For a small sample, the posterior mean might land in a region of very low actual probability mass (e.g., in the empty space between two dense arms of the posterior). Because DIC evaluates the likelihood at this single mathematically awkward point, it can wildly misestimate the generalization error.</p>
<p>WAIC, on the other hand, averages the likelihood over the <em>entire</em> posterior distribution and calculates the functional variance point-by-point. It entirely avoids point estimation.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/gustavdelius\.github\.io\/Watanabe\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>