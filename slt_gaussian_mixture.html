<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.27">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Illustrating Watanabe’s Singular Learning Theory with a Gaussian Mixture Model – Watanabe's Singular Learning Theory</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js" integrity="sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2" crossorigin="anonymous"></script><script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-ed96de9b727972fe78a7b5d16c58bf87.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-690d2c05533395d17c836f89f1947714.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdn.jsdelivr.net/npm/requirejs@2.3.6/require.min.js" integrity="sha384-c9c+LnTbwQ3aujuU7ULEPVvgLs+Fn6fJUvIGTsuu1ZcCf11fiEubah0ttpca4ntM sha384-6V1/AdqZRWk1KAlWbKBlGhN7VG4iE/yAZcO6NZPMF8od0vukrvr0tg4qY6NSrItx" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Watanabe’s Singular Learning Theory</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="./index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./cheat_sheet.html"> 
<span class="menu-text">Cheat Sheet</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./regular_and_singular_models.html"> 
<span class="menu-text">Regular and Singular Models</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link active" href="./slt_gaussian_mixture.html" aria-current="page"> 
<span class="menu-text">SLT Gaussian Mixture</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./neural_network_figure_2_7.html"> 
<span class="menu-text">Neural Network (Figure 2.7)</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
    <a href="https://github.com/gustavdelius/Watanabe" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-github"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#chapter-1.-introduction-the-model-and-the-true-distribution" id="toc-chapter-1.-introduction-the-model-and-the-true-distribution" class="nav-link active" data-scroll-target="#chapter-1.-introduction-the-model-and-the-true-distribution">Chapter 1. Introduction: The Model and the True Distribution</a>
  <ul class="collapse">
  <li><a href="#the-two-component-gaussian-mixture" id="toc-the-two-component-gaussian-mixture" class="nav-link" data-scroll-target="#the-two-component-gaussian-mixture">The Two-Component Gaussian Mixture</a></li>
  <li><a href="#the-set-of-true-parameters" id="toc-the-set-of-true-parameters" class="nav-link" data-scroll-target="#the-set-of-true-parameters">The Set of True Parameters</a></li>
  <li><a href="#the-kullback-leibler-divergence" id="toc-the-kullback-leibler-divergence" class="nav-link" data-scroll-target="#the-kullback-leibler-divergence">The Kullback-Leibler Divergence</a></li>
  </ul></li>
  <li><a href="#chapter-2.-resolution-of-singularities" id="toc-chapter-2.-resolution-of-singularities" class="nav-link" data-scroll-target="#chapter-2.-resolution-of-singularities">Chapter 2. Resolution of Singularities</a>
  <ul class="collapse">
  <li><a href="#the-blow-up-transformation" id="toc-the-blow-up-transformation" class="nav-link" data-scroll-target="#the-blow-up-transformation">The Blow-Up Transformation</a></li>
  </ul></li>
  <li><a href="#chapter-3.-standard-form-and-real-log-canonical-threshold" id="toc-chapter-3.-standard-form-and-real-log-canonical-threshold" class="nav-link" data-scroll-target="#chapter-3.-standard-form-and-real-log-canonical-threshold">Chapter 3. Standard Form and Real Log Canonical Threshold</a></li>
  <li><a href="#chapter-4.-singular-fluctuation-and-free-energy" id="toc-chapter-4.-singular-fluctuation-and-free-energy" class="nav-link" data-scroll-target="#chapter-4.-singular-fluctuation-and-free-energy">Chapter 4. Singular Fluctuation and Free Energy</a></li>
  <li><a href="#chapter-5.-generalization-and-training-errors" id="toc-chapter-5.-generalization-and-training-errors" class="nav-link" data-scroll-target="#chapter-5.-generalization-and-training-errors">Chapter 5. Generalization and Training Errors</a></li>
  <li><a href="#chapter-6.-asymptotic-expansion-and-waic" id="toc-chapter-6.-asymptotic-expansion-and-waic" class="nav-link" data-scroll-target="#chapter-6.-asymptotic-expansion-and-waic">Chapter 6. Asymptotic Expansion and WAIC</a>
  <ul class="collapse">
  <li><a href="#calculating-waic-in-practice" id="toc-calculating-waic-in-practice" class="nav-link" data-scroll-target="#calculating-waic-in-practice">Calculating WAIC in Practice</a></li>
  <li><a href="#visualizing-the-posterior" id="toc-visualizing-the-posterior" class="nav-link" data-scroll-target="#visualizing-the-posterior">Visualizing the Posterior</a></li>
  <li><a href="#contrast-asymptotic-regularity-vs.-finite-sample-singularity" id="toc-contrast-asymptotic-regularity-vs.-finite-sample-singularity" class="nav-link" data-scroll-target="#contrast-asymptotic-regularity-vs.-finite-sample-singularity">Contrast: Asymptotic Regularity vs.&nbsp;Finite-Sample Singularity</a></li>
  <li><a href="#is-waic-still-good-for-small-samples" id="toc-is-waic-still-good-for-small-samples" class="nav-link" data-scroll-target="#is-waic-still-good-for-small-samples">Is WAIC Still Good for Small Samples?</a></li>
  </ul></li>
  <li><a href="#chapter7.-gibbs-sampler-for-gaussian-mixture" id="toc-chapter7.-gibbs-sampler-for-gaussian-mixture" class="nav-link" data-scroll-target="#chapter7.-gibbs-sampler-for-gaussian-mixture">Chapter7. Gibbs Sampler for Gaussian Mixture</a>
  <ul class="collapse">
  <li><a href="#gibbs-sampler-vs.-standard-mcmc-metropolis-hastings" id="toc-gibbs-sampler-vs.-standard-mcmc-metropolis-hastings" class="nav-link" data-scroll-target="#gibbs-sampler-vs.-standard-mcmc-metropolis-hastings">Gibbs Sampler vs.&nbsp;Standard MCMC (Metropolis-Hastings)</a></li>
  </ul></li>
  <li><a href="#chapter-8.-comparing-information-criteria" id="toc-chapter-8.-comparing-information-criteria" class="nav-link" data-scroll-target="#chapter-8.-comparing-information-criteria">Chapter 8. Comparing Information Criteria</a>
  <ul class="collapse">
  <li><a href="#interpretation-of-results" id="toc-interpretation-of-results" class="nav-link" data-scroll-target="#interpretation-of-results">Interpretation of Results</a></li>
  </ul></li>
  <li><a href="#chapter-9.-phase-transition-in-a-hierarchical-model" id="toc-chapter-9.-phase-transition-in-a-hierarchical-model" class="nav-link" data-scroll-target="#chapter-9.-phase-transition-in-a-hierarchical-model">Chapter 9. Phase Transition in a Hierarchical Model</a>
  <ul class="collapse">
  <li><a href="#example-67-normal-mixture-phase-transition" id="toc-example-67-normal-mixture-phase-transition" class="nav-link" data-scroll-target="#example-67-normal-mixture-phase-transition">Example 67: Normal Mixture Phase Transition</a></li>
  <li><a href="#the-two-modes-of-the-model" id="toc-the-two-modes-of-the-model" class="nav-link" data-scroll-target="#the-two-modes-of-the-model">The Two Modes of the Model</a></li>
  <li><a href="#calculating-the-rlct" id="toc-calculating-the-rlct" class="nav-link" data-scroll-target="#calculating-the-rlct">Calculating the RLCT</a></li>
  <li><a href="#reproducing-figures-9.1-and-9.2-error-metric-phase-transition" id="toc-reproducing-figures-9.1-and-9.2-error-metric-phase-transition" class="nav-link" data-scroll-target="#reproducing-figures-9.1-and-9.2-error-metric-phase-transition">Reproducing Figures 9.1 and 9.2: Error Metric Phase Transition</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Illustrating Watanabe’s Singular Learning Theory with a Gaussian Mixture Model</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>This document provides a walk-through of Sumio Watanabe’s <em>Singular Learning Theory</em>, using a two-component Gaussian Mixture Model (GMM) as a concrete pedagogical example. We had already seen in <a href="./regular_and_singular_models.html#sec:singular-model">Regular and Singular Models</a> that this model is singular.</p>
<p>Each section in this document corresponds to one of the chapters in Watanabe’s book and illustrates some aspect of that chapter with the help of the Gaussian Mixture Model.</p>
<section id="chapter-1.-introduction-the-model-and-the-true-distribution" class="level2">
<h2 class="anchored" data-anchor-id="chapter-1.-introduction-the-model-and-the-true-distribution">Chapter 1. Introduction: The Model and the True Distribution</h2>
<p>In statistical formulation, we consider a learning machine defined by a parametric statistical model <span class="math inline">\(p(x|w)\)</span> and a true data-generating distribution <span class="math inline">\(q(x)\)</span>.</p>
<section id="the-two-component-gaussian-mixture" class="level3">
<h3 class="anchored" data-anchor-id="the-two-component-gaussian-mixture">The Two-Component Gaussian Mixture</h3>
<p>Let’s define a simple 2-parameter Gaussian Mixture Model where one component is fixed at the origin: <span class="math display">\[ p(x|w) = (1-a) \mathcal{N}(x|0, 1) + a \mathcal{N}(x|\mu, 1) \]</span> Here, the parameter vector is <span class="math inline">\(w = (a, \mu) \in W = [0, 1] \times [-c, c]\)</span>.</p>
<p>Suppose the <strong>true distribution</strong> is simply a standard normal distribution: <span class="math display">\[ q(x) = \mathcal{N}(x|0, 1) \]</span></p>
<div id="cell-fig-distributions" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">400</span>)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>q <span class="op">=</span> np.exp(<span class="op">-</span>x<span class="op">**</span><span class="dv">2</span><span class="op">/</span><span class="dv">2</span>)<span class="op">/</span>np.sqrt(<span class="dv">2</span><span class="op">*</span>np.pi)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> <span class="fl">0.7</span> <span class="op">*</span> np.exp(<span class="op">-</span>x<span class="op">**</span><span class="dv">2</span><span class="op">/</span><span class="dv">2</span>)<span class="op">/</span>np.sqrt(<span class="dv">2</span><span class="op">*</span>np.pi) <span class="op">+</span> <span class="fl">0.3</span> <span class="op">*</span> np.exp(<span class="op">-</span>(x<span class="op">-</span><span class="fl">1.5</span>)<span class="op">**</span><span class="dv">2</span><span class="op">/</span><span class="dv">2</span>)<span class="op">/</span>np.sqrt(<span class="dv">2</span><span class="op">*</span>np.pi)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">5</span>))</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>plt.plot(x, q, label<span class="op">=</span><span class="vs">r'True Distribution </span><span class="dv">$</span><span class="vs">q</span><span class="kw">(</span><span class="vs">x</span><span class="kw">)</span><span class="vs"> = </span><span class="dv">\m</span><span class="vs">athcal{N}</span><span class="kw">(</span><span class="vs">0,1</span><span class="kw">)</span><span class="dv">$</span><span class="vs">'</span>, color<span class="op">=</span><span class="st">'blue'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>plt.plot(x, p, label<span class="op">=</span><span class="vs">r'Model </span><span class="dv">$</span><span class="vs">p</span><span class="kw">(</span><span class="vs">x</span><span class="cf">|</span><span class="vs">w</span><span class="kw">)</span><span class="dv">$</span><span class="vs"> with </span><span class="dv">$</span><span class="vs">a=0</span><span class="dv">.</span><span class="vs">3, </span><span class="dv">\m</span><span class="vs">u=1</span><span class="dv">.</span><span class="vs">5</span><span class="dv">$</span><span class="vs">'</span>, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Gaussian Mixture Model vs True Distribution"</span>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"x"</span>)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Density"</span>)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-distributions" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-distributions-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="slt_gaussian_mixture_files/figure-html/fig-distributions-output-1.png" width="758" height="470" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-distributions-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: True Distribution vs Model
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="the-set-of-true-parameters" class="level3">
<h3 class="anchored" data-anchor-id="the-set-of-true-parameters">The Set of True Parameters</h3>
<p>The true distribution is realized by the model whenever <span class="math inline">\(p(x|w) = q(x)\)</span>. By inspecting the equation: <span class="math display">\[ (1-a) \mathcal{N}(x|0, 1) + a \mathcal{N}(x|\mu, 1) = \mathcal{N}(x|0, 1) \]</span> <span class="math display">\[ a (\mathcal{N}(x|\mu, 1) - \mathcal{N}(x|0, 1)) = 0 \]</span></p>
<p>This holds true if and only if <strong><span class="math inline">\(a = 0\)</span></strong> (the mixing proportion is zero) OR <strong><span class="math inline">\(\mu = 0\)</span></strong> (both components are identical). Thus, the set of true parameters <span class="math inline">\(W_0\)</span> is: <span class="math display">\[ W_0 = \{ (a, \mu) \in W : a = 0 \text{ or } \mu = 0 \} \]</span></p>
<p>This means that <span class="math inline">\(W_0\)</span> is not a single point, but the <strong>union of two intersecting lines</strong>. In classical (regular) statistical theory, <span class="math inline">\(W_0\)</span> is assumed to be a single point, and the Fisher Information Matrix is positive definite. Here, the Fisher Information Matrix degenerates on <span class="math inline">\(W_0\)</span>, making this a <strong>singular model</strong>.</p>
</section>
<section id="the-kullback-leibler-divergence" class="level3">
<h3 class="anchored" data-anchor-id="the-kullback-leibler-divergence">The Kullback-Leibler Divergence</h3>
<p>The log-likelihood ratio (empirical loss) is driven by the Kullback-Leibler (KL) divergence from <span class="math inline">\(q(x)\)</span> to <span class="math inline">\(p(x|w)\)</span>: <span class="math display">\[ K(w) = \int q(x) \log \frac{q(x)}{p(x|w)} dx \]</span></p>
<p>Using a Taylor expansion for small <span class="math inline">\(a\)</span> and <span class="math inline">\(\mu\)</span>, we can approximate <span class="math inline">\(p(x|w)\)</span>: <span class="math display">\[ p(x|w) = \mathcal{N}(x|0, 1) \left[ 1 + a (e^{\mu x - \mu^2/2} - 1) \right] \approx \mathcal{N}(x|0, 1) \left[ 1 + a\left(\mu x + \frac{1}{2}\mu^2(x^2 - 1)\right) \right] \]</span></p>
<p>Plugging this into the KL divergence and using <span class="math inline">\(-\log(1+z) \approx -z + z^2/2\)</span>, the linear terms integrate to 0 under <span class="math inline">\(q(x)\)</span>, leaving the leading non-zero term: <span class="math display">\[ K(w) \approx \frac{1}{2} a^2 \mu^2 \]</span></p>
<div id="cell-fig-kl-divergence" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">400</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>M <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">400</span>)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>AA, MM <span class="op">=</span> np.meshgrid(A, M)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>K <span class="op">=</span> <span class="fl">0.5</span> <span class="op">*</span> (AA<span class="op">**</span><span class="dv">2</span>) <span class="op">*</span> (MM<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>contour <span class="op">=</span> plt.contourf(AA, MM, K, levels<span class="op">=</span><span class="dv">20</span>, cmap<span class="op">=</span><span class="st">'viridis_r'</span>)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>plt.colorbar(contour, label<span class="op">=</span><span class="vs">r'KL Divergence </span><span class="dv">$</span><span class="vs">K</span><span class="kw">(</span><span class="vs">a, </span><span class="dv">\m</span><span class="vs">u</span><span class="kw">)</span><span class="dv">$</span><span class="vs">'</span>)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>plt.axhline(<span class="dv">0</span>, color<span class="op">=</span><span class="st">'red'</span>, linewidth<span class="op">=</span><span class="dv">3</span>, label<span class="op">=</span><span class="st">'True Parameters ($a=0$)'</span>)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>plt.axvline(<span class="dv">0</span>, color<span class="op">=</span><span class="st">'red'</span>, linewidth<span class="op">=</span><span class="dv">3</span>, label<span class="op">=</span><span class="vs">r'True Parameters </span><span class="kw">(</span><span class="dv">$\m</span><span class="vs">u=0</span><span class="dv">$</span><span class="kw">)</span><span class="vs">'</span>)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Parameter Space and KL Divergence"</span>)</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Parameter $a$"</span>)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r"Parameter </span><span class="dv">$\m</span><span class="vs">u</span><span class="dv">$</span><span class="vs">"</span>)</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-kl-divergence" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-kl-divergence-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="slt_gaussian_mixture_files/figure-html/fig-kl-divergence-output-1.png" width="731" height="566" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-kl-divergence-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Parameter Space and KL Divergence. Notice how the valley of <span class="math inline">\(K(w)=0\)</span> forms a cross at <span class="math inline">\(a=0\)</span> and <span class="math inline">\(\mu=0\)</span>.
</figcaption>
</figure>
</div>
</div>
</div>
<hr>
</section>
</section>
<section id="chapter-2.-resolution-of-singularities" class="level2">
<h2 class="anchored" data-anchor-id="chapter-2.-resolution-of-singularities">Chapter 2. Resolution of Singularities</h2>
<p>Because the set of true parameters <span class="math inline">\(W_0\)</span> has a singularity (an intersection forming a cross), standard asymptotic expansions (like the Laplace approximation) fail. Watanabe employs <strong>Hironaka’s Theorem on the Resolution of Singularities</strong> from algebraic geometry to resolve this.</p>
<p>The theorem states that there exists a real analytic manifold <span class="math inline">\(\mathcal{M}\)</span> and a proper analytic map <span class="math inline">\(g: \mathcal{M} \to W\)</span> (a “blow-up”) such that the composition <span class="math inline">\(K(g(u))\)</span> has a simple normal crossing form.</p>
<section id="the-blow-up-transformation" class="level3">
<h3 class="anchored" data-anchor-id="the-blow-up-transformation">The Blow-Up Transformation</h3>
<p>For the approximation <span class="math inline">\(K(w) \approx \frac{1}{2} a^2 \mu^2\)</span>, the true parameters <span class="math inline">\(W_0\)</span> correspond to the crossing lines <span class="math inline">\(a=0\)</span> and <span class="math inline">\(\mu=0\)</span>. To resolve this intersection, we apply a “blow-up” transformation. A blow-up geometrically replaces the problematic intersection point (the origin) with an entire line (called the exceptional divisor), separating the paths that cross there.</p>
<p>We can reparameterize the space by keeping track of the <em>slope</em> of lines passing through the origin. We define a local coordinate chart (a directional blow-up) as: <span class="math display">\[ a = u_1 \]</span> <span class="math display">\[ \mu = u_1 u_2 \]</span></p>
<p>Here, <span class="math inline">\(u_1\)</span> simply represents the original <span class="math inline">\(a\)</span> coordinate, while <span class="math inline">\(u_2 = \mu / a\)</span> represents the slope of a line passing through the origin in the <span class="math inline">\((a, \mu)\)</span> parameter space.</p>
<ul>
<li>A single point in the original space—the origin <span class="math inline">\((a=0, \mu=0)\)</span>—now corresponds to the entire line <span class="math inline">\(u_1 = 0\)</span> for any value of <span class="math inline">\(u_2\)</span> in the new space.</li>
<li>The crossing lines in the <span class="math inline">\((a, \mu)\)</span> space have been pulled apart. Actually, our new coordinate system does not extend to the line <span class="math inline">\(a=0\)</span>, but we can get arbitrarily close to it. We would need another patch with coordinates <span class="math inline">\(\mu, a/\mu\)</span> to represent the line <span class="math inline">\(a=0\)</span>.</li>
</ul>
<p>The KL divergence in these new coordinates <span class="math inline">\(u = (u_1, u_2)\)</span> is <span class="math display">\[ K(g(u)) \approx \frac{1}{2} (u_1)^2 (u_1 u_2)^2 = \frac{1}{2} u_1^4 u_2^2 \]</span></p>
<p>We must also account for the distortion of the volume measure, dictated by the Jacobian of <span class="math inline">\(g\)</span>: <span class="math display">\[ dw = |g'(u)| du = \left| \det \begin{pmatrix} 1 &amp; 0 \\ u_2 &amp; u_1 \end{pmatrix} \right| du_1 du_2 = |u_1| du_1 du_2 \]</span></p>
<hr>
</section>
</section>
<section id="chapter-3.-standard-form-and-real-log-canonical-threshold" class="level2">
<h2 class="anchored" data-anchor-id="chapter-3.-standard-form-and-real-log-canonical-threshold">Chapter 3. Standard Form and Real Log Canonical Threshold</h2>
<p>In Chapter 3, Watanabe introduces the concept of the <strong>Real Log Canonical Threshold (RLCT)</strong>, denoted by <span class="math inline">\(\lambda\)</span>, and its multiplicity <span class="math inline">\(m\)</span>. These two algebraic invariants completely govern the asymptotic behavior of the learning machine.</p>
<p>To understand why the blow-up was a strictly necessary algebraic maneuver, we have to look at how <span class="math inline">\(\lambda\)</span> is formally calculated. By definition, the RLCT is found by examining the analytic continuation of the <strong>zeta function</strong> of the statistical model, given by the integral: <span class="math display">\[ \zeta(z) = \int (K(w))^z \varphi(w) dw \]</span> where <span class="math inline">\(\varphi(w)\)</span> is the prior distribution and <span class="math inline">\(z \in \mathbb{C}\)</span> (<span class="math inline">\(\Re(z) &gt; 0\)</span>). The RLCT <span class="math inline">\(\lambda\)</span> is defined such that <span class="math inline">\(-\lambda\)</span> is the largest (closest to zero) real pole of this function, and its multiplicity <span class="math inline">\(m\)</span> is the order of this pole.</p>
<p>Without the blow-up, evaluating this integral and finding its poles is mathematically intractable. The true KL divergence <span class="math inline">\(K(w)\)</span> isn’t just a simple polynomial like <span class="math inline">\(a^2\mu^2\)</span>; it contains an infinite series of higher-order terms from the Taylor expansion. Because the variables are coupled in a highly non-linear way at the singularity (the cross <span class="math inline">\(a=0, \mu=0\)</span>), you cannot separate the variables to evaluate the integral.</p>
<p><strong>This is where the blow-up of the singularity resolves the integration problem.</strong> Hironaka’s Theorem guarantees that after passing to the resolved coordinates <span class="math inline">\(u\)</span>, the fully complex divergence <span class="math inline">\(K(g(u))\)</span> perfectly factors into a single monomial multiplied by a non-vanishing positive analytic function <span class="math inline">\(b(u) &gt; 0\)</span>. The prior measure and Jacobian of the blow-up also become a simple monomial multiplied by a strictly positive function <span class="math inline">\(c(u) &gt; 0\)</span>.</p>
<p>When we substitute this <strong>Standard Form</strong> into the zeta function integral using our resolved coordinates <span class="math inline">\(u\)</span>, the variables completely decouple near the origin: <span class="math display">\[ \zeta(z) = \int \left( u_1^{2k_{1}} u_2^{2k_{2}} b(u) \right)^z \left( u_1^{h_1} u_2^{h_2} c(u) \right) du_1 du_2 \]</span> <span class="math display">\[ \zeta(z) \approx C \left( \int u_1^{2k_1 z + h_1} du_1 \right) \left( \int u_2^{2k_2 z + h_2} du_2 \right) \]</span></p>
<p>Evaluating these independent 1D integrals yields formulas of the type: <span class="math display">\[ \int_0^\epsilon u^{2k z + h} du = \frac{\epsilon^{2k z + h + 1}}{2k z + h + 1} \]</span> This expression clearly has a pole exactly when the denominator is zero, i.e., at <span class="math inline">\(z = -\frac{h + 1}{2k}\)</span>.</p>
<p>By separating the variables, the blow-up perfectly isolates the poles of the zeta function!</p>
<p>Using our specific factors from the GMM blow-up:</p>
<ol type="1">
<li>Divergence function: <span class="math inline">\(K(u) = u_1^4 u_2^2\)</span> (so <span class="math inline">\(k_1 = 2\)</span>, <span class="math inline">\(k_2 = 1\)</span>)</li>
<li>Prior measure / Jacobian: <span class="math inline">\(\Phi(u) = u_1^1 u_2^0\)</span> (so <span class="math inline">\(h_1 = 1\)</span>, <span class="math inline">\(h_2 = 0\)</span>). <em>Note: This assumes a prior <span class="math inline">\(\varphi(w)\)</span> that does not vanish at the origin, such as a uniform prior <span class="math inline">\(\varphi(w) \propto 1\)</span>. For such a prior, the only algebraic zeros in the integral’s measure come from the Jacobian of the blow-up <span class="math inline">\(|u_1|\)</span>. This gives us <span class="math inline">\(u_1^1 u_2^0\)</span>, resulting in <span class="math inline">\(h_1 = 1, h_2 = 0\)</span>.</em></li>
</ol>
<p>The candidate poles <span class="math inline">\(z = -\lambda_j\)</span> along each coordinate axis <span class="math inline">\(j\)</span> give us: <span class="math display">\[ \lambda_j = \frac{h_j + 1}{2k_j} \]</span></p>
<p>For our <span class="math inline">\(u_1\)</span> and <span class="math inline">\(u_2\)</span>:</p>
<ul>
<li><span class="math inline">\(\lambda_1 = \frac{1 + 1}{4} = \frac{1}{2}\)</span></li>
<li><span class="math inline">\(\lambda_2 = \frac{0 + 1}{2} = \frac{1}{2}\)</span></li>
</ul>
<p>The overall RLCT <span class="math inline">\(\lambda\)</span> is dictated by the pole closest to zero, which is the minimum of these values: <span class="math display">\[ \lambda = \min(\lambda_1, \lambda_2) = \min(1/2, 1/2) = \frac{1}{2} \]</span></p>
<p>The <strong>multiplicity</strong> <span class="math inline">\(m\)</span> is the order of this leading pole, which equals the number of coordinate indices <span class="math inline">\(j\)</span> that achieve this minimum. Here, both <span class="math inline">\(\lambda_1\)</span> and <span class="math inline">\(\lambda_2\)</span> equal <span class="math inline">\(1/2\)</span>. Thus: <span class="math display">\[ m = 2 \]</span></p>
<p><em>Note: In a regular model with parameter dimension <span class="math inline">\(d=2\)</span>, the RLCT is always <span class="math inline">\(\lambda = d/2 = 1\)</span>. Our singular GMM has <span class="math inline">\(\lambda = 1/2 &lt; 1\)</span>, showcasing the mathematical definition of a singular learning machine.</em></p>
<hr>
</section>
<section id="chapter-4.-singular-fluctuation-and-free-energy" class="level2">
<h2 class="anchored" data-anchor-id="chapter-4.-singular-fluctuation-and-free-energy">Chapter 4. Singular Fluctuation and Free Energy</h2>
<p>In Bayesian evaluation, the <strong>Stochastic Complexity</strong> or <strong>Free Energy</strong> <span class="math inline">\(F_n\)</span> represents the negative log-marginal likelihood (evidence) of the data <span class="math inline">\(X^n\)</span>: <span class="math display">\[ F_n = -\log Z_n = -\log \int e^{-n L_n(w)} \varphi(w) dw \]</span> where <span class="math inline">\(L_n(w) = -\frac{1}{n} \sum \log p(X_i|w)\)</span> is the empirical loss.</p>
<p>Chapter 4 defines how the parameter posterior behaves under singular fluctuations. Using the algebraic invariants we just found, SLT proves that the free energy asymptotically expands as:</p>
<p><span class="math display">\[ F_n \approx n L_n(w_0) + \lambda \log n - (m-1) \log \log n + O_p(1) \]</span></p>
<p>For our two-component Gaussian Mixture Model: <span class="math display">\[ F_n \approx n L_n(w_0) + \frac{1}{2} \log n - \log \log n + O_p(1) \]</span></p>
<div id="cell-fig-free-energy" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> np.logspace(<span class="dv">1</span>, <span class="dv">4</span>, <span class="dv">100</span>)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>F_reg <span class="op">=</span> <span class="fl">1.0</span> <span class="op">*</span> np.log(n)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>F_sing <span class="op">=</span> <span class="fl">0.5</span> <span class="op">*</span> np.log(n) <span class="op">-</span> <span class="fl">1.0</span> <span class="op">*</span> np.log(np.log(n <span class="op">+</span> <span class="fl">1.1</span>))</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">5</span>))</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>plt.plot(n, F_reg, label<span class="op">=</span><span class="vs">r'Regular Model </span><span class="kw">(</span><span class="dv">$</span><span class="vs">d=2 </span><span class="er">\</span><span class="vs">Rightarrow </span><span class="er">\</span><span class="vs">lambda=1</span><span class="dv">$</span><span class="kw">)</span><span class="vs">'</span>, color<span class="op">=</span><span class="st">'red'</span>)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>plt.plot(n, F_sing, label<span class="op">=</span><span class="vs">r'Singular Model </span><span class="kw">(</span><span class="vs">GMM, </span><span class="dv">$</span><span class="er">\</span><span class="vs">lambda=0</span><span class="dv">.</span><span class="vs">5, m=2</span><span class="dv">$</span><span class="kw">)</span><span class="vs">'</span>, color<span class="op">=</span><span class="st">'blue'</span>)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Asymptotic Free Energy Penalty vs Sample Size"</span>)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Sample Size $n$ (log scale)"</span>)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Penalty Term"</span>)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>plt.xscale(<span class="st">'log'</span>)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-free-energy" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-free-energy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="slt_gaussian_mixture_files/figure-html/fig-free-energy-output-1.png" width="758" height="466" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-free-energy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Asymptotic Free Energy. The penalty term for the singular model is significantly smaller than for a regular model, making singular models heavily preferred by the marginal likelihood.
</figcaption>
</figure>
</div>
</div>
</div>
<p>This result breaks the classical Bayesian Information Criterion (BIC), which assumes a penalty of <span class="math inline">\(\frac{d}{2} \log n\)</span>. BIC would have penalized this 2-parameter model by <span class="math inline">\(1.0 \log n\)</span>, entirely missing the true Bayesian Occam’s Razor effect on the singular manifold.</p>
<hr>
</section>
<section id="chapter-5.-generalization-and-training-errors" class="level2">
<h2 class="anchored" data-anchor-id="chapter-5.-generalization-and-training-errors">Chapter 5. Generalization and Training Errors</h2>
<p>Chapter 5 links the geometry of the parameter space to the average errors. Let <span class="math inline">\(G_n\)</span> be the <strong>Generalization Error</strong> (expected loss on new data) and <span class="math inline">\(T_n\)</span> be the <strong>Training Error</strong> (empirical loss on the training set).</p>
<p>In traditional regular statistics (like AIC), the expectation of these errors relies on <span class="math inline">\(d\)</span>: <span class="math display">\[ \mathbb{E}[G_n] = L(w_0) + \frac{d}{2n} \]</span> <span class="math display">\[ \mathbb{E}[T_n] = L(w_0) - \frac{d}{2n} \]</span></p>
<p>In Singular Learning Theory, Watanabe elegantly proves a symmetry relation using <span class="math inline">\(\lambda\)</span>: <span class="math display">\[ \mathbb{E}[G_n] = L(w_0) + \frac{\lambda}{n} \]</span> <span class="math display">\[ \mathbb{E}[T_n] = L(w_0) - \frac{\lambda}{n} \]</span></p>
<p>For our continuous GMM, <span class="math inline">\(\lambda = 1/2\)</span>. Therefore:</p>
<ul>
<li>Expected Generalization Error converges with rate <span class="math inline">\(\frac{0.5}{n}\)</span>.</li>
<li>Expected Training Error converges with rate <span class="math inline">\(-\frac{0.5}{n}\)</span>.</li>
</ul>
<p>This explains why heavily overparameterized singular models (like deep neural networks and complex mixtures) often generalize better than their parameter count <span class="math inline">\(d\)</span> would imply; their learning dynamics are constrained by the smaller geometric invariant <span class="math inline">\(\lambda &lt; d/2\)</span>.</p>
<hr>
</section>
<section id="chapter-6.-asymptotic-expansion-and-waic" class="level2">
<h2 class="anchored" data-anchor-id="chapter-6.-asymptotic-expansion-and-waic">Chapter 6. Asymptotic Expansion and WAIC</h2>
<p>Finally, Chapter 6 resolves a crucial practical problem. Since calculating <span class="math inline">\(\lambda\)</span> analytically requires algebraic blow-ups (which is impossible for massive modern models like LLMs), we cannot use it directly to estimate the generalization error. Furthermore, cross-validation can be unstable in singular models.</p>
<p>Watanabe introduces the <strong>Widely Applicable Information Criterion (WAIC)</strong>: <span class="math display">\[ \text{WAIC} = T_n + \frac{1}{n} \sum_{i=1}^n V_w \left( \log p(X_i | w) \right) \]</span> Where <span class="math inline">\(V_w\)</span> is the posterior variance of the log-likelihood for data point <span class="math inline">\(X_i\)</span>.</p>
<p>The core theorem of Chapter 6 proves that WAIC is an asymptotically unbiased estimator of the generalization error, <strong>even for singular models</strong>: <span class="math display">\[ \mathbb{E}[\text{WAIC}] \approx \mathbb{E}[G_n] \]</span></p>
<p>In the context of our GMM, evaluating the posterior variance computationally via Markov Chain Monte Carlo (MCMC) allows us to estimate the true hold-out performance without needing to analytically find <span class="math inline">\(\lambda = 1/2\)</span> or <span class="math inline">\(m=2\)</span>, proving WAIC’s universal applicability in modern deep learning and mixture modeling.</p>
<section id="calculating-waic-in-practice" class="level3">
<h3 class="anchored" data-anchor-id="calculating-waic-in-practice">Calculating WAIC in Practice</h3>
<p>To make this concrete, let’s generate a synthetic dataset from the true distribution <span class="math inline">\(q(x) = \mathcal{N}(0, 1)\)</span> and compute the WAIC for our toy GMM. By using a fine grid covering our parameter space <span class="math inline">\(W = [0, 1] \times [-2, 2]\)</span>, we can exactly compute the posterior and WAIC without relying on intricate MCMC setups.</p>
<div id="waic-calculation" class="cell" data-execution_count="5">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Generate synthetic data from true distribution q(x) = N(0,1)</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="dv">1</span>, n)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Define parameter grid for evaluating the posterior</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>a_vals <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">100</span>)</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>mu_vals <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">100</span>)</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>A, MU <span class="op">=</span> np.meshgrid(a_vals, mu_vals)</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="co"># p(x|a, mu) = (1-a)*N(x|0,1) + a*N(x|mu,1)</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> norm_pdf(x, m, s):</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.exp(<span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> ((x <span class="op">-</span> m) <span class="op">/</span> s)<span class="op">**</span><span class="dv">2</span>) <span class="op">/</span> (np.sqrt(<span class="dv">2</span> <span class="op">*</span> np.pi) <span class="op">*</span> s)</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Compute log-likelihood for each parameter pair and each data point</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>log_p <span class="op">=</span> np.zeros((n, <span class="bu">len</span>(mu_vals), <span class="bu">len</span>(a_vals)))</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, x <span class="kw">in</span> <span class="bu">enumerate</span>(X):</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>    p_x_w <span class="op">=</span> (<span class="dv">1</span> <span class="op">-</span> A) <span class="op">*</span> norm_pdf(x, <span class="dv">0</span>, <span class="dv">1</span>) <span class="op">+</span> A <span class="op">*</span> norm_pdf(x, MU, <span class="dv">1</span>)</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add a small epsilon to prevent log(0)</span></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>    log_p[i] <span class="op">=</span> np.log(p_x_w <span class="op">+</span> <span class="fl">1e-12</span>)</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. Compute unnormalized posterior probabilities (assuming uniform prior)</span></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Sum over all data points to get the full log-likelihood for each parameter pair</span></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>total_log_likelihood <span class="op">=</span> np.<span class="bu">sum</span>(log_p, axis<span class="op">=</span><span class="dv">0</span>) </span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Subtract maximum for numerical stability before exponentiating</span></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>log_posterior <span class="op">=</span> total_log_likelihood <span class="op">-</span> np.<span class="bu">max</span>(total_log_likelihood)</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>posterior <span class="op">=</span> np.exp(log_posterior)</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Normalize to create a valid probability distribution over the grid</span></span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>posterior <span class="op">/=</span> np.<span class="bu">sum</span>(posterior) </span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a><span class="co"># 5. Calculate WAIC</span></span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>predictive_density <span class="op">=</span> np.zeros(n)</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>V_w <span class="op">=</span> np.zeros(n)</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Expected volume: E_w[p(X_i|w)]</span></span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a>    predictive_density[i] <span class="op">=</span> np.<span class="bu">sum</span>(np.exp(log_p[i]) <span class="op">*</span> posterior)</span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Posterior variance of the log-likelihood: V_w(log p(X_i|w))</span></span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a>    expected_log_p <span class="op">=</span> np.<span class="bu">sum</span>(log_p[i] <span class="op">*</span> posterior)</span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a>    expected_log_p_sq <span class="op">=</span> np.<span class="bu">sum</span>((log_p[i]<span class="op">**</span><span class="dv">2</span>) <span class="op">*</span> posterior)</span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a>    V_w[i] <span class="op">=</span> expected_log_p_sq <span class="op">-</span> expected_log_p<span class="op">**</span><span class="dv">2</span></span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a><span class="co"># T_n: Empirical training loss of the Bayes predictive distribution</span></span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a>T_n <span class="op">=</span> <span class="op">-</span><span class="fl">1.0</span> <span class="op">/</span> n <span class="op">*</span> np.<span class="bu">sum</span>(np.log(predictive_density <span class="op">+</span> <span class="fl">1e-12</span>))</span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a><span class="co"># Functional variance penalty term</span></span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a><span class="co"># This corresponds to (1/n) * sum(V_w)</span></span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a>penalty <span class="op">=</span> np.mean(V_w)</span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-53"><a href="#cb4-53" aria-hidden="true" tabindex="-1"></a>waic <span class="op">=</span> T_n <span class="op">+</span> penalty</span>
<span id="cb4-54"><a href="#cb4-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-55"><a href="#cb4-55" aria-hidden="true" tabindex="-1"></a><span class="co"># 6. Calculate True Generalization Error on massive holdout</span></span>
<span id="cb4-56"><a href="#cb4-56" aria-hidden="true" tabindex="-1"></a>n_test <span class="op">=</span> <span class="dv">20000</span></span>
<span id="cb4-57"><a href="#cb4-57" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="dv">1</span>, n_test)</span>
<span id="cb4-58"><a href="#cb4-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-59"><a href="#cb4-59" aria-hidden="true" tabindex="-1"></a>pred_dens_test <span class="op">=</span> np.zeros(n_test)</span>
<span id="cb4-60"><a href="#cb4-60" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">2000</span></span>
<span id="cb4-61"><a href="#cb4-61" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> b <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, n_test, batch_size):</span>
<span id="cb4-62"><a href="#cb4-62" aria-hidden="true" tabindex="-1"></a>    end_idx <span class="op">=</span> <span class="bu">min</span>(b <span class="op">+</span> batch_size, n_test)</span>
<span id="cb4-63"><a href="#cb4-63" aria-hidden="true" tabindex="-1"></a>    X_batch <span class="op">=</span> X_test[b:end_idx]</span>
<span id="cb4-64"><a href="#cb4-64" aria-hidden="true" tabindex="-1"></a>    X_expanded <span class="op">=</span> X_batch[:, np.newaxis, np.newaxis]</span>
<span id="cb4-65"><a href="#cb4-65" aria-hidden="true" tabindex="-1"></a>    p_batch <span class="op">=</span> (<span class="dv">1</span> <span class="op">-</span> A) <span class="op">*</span> norm_pdf(X_expanded, <span class="dv">0</span>, <span class="dv">1</span>) <span class="op">+</span> A <span class="op">*</span> norm_pdf(X_expanded, MU, <span class="dv">1</span>)</span>
<span id="cb4-66"><a href="#cb4-66" aria-hidden="true" tabindex="-1"></a>    expected_p <span class="op">=</span> np.<span class="bu">sum</span>(p_batch <span class="op">*</span> posterior, axis<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb4-67"><a href="#cb4-67" aria-hidden="true" tabindex="-1"></a>    pred_dens_test[b:end_idx] <span class="op">=</span> expected_p</span>
<span id="cb4-68"><a href="#cb4-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-69"><a href="#cb4-69" aria-hidden="true" tabindex="-1"></a>gen_error <span class="op">=</span> <span class="op">-</span><span class="fl">1.0</span> <span class="op">/</span> n_test <span class="op">*</span> np.<span class="bu">sum</span>(np.log(pred_dens_test <span class="op">+</span> <span class="fl">1e-12</span>))</span>
<span id="cb4-70"><a href="#cb4-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-71"><a href="#cb4-71" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Sample size n = </span><span class="sc">{</span>n<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb4-72"><a href="#cb4-72" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Training Error (T_n) = </span><span class="sc">{</span>T_n<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb4-73"><a href="#cb4-73" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Penalty Term (V / n) = </span><span class="sc">{</span>penalty<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb4-74"><a href="#cb4-74" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"WAIC = </span><span class="sc">{</span>waic<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb4-75"><a href="#cb4-75" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Generalization Error = </span><span class="sc">{</span>gen_error<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Sample size n = 100
Training Error (T_n) = 1.3310
Penalty Term (V / n) = 0.0057
WAIC = 1.3367
Generalization Error = 1.4243</code></pre>
</div>
</div>
</section>
<section id="visualizing-the-posterior" class="level3">
<h3 class="anchored" data-anchor-id="visualizing-the-posterior">Visualizing the Posterior</h3>
<p>We can also visualize the posterior distribution over our parameter grid to see how it concentrates around the true parameters. Because this is a singular model, the asymptotic distribution does not look like a neat Gaussian blob—instead, it concentrates along the non-identifiable manifold (the cross at <span class="math inline">\(a=0\)</span> or <span class="math inline">\(\mu=0\)</span>).</p>
<div id="cell-fig-posterior" class="cell" data-execution_count="6">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>contour <span class="op">=</span> plt.contourf(A, MU, posterior, levels<span class="op">=</span><span class="dv">20</span>, cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>plt.colorbar(contour, label<span class="op">=</span><span class="st">'Posterior Probability'</span>)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>plt.axhline(<span class="dv">0</span>, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="st">'True Parameters ($a=0$)'</span>)</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>plt.axvline(<span class="dv">0</span>, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="vs">r'True Parameters </span><span class="kw">(</span><span class="dv">$\m</span><span class="vs">u=0</span><span class="dv">$</span><span class="kw">)</span><span class="vs">'</span>)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Posterior Distribution of GMM Parameters"</span>)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Parameter $a$"</span>)</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r"Parameter </span><span class="dv">$\m</span><span class="vs">u</span><span class="dv">$</span><span class="vs">"</span>)</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-posterior" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-posterior-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="slt_gaussian_mixture_files/figure-html/fig-posterior-output-1.png" width="749" height="566" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-posterior-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Posterior Distribution of GMM Parameters. Notice how the probability mass is drawn out along the axes representing the true parameters.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="contrast-asymptotic-regularity-vs.-finite-sample-singularity" class="level3">
<h3 class="anchored" data-anchor-id="contrast-asymptotic-regularity-vs.-finite-sample-singularity">Contrast: Asymptotic Regularity vs.&nbsp;Finite-Sample Singularity</h3>
<p>What happens if the true distribution <span class="math inline">\(q(x)\)</span> is actually drawn from a genuine two-component mixture with <strong>both <span class="math inline">\(a \neq 0\)</span> and <span class="math inline">\(\mu \neq 0\)</span></strong>? For example, setting <span class="math inline">\(a=0.5\)</span> and <span class="math inline">\(\mu=0.3\)</span>.</p>
<p>In this case, the true parameters <span class="math inline">\((a_0, \mu_0)\)</span> lie in the interior of the parameter space, and there is exactly one parameter combination that matches the distribution. The true parameter space shrinks to a single, unique point. Because the true parameters are uniquely identifiable, the log-likelihood function has a strict minimum at <span class="math inline">\((a_0, \mu_0)\)</span>, and the Hessian (Fisher Information Matrix) becomes positive definite. <strong>Asymptotically (<span class="math inline">\(n \to \infty\)</span>), the model behaves as a standard regular model</strong>.</p>
<p>In this theoretical asymptotic scenario: 1. The Real Log Canonical Threshold becomes <span class="math inline">\(\lambda = d/2 = 2/2 = 1.0\)</span>. 2. The multiplicity is <span class="math inline">\(m = 1\)</span>. 3. The Bayesian Occam’s Razor penalty term in WAIC should converge to roughly <span class="math inline">\(1.0\times (\log n)/n\)</span> for free energy, or <span class="math inline">\(1.0/n\)</span> for generalization error. We expect the functional variance <span class="math inline">\(V\)</span> to approximate this <span class="math inline">\(1.0\)</span> factor (rather than <span class="math inline">\(0.5\)</span>).</p>
<p>Let’s test this by running our WAIC code again, but generating the data from <span class="math inline">\(a=0.5\)</span> and <span class="math inline">\(\mu=0.3\)</span>:</p>
<div id="cell-waic-regular-case" class="cell" data-execution_count="7">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Generate synthetic data from a true mixture model</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>n_reg <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>a_true <span class="op">=</span> <span class="fl">0.5</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>mu_true <span class="op">=</span> <span class="fl">0.3</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> np.random.binomial(<span class="dv">1</span>, a_true, n_reg)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>X_reg <span class="op">=</span> np.zeros(n_reg)</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>X_reg[z <span class="op">==</span> <span class="dv">0</span>] <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="dv">1</span>, np.<span class="bu">sum</span>(z <span class="op">==</span> <span class="dv">0</span>))</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>X_reg[z <span class="op">==</span> <span class="dv">1</span>] <span class="op">=</span> np.random.normal(mu_true, <span class="dv">1</span>, np.<span class="bu">sum</span>(z <span class="op">==</span> <span class="dv">1</span>))</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Compute log-likelihood</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>log_p_reg <span class="op">=</span> np.zeros((n_reg, <span class="bu">len</span>(mu_vals), <span class="bu">len</span>(a_vals)))</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, x <span class="kw">in</span> <span class="bu">enumerate</span>(X_reg):</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>    p_x_w <span class="op">=</span> (<span class="dv">1</span> <span class="op">-</span> A) <span class="op">*</span> norm_pdf(x, <span class="dv">0</span>, <span class="dv">1</span>) <span class="op">+</span> A <span class="op">*</span> norm_pdf(x, MU, <span class="dv">1</span>)</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>    log_p_reg[i] <span class="op">=</span> np.log(p_x_w <span class="op">+</span> <span class="fl">1e-12</span>)</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Compute posterior</span></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>total_log_likelihood_reg <span class="op">=</span> np.<span class="bu">sum</span>(log_p_reg, axis<span class="op">=</span><span class="dv">0</span>) </span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>log_posterior_reg <span class="op">=</span> total_log_likelihood_reg <span class="op">-</span> np.<span class="bu">max</span>(total_log_likelihood_reg)</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>posterior_reg <span class="op">=</span> np.exp(log_posterior_reg)</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>posterior_reg <span class="op">/=</span> np.<span class="bu">sum</span>(posterior_reg) </span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. Calculate WAIC penalty</span></span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>predictive_density_reg <span class="op">=</span> np.zeros(n_reg)</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>V_w_reg <span class="op">=</span> np.zeros(n_reg)</span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_reg):</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>    predictive_density_reg[i] <span class="op">=</span> np.<span class="bu">sum</span>(np.exp(log_p_reg[i]) <span class="op">*</span> posterior_reg)</span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>    expected_log_p <span class="op">=</span> np.<span class="bu">sum</span>(log_p_reg[i] <span class="op">*</span> posterior_reg)</span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>    expected_log_p_sq <span class="op">=</span> np.<span class="bu">sum</span>((log_p_reg[i]<span class="op">**</span><span class="dv">2</span>) <span class="op">*</span> posterior_reg)</span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a>    V_w_reg[i] <span class="op">=</span> expected_log_p_sq <span class="op">-</span> expected_log_p<span class="op">**</span><span class="dv">2</span></span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a>T_n_reg <span class="op">=</span> <span class="op">-</span><span class="fl">1.0</span> <span class="op">/</span> n_reg <span class="op">*</span> np.<span class="bu">sum</span>(np.log(predictive_density_reg <span class="op">+</span> <span class="fl">1e-12</span>))</span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a>penalty_reg <span class="op">=</span> np.mean(V_w_reg)</span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a>waic_reg <span class="op">=</span> T_n_reg <span class="op">+</span> penalty_reg</span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a><span class="co"># 5. Calculate True Generalization Error</span></span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a>n_test <span class="op">=</span> <span class="dv">20000</span></span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a>z_test <span class="op">=</span> np.random.binomial(<span class="dv">1</span>, a_true, n_test)</span>
<span id="cb7-40"><a href="#cb7-40" aria-hidden="true" tabindex="-1"></a>X_test_reg <span class="op">=</span> np.zeros(n_test)</span>
<span id="cb7-41"><a href="#cb7-41" aria-hidden="true" tabindex="-1"></a>X_test_reg[z_test <span class="op">==</span> <span class="dv">0</span>] <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="dv">1</span>, np.<span class="bu">sum</span>(z_test <span class="op">==</span> <span class="dv">0</span>))</span>
<span id="cb7-42"><a href="#cb7-42" aria-hidden="true" tabindex="-1"></a>X_test_reg[z_test <span class="op">==</span> <span class="dv">1</span>] <span class="op">=</span> np.random.normal(mu_true, <span class="dv">1</span>, np.<span class="bu">sum</span>(z_test <span class="op">==</span> <span class="dv">1</span>))</span>
<span id="cb7-43"><a href="#cb7-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-44"><a href="#cb7-44" aria-hidden="true" tabindex="-1"></a>pred_dens_test_reg <span class="op">=</span> np.zeros(n_test)</span>
<span id="cb7-45"><a href="#cb7-45" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">2000</span></span>
<span id="cb7-46"><a href="#cb7-46" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> b <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, n_test, batch_size):</span>
<span id="cb7-47"><a href="#cb7-47" aria-hidden="true" tabindex="-1"></a>    end_idx <span class="op">=</span> <span class="bu">min</span>(b <span class="op">+</span> batch_size, n_test)</span>
<span id="cb7-48"><a href="#cb7-48" aria-hidden="true" tabindex="-1"></a>    X_batch <span class="op">=</span> X_test_reg[b:end_idx]</span>
<span id="cb7-49"><a href="#cb7-49" aria-hidden="true" tabindex="-1"></a>    X_expanded <span class="op">=</span> X_batch[:, np.newaxis, np.newaxis]</span>
<span id="cb7-50"><a href="#cb7-50" aria-hidden="true" tabindex="-1"></a>    p_batch <span class="op">=</span> (<span class="dv">1</span> <span class="op">-</span> A) <span class="op">*</span> norm_pdf(X_expanded, <span class="dv">0</span>, <span class="dv">1</span>) <span class="op">+</span> A <span class="op">*</span> norm_pdf(X_expanded, MU, <span class="dv">1</span>)</span>
<span id="cb7-51"><a href="#cb7-51" aria-hidden="true" tabindex="-1"></a>    expected_p <span class="op">=</span> np.<span class="bu">sum</span>(p_batch <span class="op">*</span> posterior_reg, axis<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb7-52"><a href="#cb7-52" aria-hidden="true" tabindex="-1"></a>    pred_dens_test_reg[b:end_idx] <span class="op">=</span> expected_p</span>
<span id="cb7-53"><a href="#cb7-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-54"><a href="#cb7-54" aria-hidden="true" tabindex="-1"></a>gen_error_reg <span class="op">=</span> <span class="op">-</span><span class="fl">1.0</span> <span class="op">/</span> n_test <span class="op">*</span> np.<span class="bu">sum</span>(np.log(pred_dens_test_reg <span class="op">+</span> <span class="fl">1e-12</span>))</span>
<span id="cb7-55"><a href="#cb7-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-56"><a href="#cb7-56" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Sample size n = </span><span class="sc">{</span>n_reg<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb7-57"><a href="#cb7-57" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Training Error (T_n) = </span><span class="sc">{</span>T_n_reg<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb7-58"><a href="#cb7-58" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Regular Penalty Term (V / n) = </span><span class="sc">{</span>penalty_reg<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb7-59"><a href="#cb7-59" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"WAIC = </span><span class="sc">{</span>waic_reg<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb7-60"><a href="#cb7-60" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Generalization Error = </span><span class="sc">{</span>gen_error_reg<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb7-61"><a href="#cb7-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-62"><a href="#cb7-62" aria-hidden="true" tabindex="-1"></a><span class="co"># 6. Plot the new regular posterior</span></span>
<span id="cb7-63"><a href="#cb7-63" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">5</span>))</span>
<span id="cb7-64"><a href="#cb7-64" aria-hidden="true" tabindex="-1"></a>contour3 <span class="op">=</span> plt.contourf(A, MU, posterior_reg, levels<span class="op">=</span><span class="dv">20</span>, cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb7-65"><a href="#cb7-65" aria-hidden="true" tabindex="-1"></a>plt.colorbar(contour3, label<span class="op">=</span><span class="st">'Posterior Probability'</span>)</span>
<span id="cb7-66"><a href="#cb7-66" aria-hidden="true" tabindex="-1"></a>plt.scatter([a_true], [mu_true], color<span class="op">=</span><span class="st">'red'</span>, marker<span class="op">=</span><span class="st">'x'</span>, s<span class="op">=</span><span class="dv">100</span>, linewidths<span class="op">=</span><span class="dv">3</span>, label<span class="op">=</span><span class="st">'True Parameters'</span>)</span>
<span id="cb7-67"><a href="#cb7-67" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Posterior Distribution (Regular Model)"</span>)</span>
<span id="cb7-68"><a href="#cb7-68" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Parameter $a$"</span>)</span>
<span id="cb7-69"><a href="#cb7-69" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r"Parameter </span><span class="dv">$\m</span><span class="vs">u</span><span class="dv">$</span><span class="vs">"</span>)</span>
<span id="cb7-70"><a href="#cb7-70" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb7-71"><a href="#cb7-71" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb7-72"><a href="#cb7-72" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Sample size n = 100
Training Error (T_n) = 1.3483
Regular Penalty Term (V / n) = 0.0073
WAIC = 1.3556
Generalization Error = 1.4317</code></pre>
</div>
<div class="cell-output cell-output-display">
<div id="waic-regular-case" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="slt_gaussian_mixture_files/figure-html/waic-regular-case-output-2.png" width="656" height="470" class="figure-img"></p>
<figcaption>Posterior Distribution for the ‘Regular’ Case with Finite Sample Size. Because the true parameter <span class="math inline">\(\mu=0.3\)</span> is very close to the singularity <span class="math inline">\(\mu=0\)</span>, the sample size <span class="math inline">\(n=100\)</span> is not large enough to resolve the parameters perfectly. The posterior is gravitationally pulled into the singularity, maintaining the shape and WAIC penalty of a singular model.</figcaption>
</figure>
</div>
</div>
</div>
<section id="the-gravitational-pull-of-the-singularity" class="level4">
<h4 class="anchored" data-anchor-id="the-gravitational-pull-of-the-singularity">The Gravitational Pull of the Singularity</h4>
<p>Notice what happened in the plot and output! Despite the true model being theoretically regular, the penalty term <span class="math inline">\(V \approx 0.73\)</span> is far closer to the singular penalty <span class="math inline">\(0.5\)</span> than the regular penalty <span class="math inline">\(1.0\)</span>. And the posterior clearly does not look like a neat Gaussian blob; it is smeared out along the <span class="math inline">\(\mu=0\)</span> axis exactly like the singular case.</p>
<p>Why? Because the true parameter <span class="math inline">\(\mu=0.3\)</span> is <strong>extremely close to the singular manifold</strong> (<span class="math inline">\(\mu=0\)</span>). Watanabe’s theory is asymptotic (<span class="math inline">\(n \to \infty\)</span>). For a finite sample size of <span class="math inline">\(n=100\)</span>, the data simply does not have enough resolution to confidently distinguish the true distribution <span class="math inline">\(\mu=0.3\)</span> from the singularity at <span class="math inline">\(\mu=0\)</span>. The likelihood spills over into the singularity, and the posterior is “gravitationally pulled” into the non-identifiable manifold, forcing the model to behave like a singular machine. It would take a much larger sample size (e.g., <span class="math inline">\(n=10,000\)</span>) for the regular asymptotic theory to overcome the geometry of the singularity locally.</p>
<p>This is a profound realization from Singular Learning Theory: singularities govern the learning dynamics of models in practice, <strong>even when the true distribution is slightly off the singularity</strong>, because finite samples cannot perfectly resolve the identifiability.</p>
<p>The mathematical beauty of Chapter 6 is that <strong>WAIC works uniformly in both these cases without adjustment</strong>. The functional variance inherently adapts to the finite-sample geometry of the posterior, providing an accurate estimator of the true generalization error regardless of whether the model is trapped in a singularity or has broken free!</p>
</section>
</section>
<section id="is-waic-still-good-for-small-samples" class="level3">
<h3 class="anchored" data-anchor-id="is-waic-still-good-for-small-samples">Is WAIC Still Good for Small Samples?</h3>
<p>A natural follow-up question is whether WAIC remains a good estimator of generalization error when the sample size <span class="math inline">\(n\)</span> is very small. <strong>The short answer is yes</strong>, and this is exactly where WAIC shines compared to classical criteria like the Deviance Information Criterion (DIC).</p>
<p>Classical criteria like DIC rely on a <em>point estimate</em> of the parameter (usually the posterior mean or mode). In a singular model, the posterior is highly non-Gaussian and often stretched along a non-identifiable manifold (like the cross at <span class="math inline">\(\mu=0\)</span> or <span class="math inline">\(a=0\)</span>). For a small sample, the posterior mean might land in a region of very low actual probability mass (e.g., in the empty space between two dense arms of the posterior). Because DIC evaluates the likelihood at this single mathematically awkward point, it can wildly misestimate the generalization error.</p>
<p>WAIC, on the other hand, averages the likelihood over the <em>entire</em> posterior distribution and calculates the functional variance point-by-point. It entirely avoids point estimation.</p>
</section>
</section>
<section id="chapter7.-gibbs-sampler-for-gaussian-mixture" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="chapter7.-gibbs-sampler-for-gaussian-mixture">Chapter7. Gibbs Sampler for Gaussian Mixture</h2>
<p>A Gibbs sampler is often employed in mixture models. Let us derive a Gibbs sampler for the two-component Gaussian mixture model defined by <span class="math display">\[
p(x|a, b) = a \mathcal{N}(x|b_1) + (1-a) \mathcal{N}(x|b_2)
\]</span> where <span class="math inline">\(a \in [0, 1]\)</span> and <span class="math inline">\(b = (b_1, b_2)\)</span> are parameters. For the prior, we adopt a Beta distribution with index <span class="math inline">\((\alpha, \beta)\)</span> and a normal distribution: <span class="math display">\[
\varphi(a) = \frac{1}{z_1} a^{\alpha - 1} (1-a)^{\beta - 1}
\]</span> <span class="math display">\[
\varphi(b) = \frac{1}{z_2} \exp\left(-\frac{b_1^2 + b_2^2}{2\sigma^2}\right)
\]</span> Let <span class="math inline">\(y \in \{0, 1\}\)</span> be a latent competitive variable indicating the component membership. Then a normal mixture <span class="math inline">\(p(x|a, b)\)</span> can be understood as a statistical model <span class="math display">\[
p(x, y|a, b) = (a \mathcal{N}(x|b_1))^{y} ((1-a) \mathcal{N}(x|b_2))^{1-y}
\]</span> It follows that <span class="math display">\[
p(x|a, b) = \sum_{y \in \{0, 1\}} p(x, y|a, b)
\]</span> Therefore a normal mixture <span class="math inline">\(p(x|a, b)\)</span> can be understood as a statistical model <span class="math inline">\(p(x, y|a, b)\)</span> which has a latent or hidden variable <span class="math inline">\(y \in \{0, 1\}\)</span>.</p>
<p>Let <span class="math inline">\(x^n = \{x_1, x_2, \dots, x_n\}\)</span> be an independent sample and <span class="math inline">\(y_i \in \{0, 1\}\)</span> be the hidden variable which corresponds to a sample point <span class="math inline">\(x_i\)</span>. We use a notation <span class="math inline">\(y^n = \{y_1, y_2, \dots, y_n\}\)</span>. Then the joint probability is <span class="math display">\[
p(a, b, x^n, y^n) = \frac{1}{Z} \left(a^{\alpha - 1 + n_1} \exp(-H_1(b_1))\right) \left((1-a)^{\beta - 1 + n_2} \exp(-H_2(b_2))\right)
\]</span> where <span class="math inline">\(Z\)</span> is a normalizing constant and <span class="math display">\[
n_1 = \sum_{i=1}^n y_i, \quad n_2 = \sum_{i=1}^n (1 - y_i) = n - n_1
\]</span> <span class="math display">\[
H_1(b_1) = \frac{b_1^2}{2\sigma^2} + \frac{1}{2} \sum_{i=1}^n y_i (x_i - b_1)^2
\]</span> <span class="math display">\[
H_2(b_2) = \frac{b_2^2}{2\sigma^2} + \frac{1}{2} \sum_{i=1}^n (1-y_i) (x_i - b_2)^2
\]</span></p>
<p>In Bayesian estimation, we need the posterior parameters <span class="math inline">\((a, b)\)</span> which are subject to <span class="math inline">\(p(a, b|x^n)\)</span>. It is sufficient to make a Gibbs sampler for <span class="math inline">\((a, b)\)</span> and <span class="math inline">\(y^n\)</span>, in which the conditional probabilities we need are <span class="math inline">\(p(y^n|a, b, x^n)\)</span> and <span class="math inline">\(p(a, b|x^n, y^n)\)</span>.</p>
<p>Under the probability distribution <span class="math inline">\(p(y^n|a, b, x^n)\)</span> the <span class="math inline">\(y_1, y_2, \dots, y_n\)</span> are independent and, for each <span class="math inline">\(i\)</span>: <span class="math display">\[
p(y_i = 1 | a, b, x_i) = \frac{a \mathcal{N}(x_i|b_1)}{a \mathcal{N}(x_i|b_1) + (1-a) \mathcal{N}(x_i|b_2)}
\]</span></p>
<p>On the other hand, if <span class="math inline">\((a, b_1, b_2)\)</span> is subject to the probability distribution <span class="math inline">\(p(a, b|x^n, y^n)\)</span>, they are independent. Hence <span class="math display">\[
p(a, b|x^n, y^n) = p(a|x^n, y^n) p(b_1|x^n, y^n) p(b_2|x^n, y^n)
\]</span> The variable <span class="math inline">\(a\)</span> is subject to the Beta distribution with index <span class="math inline">\(\alpha + n_1\)</span> and <span class="math inline">\(\beta + n_2\)</span>: <span class="math display">\[
p(a|x^n, y^n) \propto a^{\alpha + n_1 - 1} (1-a)^{\beta + n_2 - 1}
\]</span></p>
<p>The variable <span class="math inline">\(b_1\)</span> and <span class="math inline">\(b_2\)</span> are subject to the normal distribution with average <span class="math inline">\(b_k^*\)</span> and variance <span class="math inline">\((\sigma_k^*)^2\)</span>: <span class="math display">\[
p(b_k|x^n, y^n) = \mathcal{N}(b_k^*, (\sigma_k^*)^2) \quad \text{for } k=1, 2
\]</span> where <span class="math display">\[
b_1^* = \frac{\sum_{i=1}^n y_i x_i}{1/\sigma^2 + n_1}, \quad b_2^* = \frac{\sum_{i=1}^n (1-y_i) x_i}{1/\sigma^2 + n_2}
\]</span> <span class="math display">\[
(\sigma_1^*)^2 = \frac{1}{1/\sigma^2 + n_1}, \quad (\sigma_2^*)^2 = \frac{1}{1/\sigma^2 + n_2}
\]</span></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Derivation of the Normal Distribution Parameters for <span class="math inline">\(b_1\)</span> and <span class="math inline">\(b_2\)</span>
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>From the joint probability, the full conditional posterior for <span class="math inline">\(b_1\)</span> is proportional to the <span class="math inline">\(\exp(-H_1(b_1))\)</span> term: <span class="math display">\[
p(b_1|x^n, y^n) \propto \exp(-H_1(b_1))
\]</span> Expanding <span class="math inline">\(H_1(b_1)\)</span>: <span class="math display">\[
H_1(b_1) = \frac{b_1^2}{2\sigma^2} + \frac{1}{2} \sum_{i=1}^n y_i (x_i - b_1)^2 = \frac{b_1^2}{2\sigma^2} + \frac{1}{2} \sum_{i=1}^n y_i (x_i^2 - 2x_i b_1 + b_1^2)
\]</span> Grouping the terms by powers of <span class="math inline">\(b_1\)</span> and ignoring terms that do not depend on <span class="math inline">\(b_1\)</span> (which get absorbed into the normalization constant): <span class="math display">\[
H_1(b_1) = \frac{1}{2} \left[ \left(\frac{1}{\sigma^2} + \sum_{i=1}^n y_i\right) b_1^2 - 2 \left(\sum_{i=1}^n y_i x_i\right) b_1 \right] + \text{const}
\]</span> Using <span class="math inline">\(n_1 = \sum_{i=1}^n y_i\)</span>, we can complete the square to match the form of a Gaussian exponent <span class="math inline">\(\frac{1}{2(\sigma_1^*)^2} (b_1 - b_1^*)^2\)</span>: <span class="math display">\[
\frac{1}{2(\sigma_1^*)^2} = \frac{1}{2} \left(\frac{1}{\sigma^2} + n_1\right) \implies (\sigma_1^*)^2 = \frac{1}{1/\sigma^2 + n_1}
\]</span> <span class="math display">\[
\frac{b_1^*}{(\sigma_1^*)^2} = \sum_{i=1}^n y_i x_i \implies b_1^* = \frac{\sum_{i=1}^n y_i x_i}{1/\sigma^2 + n_1}
\]</span> The exact same algebraic steps apply to <span class="math inline">\(H_2(b_2)\)</span>, swapping <span class="math inline">\(y_i\)</span> for <span class="math inline">\((1-y_i)\)</span> and <span class="math inline">\(n_1\)</span> for <span class="math inline">\(n_2\)</span>.</p>
</div>
</div>
</div>
<p>Hence the algorithm for the Gibbs sampler is defined by the following steps:</p>
<ol type="1">
<li>A parameter set <span class="math inline">\((a, b_1, b_2)\)</span> is initialized.</li>
<li>A set of hidden variables <span class="math inline">\(\{y_i\} \in \{0, 1\}^n\)</span> is drawn from the probability distribution <span class="math display">\[
P(y_i = 1|a, b, x_i) = \frac{a \mathcal{N}(x_i|b_1)}{a \mathcal{N}(x_i|b_1) + (1-a) \mathcal{N}(x_i|b_2)}
\]</span></li>
<li>Using <span class="math inline">\(n_1 = \sum_{i=1}^n y_i\)</span> and <span class="math inline">\(n_2 = n - n_1\)</span>, a parameter <span class="math inline">\(a\)</span> is drawn from the Beta distribution: <span class="math display">\[
a \sim \text{Beta}(\alpha + n_1, \beta + n_2)
\]</span></li>
<li>Parameters <span class="math inline">\(b_1\)</span> and <span class="math inline">\(b_2\)</span> are drawn from the normal distributions <span class="math display">\[
b_k \sim \mathcal{N}(b_k^*, (\sigma_k^*)^2)
\]</span> for <span class="math inline">\(k=1, 2\)</span>.</li>
<li>Return to (2).</li>
</ol>
<div id="21d27e71" class="cell" data-execution_count="8">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> norm_pdf(x, mean, std<span class="op">=</span><span class="fl">1.0</span>):</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.exp(<span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> ((x <span class="op">-</span> mean) <span class="op">/</span> std)<span class="op">**</span><span class="dv">2</span>) <span class="op">/</span> (std <span class="op">*</span> np.sqrt(<span class="dv">2</span> <span class="op">*</span> np.pi))</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gibbs_sampler(X, num_iterations<span class="op">=</span><span class="dv">4500</span>, alpha<span class="op">=</span><span class="fl">1.0</span>, beta<span class="op">=</span><span class="fl">1.0</span>, sigma2<span class="op">=</span><span class="fl">1.0</span>):</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> <span class="bu">len</span>(X)</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>    a, b1, b2 <span class="op">=</span> <span class="fl">0.5</span>, <span class="fl">1.0</span>, <span class="op">-</span><span class="fl">1.0</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>    samples_a, samples_b1, samples_b2 <span class="op">=</span> (np.zeros(num_iterations) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">3</span>))</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(num_iterations):</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>        p1 <span class="op">=</span> a <span class="op">*</span> norm_pdf(X, b1) <span class="op">+</span> <span class="fl">1e-15</span></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>        p2 <span class="op">=</span> (<span class="dv">1</span> <span class="op">-</span> a) <span class="op">*</span> norm_pdf(X, b2) <span class="op">+</span> <span class="fl">1e-15</span></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> np.random.binomial(<span class="dv">1</span>, p1 <span class="op">/</span> (p1 <span class="op">+</span> p2))</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>        n1 <span class="op">=</span> np.<span class="bu">sum</span>(y)</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>        n2 <span class="op">=</span> n <span class="op">-</span> n1</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>        a <span class="op">=</span> np.random.beta(alpha <span class="op">+</span> n1, beta <span class="op">+</span> n2)</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>        var_b1 <span class="op">=</span> <span class="fl">1.0</span> <span class="op">/</span> (<span class="fl">1.0</span> <span class="op">/</span> sigma2 <span class="op">+</span> n1)</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>        b1 <span class="op">=</span> np.random.normal((np.<span class="bu">sum</span>(y <span class="op">*</span> X)) <span class="op">*</span> var_b1, np.sqrt(var_b1))</span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>        var_b2 <span class="op">=</span> <span class="fl">1.0</span> <span class="op">/</span> (<span class="fl">1.0</span> <span class="op">/</span> sigma2 <span class="op">+</span> n2)</span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>        b2 <span class="op">=</span> np.random.normal((np.<span class="bu">sum</span>((<span class="dv">1</span> <span class="op">-</span> y) <span class="op">*</span> X)) <span class="op">*</span> var_b2, np.sqrt(var_b2))</span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>        samples_a[t], samples_b1[t], samples_b2[t] <span class="op">=</span> a, b1, b2</span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> samples_a[<span class="dv">500</span>:], samples_b1[<span class="dv">500</span>:], samples_b2[<span class="dv">500</span>:]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
</div>
<p>As an example we use this Gibbs sampler to sample from the posterior resulting from distributions with true parameters <span class="math inline">\(b_1 = 0.2B\)</span> and <span class="math inline">\(b_2 = -0.2B\)</span>. As <span class="math inline">\(B\)</span> goes from <span class="math inline">\(4\)</span> down to <span class="math inline">\(1\)</span>, the components overlap more, and the posterior becomes singular.</p>
<div id="cell-gibbs-sampler-example-52" class="cell page-columns page-full" data-execution_count="9">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>B_values <span class="op">=</span> [<span class="dv">4</span>, <span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">1</span>]</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">10</span>))</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> idx, B <span class="kw">in</span> <span class="bu">enumerate</span>(B_values):</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>    b1_true, b2_true <span class="op">=</span> <span class="fl">0.2</span> <span class="op">*</span> B, <span class="op">-</span><span class="fl">0.2</span> <span class="op">*</span> B</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>    z <span class="op">=</span> np.random.binomial(<span class="dv">1</span>, <span class="fl">0.5</span>, size<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> np.where(z <span class="op">==</span> <span class="dv">1</span>, np.random.normal(b1_true, <span class="fl">1.0</span>, <span class="dv">100</span>), np.random.normal(b2_true, <span class="fl">1.0</span>, <span class="dv">100</span>))</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>    sa, sb1, sb2 <span class="op">=</span> gibbs_sampler(X)</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>    ax <span class="op">=</span> axes.flatten()[idx]</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>    ax.scatter(sb1, sb2, alpha<span class="op">=</span><span class="fl">0.1</span>, s<span class="op">=</span><span class="dv">5</span>, c<span class="op">=</span><span class="st">'blue'</span>, label<span class="op">=</span><span class="st">'Posterior Samples'</span>)</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>    ax.plot([b1_true, b2_true], [b2_true, b1_true], <span class="st">'wo'</span>, markeredgecolor<span class="op">=</span><span class="st">'black'</span>, markersize<span class="op">=</span><span class="dv">10</span>, label<span class="op">=</span><span class="st">'True Centers'</span>)</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>    ax.set_title(<span class="ss">f"B = </span><span class="sc">{</span>B<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>    ax.set_xlabel(<span class="st">"$b_1$"</span>)</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>    ax.set_ylabel(<span class="st">"$b_2$"</span>)</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>    limit <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>    ax.set_xlim(<span class="op">-</span>limit, limit)</span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>    ax.set_ylim(<span class="op">-</span>limit, limit)</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> idx <span class="op">==</span> <span class="dv">0</span>: ax.legend()</span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display page-columns page-full">
<div id="gibbs-sampler-example-52" class="quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="figure page-columns page-full">
<p class="page-columns page-full"><img src="slt_gaussian_mixture_files/figure-html/gibbs-sampler-example-52-output-1.png" width="948" height="949" class="figure-img column-page"></p>
<figcaption>Posterior parameter sample points of <span class="math inline">\(b_1\)</span> and <span class="math inline">\(b_2\)</span> for different true separation values <span class="math inline">\(B\)</span>. As <span class="math inline">\(B \le 2\)</span>, the posterior distribution loses clear separation and becomes singular.</figcaption>
</figure>
</div>
</div>
</div>
<section id="gibbs-sampler-vs.-standard-mcmc-metropolis-hastings" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="gibbs-sampler-vs.-standard-mcmc-metropolis-hastings">Gibbs Sampler vs.&nbsp;Standard MCMC (Metropolis-Hastings)</h3>
<p>Which sampling method is better? In Chapter 1, we used a standard Metropolis-Hastings (MH) algorithm to sample from the posterior of a Gaussian Mixture Model. In this chapter, we derived a Gibbs sampler for the exact same model. The choice between them comes with important trade-offs, especially in the context of mixture models and Singular Learning Theory.</p>
<p><strong>Advantages of the Gibbs Sampler:</strong></p>
<ol type="1">
<li><strong>No Step-Size Tuning:</strong> Standard MH requires tuning the proposal distribution variance (step size). If the steps are too small, the sampler moves too slowly; if they are too large, proposals are constantly rejected. Gibbs sampling draws directly from the exact conditional distributions, meaning every proposal is accepted (100% acceptance rate) without any tuning.</li>
<li><strong>Discrete Latent Variables:</strong> Gibbs sampling handles the discrete component assignment variables <span class="math inline">\(y_i \in \{0, 1\}\)</span> naturally. Continuous gradient-based MCMC methods (like Hamiltonian Monte Carlo) cannot easily handle discrete latent variables without marginalizing them out first.</li>
</ol>
<p><strong>Advantages of Standard MCMC (Metropolis-Hastings):</strong></p>
<ol type="1">
<li><strong>Generality:</strong> MH only requires that you can compute the log-likelihood <span class="math inline">\(p(x|w)\)</span> up to a constant. It applies universally to any model (neural networks, matrix factorization, etc.). Gibbs sampling requires you to explicitly derive the exact conditional distributions, which is only possible for models with <em>conjugate priors</em>.</li>
<li><strong>Direct Exploration of the Singular Manifold:</strong> SLT focuses on the continuous parameter space <span class="math inline">\(W\)</span> where <span class="math inline">\(w = (a, \mu)\)</span>. MH explores this 2D continuous space directly, moving across the singularities (like <span class="math inline">\(a=0\)</span> or <span class="math inline">\(\mu_1=\mu_2\)</span>) cleanly. A Gibbs sampler explores a much higher-dimensional space that includes all <span class="math inline">\(n\)</span> discrete hidden variables <span class="math inline">\((a, \mu, y_1, y_2, \dots, y_n)\)</span>. Near a singularity where components overlap, the latent assignments become completely unidentifiable, which often causes the Gibbs sampler’s Markov chain to become highly correlated and stall.</li>
<li><strong>Label Switching:</strong> Because the mixture components are symmetric, the true posterior has multiple identical modes. A Gibbs sampler will often get stuck in one mode, whereas a well-tuned MH sampler may transition between them more fluidly.</li>
</ol>
<p>Let us run both algorithms on the same dataset generated with <span class="math inline">\(B=2\)</span> and compare their behavior:</p>
<div id="cell-mcmc-vs-gibbs" class="cell page-columns page-full" data-cache="true" data-execution_count="10">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> log_prior(a, mu1, mu2, alpha<span class="op">=</span><span class="fl">1.0</span>, beta<span class="op">=</span><span class="fl">1.0</span>, sigma2<span class="op">=</span><span class="fl">1.0</span>):</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> (<span class="dv">0</span> <span class="op">&lt;</span> a <span class="op">&lt;</span> <span class="dv">1</span>): <span class="cf">return</span> <span class="op">-</span>np.inf</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (alpha <span class="op">-</span> <span class="dv">1</span>) <span class="op">*</span> np.log(a) <span class="op">+</span> (beta <span class="op">-</span> <span class="dv">1</span>) <span class="op">*</span> np.log(<span class="dv">1</span> <span class="op">-</span> a) <span class="op">-</span> <span class="fl">0.5</span> <span class="op">*</span> (mu1<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> mu2<span class="op">**</span><span class="dv">2</span>) <span class="op">/</span> sigma2</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mh_sampler(X, num_iterations<span class="op">=</span><span class="dv">2500</span>, alpha<span class="op">=</span><span class="fl">1.0</span>, beta_prior<span class="op">=</span><span class="fl">1.0</span>, sigma2<span class="op">=</span><span class="fl">1.0</span>):</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    a, b1, b2 <span class="op">=</span> <span class="fl">0.5</span>, <span class="fl">1.0</span>, <span class="op">-</span><span class="fl">1.0</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>    p1, p2 <span class="op">=</span> norm_pdf(X, b1), norm_pdf(X, b2)</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>    ll <span class="op">=</span> np.<span class="bu">sum</span>(np.log(a <span class="op">*</span> p1 <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> a) <span class="op">*</span> p2 <span class="op">+</span> <span class="fl">1e-15</span>)) <span class="op">+</span> log_prior(a, b1, b2, alpha, beta_prior, sigma2)</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>    samples_a, samples_b1, samples_b2 <span class="op">=</span> (np.zeros(num_iterations) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">3</span>))</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(num_iterations):</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>        a_prop <span class="op">=</span> a <span class="op">+</span> np.random.normal(<span class="dv">0</span>, <span class="fl">0.05</span>)</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>        b1_prop <span class="op">=</span> b1 <span class="op">+</span> np.random.normal(<span class="dv">0</span>, <span class="fl">0.2</span>)</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>        b2_prop <span class="op">=</span> b2 <span class="op">+</span> np.random.normal(<span class="dv">0</span>, <span class="fl">0.2</span>)</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> a_prop <span class="op">&lt;</span> <span class="dv">0</span>: a_prop <span class="op">=</span> <span class="op">-</span>a_prop</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> a_prop <span class="op">&gt;</span> <span class="dv">1</span>: a_prop <span class="op">=</span> <span class="dv">2</span> <span class="op">-</span> a_prop</span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>        p1_prop, p2_prop <span class="op">=</span> norm_pdf(X, b1_prop), norm_pdf(X, b2_prop)</span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a>        ll_prop <span class="op">=</span> np.<span class="bu">sum</span>(np.log(a_prop <span class="op">*</span> p1_prop <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> a_prop) <span class="op">*</span> p2_prop <span class="op">+</span> <span class="fl">1e-15</span>)) <span class="op">+</span> log_prior(a_prop, b1_prop, b2_prop, alpha, beta_prior, sigma2)</span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> np.log(np.random.rand()) <span class="op">&lt;</span> (ll_prop <span class="op">-</span> ll):</span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a>            a, b1, b2, ll <span class="op">=</span> a_prop, b1_prop, b2_prop, ll_prop</span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a>        samples_a[t], samples_b1[t], samples_b2[t] <span class="op">=</span> a, b1, b2</span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> samples_a[<span class="dv">500</span>:], samples_b1[<span class="dv">500</span>:], samples_b2[<span class="dv">500</span>:]</span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a>B <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a>b1_true, b2_true <span class="op">=</span> <span class="fl">0.2</span> <span class="op">*</span> B, <span class="op">-</span><span class="fl">0.2</span> <span class="op">*</span> B</span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> np.random.binomial(<span class="dv">1</span>, <span class="fl">0.5</span>, size<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb11-34"><a href="#cb11-34" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.where(z <span class="op">==</span> <span class="dv">1</span>, np.random.normal(b1_true, <span class="fl">1.0</span>, <span class="dv">100</span>), np.random.normal(b2_true, <span class="fl">1.0</span>, <span class="dv">100</span>))</span>
<span id="cb11-35"><a href="#cb11-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-36"><a href="#cb11-36" aria-hidden="true" tabindex="-1"></a>g_a, g_b1, g_b2 <span class="op">=</span> gibbs_sampler(X, num_iterations<span class="op">=</span><span class="dv">5500</span>)</span>
<span id="cb11-37"><a href="#cb11-37" aria-hidden="true" tabindex="-1"></a>m_a, m_b1, m_b2 <span class="op">=</span> mh_sampler(X, num_iterations<span class="op">=</span><span class="dv">5500</span>)</span>
<span id="cb11-38"><a href="#cb11-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-39"><a href="#cb11-39" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb11-40"><a href="#cb11-40" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">0.7</span></span>
<span id="cb11-41"><a href="#cb11-41" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">0</span>].plot(m_b1, label<span class="op">=</span><span class="st">'b1'</span>, alpha<span class="op">=</span>alpha)</span>
<span id="cb11-42"><a href="#cb11-42" aria-hidden="true" tabindex="-1"></a><span class="co">#axes[0, 0].plot(m_b2, label='b2', alpha=alpha)</span></span>
<span id="cb11-43"><a href="#cb11-43" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">0</span>].set_title(<span class="st">"Metropolis-Hastings Trace"</span>)</span>
<span id="cb11-44"><a href="#cb11-44" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">0</span>].set_ylabel(<span class="st">"Parameter Value"</span>)</span>
<span id="cb11-45"><a href="#cb11-45" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">0</span>].set_ylim(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb11-46"><a href="#cb11-46" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">0</span>].legend()</span>
<span id="cb11-47"><a href="#cb11-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-48"><a href="#cb11-48" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">1</span>].plot(g_b1, label<span class="op">=</span><span class="st">'b1'</span>, alpha<span class="op">=</span>alpha)</span>
<span id="cb11-49"><a href="#cb11-49" aria-hidden="true" tabindex="-1"></a><span class="co">#axes[0, 1].plot(g_b2, label='b2', alpha=alpha)</span></span>
<span id="cb11-50"><a href="#cb11-50" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">1</span>].set_title(<span class="st">"Gibbs Sampler Trace"</span>)</span>
<span id="cb11-51"><a href="#cb11-51" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">1</span>].set_ylim(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb11-52"><a href="#cb11-52" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">1</span>].legend()</span>
<span id="cb11-53"><a href="#cb11-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-54"><a href="#cb11-54" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">0.05</span></span>
<span id="cb11-55"><a href="#cb11-55" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">0</span>].scatter(m_b1, m_b2, alpha<span class="op">=</span>alpha, s<span class="op">=</span><span class="dv">5</span>, c<span class="op">=</span><span class="st">'red'</span>)</span>
<span id="cb11-56"><a href="#cb11-56" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">0</span>].plot([b1_true, b2_true], [b2_true, b1_true], <span class="st">'wo'</span>, markeredgecolor<span class="op">=</span><span class="st">'black'</span>, markersize<span class="op">=</span><span class="dv">10</span>, label<span class="op">=</span><span class="st">'True Centers'</span>)</span>
<span id="cb11-57"><a href="#cb11-57" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">0</span>].set_title(<span class="st">"Metropolis-Hastings Posterior"</span>)</span>
<span id="cb11-58"><a href="#cb11-58" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">0</span>].set_xlabel(<span class="st">"$b_1$"</span>)</span>
<span id="cb11-59"><a href="#cb11-59" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">0</span>].set_ylabel(<span class="st">"$b_2$"</span>)</span>
<span id="cb11-60"><a href="#cb11-60" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">0</span>].set_xlim(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>)<span class="op">;</span> axes[<span class="dv">1</span>, <span class="dv">0</span>].set_ylim(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb11-61"><a href="#cb11-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-62"><a href="#cb11-62" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">1</span>].scatter(g_b1, g_b2, alpha<span class="op">=</span>alpha, s<span class="op">=</span><span class="dv">5</span>, c<span class="op">=</span><span class="st">'blue'</span>)</span>
<span id="cb11-63"><a href="#cb11-63" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">1</span>].plot([b1_true, b2_true], [b2_true, b1_true], <span class="st">'wo'</span>, markeredgecolor<span class="op">=</span><span class="st">'black'</span>, markersize<span class="op">=</span><span class="dv">10</span>, label<span class="op">=</span><span class="st">'True Centers'</span>)</span>
<span id="cb11-64"><a href="#cb11-64" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">1</span>].set_title(<span class="st">"Gibbs Sampler Posterior"</span>)</span>
<span id="cb11-65"><a href="#cb11-65" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">1</span>].set_xlabel(<span class="st">"$b_1$"</span>)</span>
<span id="cb11-66"><a href="#cb11-66" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">1</span>].set_xlim(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>)<span class="op">;</span> axes[<span class="dv">1</span>, <span class="dv">1</span>].set_ylim(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb11-67"><a href="#cb11-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-68"><a href="#cb11-68" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb11-69"><a href="#cb11-69" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display page-columns page-full">
<div id="mcmc-vs-gibbs" class="quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="figure page-columns page-full">
<p class="page-columns page-full"><img src="slt_gaussian_mixture_files/figure-html/mcmc-vs-gibbs-output-1.png" width="961" height="565" class="figure-img column-page"></p>
<figcaption>Comparison of Metropolis-Hastings (left) and Gibbs Sampling (right) on the same GMM dataset (B=2). Notice how the Gibbs sampler appears to sample from a wider range of parameter values.</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="chapter-8.-comparing-information-criteria" class="level2">
<h2 class="anchored" data-anchor-id="chapter-8.-comparing-information-criteria">Chapter 8. Comparing Information Criteria</h2>
<p>In Chapter 8, Watanabe provides a sweeping experimental demonstration (Example 53) of how different information criteria behave across various geometric conditions in the parameter space. We will reproduce this study using our 1D Gaussian Mixture analog.</p>
<p>We consider the 3-parameter model: <span class="math display">\[ p(x|a, b, c) = a \mathcal{N}(x|b, 1) + (1-a) \mathcal{N}(x|c, 1) \]</span></p>
<p>Watanabe categorizes standard machine learning datasets into distinct regimes depending on whether the true distribution is <strong>Regular</strong> (identifiable parameters, bounded inverse Hessian) vs <strong>Nonregular</strong> (singular), and <strong>Realizable</strong> (true distribution is perfectly reachable by the model) vs <strong>Unrealizable</strong>:</p>
<ol type="1">
<li><strong>Regular &amp; Realizable</strong>: <span class="math inline">\(q(x) = 0.5 \mathcal{N}(x|2, 1) + 0.5 \mathcal{N}(x|-2, 1)\)</span>. The two modes are far apart, meaning the parameters <span class="math inline">\((a,b,c)\)</span> are uniquely identifiable.</li>
<li><strong>Regular &amp; Unrealizable</strong>: <span class="math inline">\(q(x) = 0.5\mathcal{N}(x|2, \sigma^2) + 0.5\mathcal{N}(x|-2, \sigma^2)\)</span> where <span class="math inline">\(\sigma=0.8\)</span>. The model cannot output variance <span class="math inline">\(\neq 1\)</span>, but it can still uniquely find the means of the separated modes.</li>
<li><strong>Nonregular &amp; Realizable</strong>: <span class="math inline">\(q(x) = \mathcal{N}(x|0, 1)\)</span>. The true parameters lie exactly on the intersecting singular manifolds (<span class="math inline">\(a \in \{0, 1\}\)</span> or <span class="math inline">\(b=c\)</span>).</li>
<li><strong>Nonregular &amp; Unrealizable</strong>: <span class="math inline">\(q(x) = \mathcal{N}(x|0, \sigma^2)\)</span>. The true parameters are pulled into the singularity, but the variance prevents perfect realization.</li>
<li><strong>Delicate</strong>: <span class="math inline">\(q(x) = 0.5 \mathcal{N}(x|0.5, 0.95^2) + 0.5 \mathcal{N}(x|-0.5, 0.95^2)\)</span>. The two components <span class="math inline">\(b=0.5\)</span> and <span class="math inline">\(c=-0.5\)</span> are extremely close to the <span class="math inline">\(b=c\)</span> singularity, straining finite-sample resolution.</li>
<li><strong>Unbalanced</strong>: <span class="math inline">\(q(x) = 0.01 \mathcal{N}(x|2, 1) + 0.99 \mathcal{N}(x|-2, 1)\)</span>. The parameter <span class="math inline">\(b\)</span> is effectively non-identifiable because component <span class="math inline">\(a=0.01\)</span> has so little weight.</li>
</ol>
<p>Because calculating the posterior for 100 trials across 6 cases requires heavy MCMC simulation, the following table loads pre-computed results from a separate Python run mimicking Watanabe’s numerical study. The values show the averaged deviation from the empirical entropy (lower variance and unbiased scaling is better).</p>
<div id="cell-calculate-criteria" class="cell" data-execution_count="11">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> json</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the cached results from the separate multiprocessing script</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="cf">try</span>:</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(<span class="st">'example_53_results.json'</span>, <span class="st">'r'</span>) <span class="im">as</span> f:</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>        results <span class="op">=</span> json.load(f)</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>    data <span class="op">=</span> []</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>    index_labels <span class="op">=</span> []</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> c_id <span class="kw">in</span> [<span class="st">"1"</span>, <span class="st">"2"</span>, <span class="st">"3"</span>, <span class="st">"4"</span>, <span class="st">"5"</span>, <span class="st">"6"</span>]:</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>        r <span class="op">=</span> results[c_id]</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>        name <span class="op">=</span> r[<span class="st">"name"</span>]</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># We append two rows for each case: Average and Std Dev</span></span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>        data.append([</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>            r[<span class="st">'G_mean'</span>], r[<span class="st">'ISCV_mean'</span>], r[<span class="st">'AIC_mean'</span>], r[<span class="st">'DIC_mean'</span>], r[<span class="st">'WAIC_mean'</span>]</span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>        ])</span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>        data.append([</span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a>            r[<span class="st">'G_std'</span>], r[<span class="st">'ISCV_std'</span>], r[<span class="st">'AIC_std'</span>], r[<span class="st">'DIC_std'</span>], r[<span class="st">'WAIC_std'</span>]</span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a>        ])</span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a>        index_labels.append((name, <span class="st">'Ave'</span>))</span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a>        index_labels.append((<span class="st">''</span>, <span class="st">'Std'</span>))</span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Format into a Multi-index dataframe to match the book's exact table style</span></span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a>    mi <span class="op">=</span> pd.MultiIndex.from_tuples(index_labels, names<span class="op">=</span>[<span class="st">'Cases'</span>, <span class="st">'Stat'</span>])</span>
<span id="cb12-29"><a href="#cb12-29" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> pd.DataFrame(data, index<span class="op">=</span>mi, columns<span class="op">=</span>[<span class="st">'G'</span>, <span class="st">'ISCV'</span>, <span class="st">'AIC_b'</span>, <span class="st">'DIC'</span>, <span class="st">'WAIC'</span>])</span>
<span id="cb12-30"><a href="#cb12-30" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-31"><a href="#cb12-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Format the float output</span></span>
<span id="cb12-32"><a href="#cb12-32" aria-hidden="true" tabindex="-1"></a>    display(df.style.<span class="bu">format</span>(<span class="st">"</span><span class="sc">{:.4f}</span><span class="st">"</span>))</span>
<span id="cb12-33"><a href="#cb12-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-34"><a href="#cb12-34" aria-hidden="true" tabindex="-1"></a><span class="cf">except</span> <span class="pp">FileNotFoundError</span>:</span>
<span id="cb12-35"><a href="#cb12-35" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Pre-calculated results example_53_results.json not found."</span>)</span>
<span id="cb12-36"><a href="#cb12-36" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Run `python3 run_example_53.py` to generate the MCMC data table."</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div id="calculate-criteria" class="cell-output cell-output-display">
<style type="text/css">
</style>

<table id="T_f7487" class="caption-top table table-sm table-striped small">
<thead>
<tr class="header">
<th class="blank" data-quarto-table-cell-role="th">&nbsp;</th>
<th class="blank level0" data-quarto-table-cell-role="th">&nbsp;</th>
<th id="T_f7487_level0_col0" class="col_heading level0 col0" data-quarto-table-cell-role="th">G</th>
<th id="T_f7487_level0_col1" class="col_heading level0 col1" data-quarto-table-cell-role="th">ISCV</th>
<th id="T_f7487_level0_col2" class="col_heading level0 col2" data-quarto-table-cell-role="th">AIC_b</th>
<th id="T_f7487_level0_col3" class="col_heading level0 col3" data-quarto-table-cell-role="th">DIC</th>
<th id="T_f7487_level0_col4" class="col_heading level0 col4" data-quarto-table-cell-role="th">WAIC</th>
</tr>
<tr class="even">
<th class="index_name level0" data-quarto-table-cell-role="th">Cases</th>
<th class="index_name level1" data-quarto-table-cell-role="th">Stat</th>
<th class="blank col0" data-quarto-table-cell-role="th">&nbsp;</th>
<th class="blank col1" data-quarto-table-cell-role="th">&nbsp;</th>
<th class="blank col2" data-quarto-table-cell-role="th">&nbsp;</th>
<th class="blank col3" data-quarto-table-cell-role="th">&nbsp;</th>
<th class="blank col4" data-quarto-table-cell-role="th">&nbsp;</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<th id="T_f7487_level0_row0" class="row_heading level0 row0" data-quarto-table-cell-role="th">Regular Realizable</th>
<th id="T_f7487_level1_row0" class="row_heading level1 row0" data-quarto-table-cell-role="th">Ave</th>
<td id="T_f7487_row0_col0" class="data row0 col0">0.0141</td>
<td id="T_f7487_row0_col1" class="data row0 col1">0.0163</td>
<td id="T_f7487_row0_col2" class="data row0 col2">0.0162</td>
<td id="T_f7487_row0_col3" class="data row0 col3">0.0161</td>
<td id="T_f7487_row0_col4" class="data row0 col4">0.0163</td>
</tr>
<tr class="even">
<th id="T_f7487_level0_row1" class="row_heading level0 row1" data-quarto-table-cell-role="th"></th>
<th id="T_f7487_level1_row1" class="row_heading level1 row1" data-quarto-table-cell-role="th">Std</th>
<td id="T_f7487_row1_col0" class="data row1 col0">0.0105</td>
<td id="T_f7487_row1_col1" class="data row1 col1">0.0101</td>
<td id="T_f7487_row1_col2" class="data row1 col2">0.0101</td>
<td id="T_f7487_row1_col3" class="data row1 col3">0.0102</td>
<td id="T_f7487_row1_col4" class="data row1 col4">0.0101</td>
</tr>
<tr class="odd">
<th id="T_f7487_level0_row2" class="row_heading level0 row2" data-quarto-table-cell-role="th">Regular Unrealizable</th>
<th id="T_f7487_level1_row2" class="row_heading level1 row2" data-quarto-table-cell-role="th">Ave</th>
<td id="T_f7487_row2_col0" class="data row2 col0">0.0548</td>
<td id="T_f7487_row2_col1" class="data row2 col1">0.0543</td>
<td id="T_f7487_row2_col2" class="data row2 col2">0.0580</td>
<td id="T_f7487_row2_col3" class="data row2 col3">0.0578</td>
<td id="T_f7487_row2_col4" class="data row2 col4">0.0543</td>
</tr>
<tr class="even">
<th id="T_f7487_level0_row3" class="row_heading level0 row3" data-quarto-table-cell-role="th"></th>
<th id="T_f7487_level1_row3" class="row_heading level1 row3" data-quarto-table-cell-role="th">Std</th>
<td id="T_f7487_row3_col0" class="data row3 col0">0.0127</td>
<td id="T_f7487_row3_col1" class="data row3 col1">0.0276</td>
<td id="T_f7487_row3_col2" class="data row3 col2">0.0283</td>
<td id="T_f7487_row3_col3" class="data row3 col3">0.0283</td>
<td id="T_f7487_row3_col4" class="data row3 col4">0.0276</td>
</tr>
<tr class="odd">
<th id="T_f7487_level0_row4" class="row_heading level0 row4" data-quarto-table-cell-role="th">Nonreg. Realizable</th>
<th id="T_f7487_level1_row4" class="row_heading level1 row4" data-quarto-table-cell-role="th">Ave</th>
<td id="T_f7487_row4_col0" class="data row4 col0">0.0081</td>
<td id="T_f7487_row4_col1" class="data row4 col1">0.0063</td>
<td id="T_f7487_row4_col2" class="data row4 col2">0.0198</td>
<td id="T_f7487_row4_col3" class="data row4 col3">-0.0163</td>
<td id="T_f7487_row4_col4" class="data row4 col4">0.0063</td>
</tr>
<tr class="even">
<th id="T_f7487_level0_row5" class="row_heading level0 row5" data-quarto-table-cell-role="th"></th>
<th id="T_f7487_level1_row5" class="row_heading level1 row5" data-quarto-table-cell-role="th">Std</th>
<td id="T_f7487_row5_col0" class="data row5 col0">0.0098</td>
<td id="T_f7487_row5_col1" class="data row5 col1">0.0102</td>
<td id="T_f7487_row5_col2" class="data row5 col2">0.0136</td>
<td id="T_f7487_row5_col3" class="data row5 col3">0.0347</td>
<td id="T_f7487_row5_col4" class="data row5 col4">0.0101</td>
</tr>
<tr class="odd">
<th id="T_f7487_level0_row6" class="row_heading level0 row6" data-quarto-table-cell-role="th">Nonreg. Unrealizable</th>
<th id="T_f7487_level1_row6" class="row_heading level1 row6" data-quarto-table-cell-role="th">Ave</th>
<td id="T_f7487_row6_col0" class="data row6 col0">0.0533</td>
<td id="T_f7487_row6_col1" class="data row6 col1">0.0538</td>
<td id="T_f7487_row6_col2" class="data row6 col2">0.0722</td>
<td id="T_f7487_row6_col3" class="data row6 col3">0.0376</td>
<td id="T_f7487_row6_col4" class="data row6 col4">0.0538</td>
</tr>
<tr class="even">
<th id="T_f7487_level0_row7" class="row_heading level0 row7" data-quarto-table-cell-role="th"></th>
<th id="T_f7487_level1_row7" class="row_heading level1 row7" data-quarto-table-cell-role="th">Std</th>
<td id="T_f7487_row7_col0" class="data row7 col0">0.0064</td>
<td id="T_f7487_row7_col1" class="data row7 col1">0.0252</td>
<td id="T_f7487_row7_col2" class="data row7 col2">0.0254</td>
<td id="T_f7487_row7_col3" class="data row7 col3">0.0468</td>
<td id="T_f7487_row7_col4" class="data row7 col4">0.0252</td>
</tr>
<tr class="odd">
<th id="T_f7487_level0_row8" class="row_heading level0 row8" data-quarto-table-cell-role="th">Delicate</th>
<th id="T_f7487_level1_row8" class="row_heading level1 row8" data-quarto-table-cell-role="th">Ave</th>
<td id="T_f7487_row8_col0" class="data row8 col0">0.0079</td>
<td id="T_f7487_row8_col1" class="data row8 col1">0.0090</td>
<td id="T_f7487_row8_col2" class="data row8 col2">0.0190</td>
<td id="T_f7487_row8_col3" class="data row8 col3">-0.0223</td>
<td id="T_f7487_row8_col4" class="data row8 col4">0.0091</td>
</tr>
<tr class="even">
<th id="T_f7487_level0_row9" class="row_heading level0 row9" data-quarto-table-cell-role="th"></th>
<th id="T_f7487_level1_row9" class="row_heading level1 row9" data-quarto-table-cell-role="th">Std</th>
<td id="T_f7487_row9_col0" class="data row9 col0">0.0075</td>
<td id="T_f7487_row9_col1" class="data row9 col1">0.0089</td>
<td id="T_f7487_row9_col2" class="data row9 col2">0.0087</td>
<td id="T_f7487_row9_col3" class="data row9 col3">0.0406</td>
<td id="T_f7487_row9_col4" class="data row9 col4">0.0089</td>
</tr>
<tr class="odd">
<th id="T_f7487_level0_row10" class="row_heading level0 row10" data-quarto-table-cell-role="th">Unbalanced</th>
<th id="T_f7487_level1_row10" class="row_heading level1 row10" data-quarto-table-cell-role="th">Ave</th>
<td id="T_f7487_row10_col0" class="data row10 col0">0.0194</td>
<td id="T_f7487_row10_col1" class="data row10 col1">0.0068</td>
<td id="T_f7487_row10_col2" class="data row10 col2">0.0122</td>
<td id="T_f7487_row10_col3" class="data row10 col3">-0.0317</td>
<td id="T_f7487_row10_col4" class="data row10 col4">0.0067</td>
</tr>
<tr class="even">
<th id="T_f7487_level0_row11" class="row_heading level0 row11" data-quarto-table-cell-role="th"></th>
<th id="T_f7487_level1_row11" class="row_heading level1 row11" data-quarto-table-cell-role="th">Std</th>
<td id="T_f7487_row11_col0" class="data row11 col0">0.0123</td>
<td id="T_f7487_row11_col1" class="data row11 col1">0.0143</td>
<td id="T_f7487_row11_col2" class="data row11 col2">0.0124</td>
<td id="T_f7487_row11_col3" class="data row11 col3">0.0873</td>
<td id="T_f7487_row11_col4" class="data row11 col4">0.0143</td>
</tr>
</tbody>
</table>
</div>
</div>
<section id="interpretation-of-results" class="level3">
<h3 class="anchored" data-anchor-id="interpretation-of-results">Interpretation of Results</h3>
<p>The experimental table provides crucial concrete evidence for Singular Learning Theory:</p>
<ul>
<li><strong>Regular Cases (1 &amp; 2):</strong> When the model is regular and identifiable, <strong>all criteria work well</strong>. AIC uses <span class="math inline">\(d/2\)</span>, which is correct here because <span class="math inline">\(\lambda=d/2\)</span>, and the posterior represents a distinct Gaussian shape around the unique true parameters, making DIC’s point-estimation valid.</li>
<li><strong>Failures of AIC in Singularity:</strong> In the delicate and nonregular cases (3 through 6), the model acts singular. AIC assumes the penalty is <span class="math inline">\(d/n\)</span>, overestimating the error because singular models have a smaller geometrical volume (<span class="math inline">\(\lambda &lt; d/2\)</span>).</li>
<li><strong>Failures of DIC in Singularity:</strong> DIC structurally relies on taking the deviance at the <em>posterior mean</em>. In nonregular (3/4) and unbalanced (6) cases, the parameter singularity forms an elongated manifold rather than a Gaussian blob. Therefore, the numerical mean often falls into an empty low-probability valley separating two valid coordinate modes. This causes DIC to generate wildly inaccurate penalties and enormous variance (as seen in the huge standard deviations).</li>
<li><strong>The Stability of WAIC and ISCV:</strong> Watanabe proves that the cross-validation error functionally mimics the mathematical singularity structure. WAIC acts as an exact asymptotic estimator of the generalization error (and thus the cross-validation error) because the local geometric variance <span class="math inline">\(V_n\)</span> identically mirrors the true <span class="math inline">\(\lambda\)</span> without needing to calculate the intractable blow-up explicitly. <strong>WAIC requires no point estimate, avoiding the fatal geometric traps of DIC.</strong></li>
</ul>
</section>
</section>
<section id="chapter-9.-phase-transition-in-a-hierarchical-model" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="chapter-9.-phase-transition-in-a-hierarchical-model">Chapter 9. Phase Transition in a Hierarchical Model</h2>
<p>In Chapter 9, Watanabe describes a <strong>Phase Transition</strong> in Bayesian statistics. A phase transition occurs when the structure of the posterior distribution changes drastically depending on a generalized hyperparameter. At this critical point, the free energy is often discontinuous or non-differentiable.</p>
<section id="example-67-normal-mixture-phase-transition" class="level3">
<h3 class="anchored" data-anchor-id="example-67-normal-mixture-phase-transition">Example 67: Normal Mixture Phase Transition</h3>
<p>Let’s adapt Example 67 from the book (pages 279-281) to our 1D model formulation. Consider the same normal mixture model for <span class="math inline">\(x \in \mathbb{R}\)</span> with parameter <span class="math inline">\(w=(a,\mu)\)</span>: <span class="math display">\[ p(x|w) = (1 - a) \mathcal{N}(x|0, 1) + a \mathcal{N}(x|\mu, 1) \]</span></p>
<p>Suppose the true distribution is simply the standard normal: <span class="math display">\[ q(x) = \mathcal{N}(x|0, 1) \]</span></p>
<p>We place a Dirichlet prior on <span class="math inline">\(a\)</span> with hyperparameter <span class="math inline">\(\alpha &gt; 0\)</span>: <span class="math display">\[ \varphi(a) \propto (a(1 - a))^{\alpha - 1} \]</span> And a wide Gaussian prior on <span class="math inline">\(\mu\)</span>: <span class="math display">\[ \varphi(\mu) \propto \exp \left( -\frac{\mu^2}{2\sigma^2} \right) \]</span> with <span class="math inline">\(\sigma = 10\)</span>. We will investigate how the hyperparameter <span class="math inline">\(\alpha\)</span> induces a phase transition.</p>
</section>
<section id="the-two-modes-of-the-model" class="level3">
<h3 class="anchored" data-anchor-id="the-two-modes-of-the-model">The Two Modes of the Model</h3>
<p>To perfectly match the true distribution <span class="math inline">\(q(x) = \mathcal{N}(x|0, 1)\)</span>, the model requires either:</p>
<ol type="1">
<li><span class="math inline">\(a = 0\)</span>: In this case, the first component perfectly matches <span class="math inline">\(q(x)\)</span>, and the parameter <span class="math inline">\(\mu\)</span> can be absolutely anything (it is completely “free” because its component has zero weight).</li>
<li><span class="math inline">\(\mu = 0\)</span>: In this case, both components are centered at the origin, matching <span class="math inline">\(q(x)\)</span>, and the mixing proportion <span class="math inline">\(a\)</span> can be absolutely anything (<span class="math inline">\(a\)</span> is “free”).</li>
</ol>
<p>These two regimes represent two intersecting singular manifolds in the parameter space. The Real Log Canonical Threshold (RLCT) is dictated by whichever singularity is “stronger”, which depends on the prior concentration <span class="math inline">\(\alpha\)</span>.</p>
</section>
<section id="calculating-the-rlct" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="calculating-the-rlct">Calculating the RLCT</h3>
<p>The true parameters are given by the union of two branches: <span class="math inline">\(a=0\)</span> (with any <span class="math inline">\(\mu\)</span>) and <span class="math inline">\(\mu=0\)</span> (with any <span class="math inline">\(a\)</span>). The overall RLCT is obtained by evaluating the poles along each branch and taking the minimum.</p>
<ol type="1">
<li><p><strong>Along the branch <span class="math inline">\(a=0\)</span> (where <span class="math inline">\(\mu \neq 0\)</span> is free):</strong> Locally, <span class="math inline">\(\mu\)</span> is practically a non-zero constant <span class="math inline">\(c\)</span>. The KL divergence <span class="math display">\[K(a, \mu) \approx \frac{1}{2} a^2 \mu^2 \approx \frac{1}{2} c^2 a^2 \propto a^2.\]</span> Thus, the polynomial power is <span class="math inline">\(k=1\)</span>. The prior contains <span class="math inline">\((a(1-a))^{\alpha-1}\)</span>, which behaves like <span class="math inline">\(a^{\alpha-1}\)</span> as <span class="math inline">\(a \to 0\)</span>. Thus, the measure has a polynomial power <span class="math inline">\(h=\alpha-1\)</span>. The candidate RLCT for this branch is <span class="math display">\[\lambda_1 = \frac{h+1}{2k} = \frac{(\alpha-1)+1}{2(1)} = \alpha/2\]</span>.</p></li>
<li><p><strong>Along the branch <span class="math inline">\(\mu=0\)</span> (where <span class="math inline">\(a \neq 0\)</span> is free):</strong> Locally, <span class="math inline">\(a\)</span> is practically a non-zero constant <span class="math inline">\(c\)</span>. The KL divergence behaves like <span class="math display">\[K(a, \mu) \approx \frac{1}{2} c^2 \mu^2 \propto \mu^2,\]</span> so <span class="math inline">\(k=1\)</span>. The prior behaves like a constant locally with respect to <span class="math inline">\(\mu\)</span> (since <span class="math inline">\(a\)</span> is near <span class="math inline">\(c\)</span> and <span class="math inline">\(\mu \approx 0\)</span>), so <span class="math inline">\(h=0\)</span>. The candidate RLCT for this branch is <span class="math display">\[\lambda_2 = \frac{h+1}{2k} = \frac{0+1}{2(1)} = 1/2.\]</span></p></li>
</ol>
<p><strong>At the intersection <span class="math inline">\((a=0, \mu=0)\)</span>:</strong> A rigorous calculation using Hironaka’s blow-up at the origin produces additional candidate poles (specifically <span class="math inline">\(\frac{\alpha+1}{4}\)</span>). However, this intersection pole is never strictly less than the minimum of the two individual branches for any <span class="math inline">\(\alpha &gt; 0\)</span>.</p>
<p>Therefore, the global RLCT is dictated purely by the minimum of the two branches: <span class="math display">\[ \lambda = \min(\lambda_1, \lambda_2) = \min(\alpha/2, 1/2) \]</span></p>
<p>This reveals a profound structural shift exactly at <strong><span class="math inline">\(\alpha = 1\)</span></strong>:</p>
<ul>
<li>For <strong><span class="math inline">\(\alpha &lt; 1\)</span></strong> (<span class="math inline">\(\lambda = \alpha/2\)</span>), the strongest singularity comes from the first branch. The posterior concentrates around <span class="math inline">\(a \approx 0\)</span>, leaving the parameter <span class="math inline">\(\mu\)</span> completely free to wander across the wide prior space.</li>
<li>For <strong><span class="math inline">\(\alpha &gt; 1\)</span></strong> (<span class="math inline">\(\lambda = 1/2\)</span>), the strongest singularity comes from the second branch. The prior naturally pulls <span class="math inline">\(a\)</span> away from the edges, closing off the first strategy. The posterior is forced to satisfy <span class="math inline">\(\mu \approx 0\)</span>, leaving <span class="math inline">\(a\)</span> free to vary.</li>
</ul>
<p>This abrupt change from “<span class="math inline">\(\mu\)</span> is free” to “<span class="math inline">\(a\)</span> is free” at <span class="math inline">\(\alpha = 1\)</span> is the <strong>phase transition</strong>. Let’s visualize this drastic change in the shape of the posterior distribution.</p>
<div id="cell-phase-transition-posterior" class="cell page-columns page-full" data-cache="true" data-execution_count="12">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> log_prior(a, mu, alpha):</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> a <span class="op">&lt;=</span> <span class="dv">0</span> <span class="kw">or</span> a <span class="op">&gt;=</span> <span class="dv">1</span>: <span class="cf">return</span> <span class="op">-</span>np.inf</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>    lp_a <span class="op">=</span> (alpha <span class="op">-</span> <span class="dv">1</span>) <span class="op">*</span> np.log(a <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> a))</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>    lp_mu <span class="op">=</span> <span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> (mu <span class="op">/</span> <span class="fl">10.0</span>)<span class="op">**</span><span class="dv">2</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> lp_a <span class="op">+</span> lp_mu</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> log_likelihood(X, a, mu):</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>    p0 <span class="op">=</span> np.exp(<span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> X<span class="op">**</span><span class="dv">2</span>) <span class="op">/</span> np.sqrt(<span class="dv">2</span> <span class="op">*</span> np.pi)</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>    pmu <span class="op">=</span> np.exp(<span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> (X <span class="op">-</span> mu)<span class="op">**</span><span class="dv">2</span>) <span class="op">/</span> np.sqrt(<span class="dv">2</span> <span class="op">*</span> np.pi)</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>    p <span class="op">=</span> (<span class="dv">1</span> <span class="op">-</span> a) <span class="op">*</span> p0 <span class="op">+</span> a <span class="op">*</span> pmu <span class="op">+</span> <span class="fl">1e-15</span></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.<span class="bu">sum</span>(np.log(p))</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sample_mcmc(alpha, n<span class="op">=</span><span class="dv">100</span>, iters<span class="op">=</span><span class="dv">25000</span>):</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>    np.random.seed(<span class="dv">42</span>)</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="dv">1</span>, n)</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize near the regular manifold to avoid getting stuck early</span></span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>    a <span class="op">=</span> <span class="fl">0.5</span></span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>    mu <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>    ll <span class="op">=</span> log_likelihood(X, a, mu)</span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>    lp <span class="op">=</span> log_prior(a, mu, alpha)</span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a>    samples_a <span class="op">=</span> np.zeros(iters)</span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a>    samples_mu <span class="op">=</span> np.zeros(iters)</span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a>    a_step, mu_step <span class="op">=</span> <span class="fl">0.1</span>, <span class="fl">0.5</span></span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(iters):</span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a>        a_new <span class="op">=</span> a <span class="op">+</span> np.random.normal(<span class="dv">0</span>, a_step)</span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a>        mu_new <span class="op">=</span> mu <span class="op">+</span> np.random.normal(<span class="dv">0</span>, mu_step)</span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="dv">0</span> <span class="op">&lt;</span> a_new <span class="op">&lt;</span> <span class="dv">1</span>:</span>
<span id="cb13-36"><a href="#cb13-36" aria-hidden="true" tabindex="-1"></a>            lp_new <span class="op">=</span> log_prior(a_new, mu_new, alpha)</span>
<span id="cb13-37"><a href="#cb13-37" aria-hidden="true" tabindex="-1"></a>            ll_new <span class="op">=</span> log_likelihood(X, a_new, mu_new)</span>
<span id="cb13-38"><a href="#cb13-38" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb13-39"><a href="#cb13-39" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> np.log(np.random.rand()) <span class="op">&lt;</span> (ll_new <span class="op">+</span> lp_new <span class="op">-</span> ll <span class="op">-</span> lp):</span>
<span id="cb13-40"><a href="#cb13-40" aria-hidden="true" tabindex="-1"></a>                a, mu <span class="op">=</span> a_new, mu_new</span>
<span id="cb13-41"><a href="#cb13-41" aria-hidden="true" tabindex="-1"></a>                ll, lp <span class="op">=</span> ll_new, lp_new</span>
<span id="cb13-42"><a href="#cb13-42" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb13-43"><a href="#cb13-43" aria-hidden="true" tabindex="-1"></a>        samples_a[i] <span class="op">=</span> a</span>
<span id="cb13-44"><a href="#cb13-44" aria-hidden="true" tabindex="-1"></a>        samples_mu[i] <span class="op">=</span> mu</span>
<span id="cb13-45"><a href="#cb13-45" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-46"><a href="#cb13-46" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Discard burn-in</span></span>
<span id="cb13-47"><a href="#cb13-47" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> samples_a[<span class="dv">5000</span>:], samples_mu[<span class="dv">5000</span>:]</span>
<span id="cb13-48"><a href="#cb13-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-49"><a href="#cb13-49" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>), sharey<span class="op">=</span><span class="st">'row'</span>)</span>
<span id="cb13-50"><a href="#cb13-50" aria-hidden="true" tabindex="-1"></a>alphas_to_test <span class="op">=</span> [<span class="fl">0.2</span>, <span class="fl">1.0</span>, <span class="fl">3.0</span>]</span>
<span id="cb13-51"><a href="#cb13-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-52"><a href="#cb13-52" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, alpha <span class="kw">in</span> <span class="bu">enumerate</span>(alphas_to_test):</span>
<span id="cb13-53"><a href="#cb13-53" aria-hidden="true" tabindex="-1"></a>    sa, smu <span class="op">=</span> sample_mcmc(alpha)</span>
<span id="cb13-54"><a href="#cb13-54" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-55"><a href="#cb13-55" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">0</span>, i].hist(sa, bins<span class="op">=</span><span class="dv">40</span>, density<span class="op">=</span><span class="va">True</span>, color<span class="op">=</span><span class="st">'skyblue'</span>, edgecolor<span class="op">=</span><span class="st">'black'</span>)</span>
<span id="cb13-56"><a href="#cb13-56" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">0</span>, i].set_title(<span class="vs">rf"</span><span class="dv">$</span><span class="ch">\a</span><span class="vs">lpha = </span><span class="sc">{</span>alpha<span class="sc">}</span><span class="dv">$</span><span class="vs">"</span>)</span>
<span id="cb13-57"><a href="#cb13-57" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">0</span>, i].set_xlim(<span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb13-58"><a href="#cb13-58" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">==</span> <span class="dv">0</span>: axes[<span class="dv">0</span>, i].set_ylabel(<span class="st">"Posterior of $a$"</span>)</span>
<span id="cb13-59"><a href="#cb13-59" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-60"><a href="#cb13-60" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">1</span>, i].hist(np.<span class="bu">abs</span>(smu), bins<span class="op">=</span><span class="dv">40</span>, density<span class="op">=</span><span class="va">True</span>, color<span class="op">=</span><span class="st">'salmon'</span>, edgecolor<span class="op">=</span><span class="st">'black'</span>)</span>
<span id="cb13-61"><a href="#cb13-61" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">1</span>, i].set_xlim(<span class="dv">0</span>, <span class="dv">6</span>)</span>
<span id="cb13-62"><a href="#cb13-62" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">==</span> <span class="dv">0</span>: axes[<span class="dv">1</span>, i].set_ylabel(<span class="vs">r"Posterior of </span><span class="dv">$</span><span class="cf">|</span><span class="dv">\m</span><span class="vs">u</span><span class="cf">|</span><span class="dv">$</span><span class="vs">"</span>)</span>
<span id="cb13-63"><a href="#cb13-63" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">1</span>, i].set_xlabel(<span class="vs">r"</span><span class="dv">$</span><span class="cf">|</span><span class="dv">\m</span><span class="vs">u</span><span class="cf">|</span><span class="dv">$</span><span class="vs">"</span>)</span>
<span id="cb13-64"><a href="#cb13-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-65"><a href="#cb13-65" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb13-66"><a href="#cb13-66" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display page-columns page-full">
<div id="phase-transition-posterior" class="quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="figure page-columns page-full">
<p class="page-columns page-full"><img src="slt_gaussian_mixture_files/figure-html/phase-transition-posterior-output-1.png" width="949" height="564" class="figure-img column-page"></p>
<figcaption>Phase Transition in the Posterior Distribution. As <span class="math inline">\(\alpha\)</span> crosses the critical point <span class="math inline">\(\alpha=1\)</span>, the posterior drastically flips from a regime where <span class="math inline">\(\mu\)</span> is free (large spread) to a regime where <span class="math inline">\(a\)</span> is free (spread across [0,1]).</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="reproducing-figures-9.1-and-9.2-error-metric-phase-transition" class="level3">
<h3 class="anchored" data-anchor-id="reproducing-figures-9.1-and-9.2-error-metric-phase-transition">Reproducing Figures 9.1 and 9.2: Error Metric Phase Transition</h3>
<p>Watanabe shows that this geometric phase transition directly causes a kink in the generalization error and WAIC. The theoretical errors are: <span class="math display">\[ \mathbb{E}[G_n - S] \approx \frac{\min(\alpha/2, 1/2)}{n} , \quad \mathbb{E}[\text{WAIC} - S_n] \approx \frac{\min(\alpha/2, 1/2)}{n} \]</span> Since WAIC correctly estimates the generalization error, it will also exhibit a broken derivative at <span class="math inline">\(\alpha=1\)</span>. The following code simulates this across multiple trials (analogous to Figures 9.1 and 9.2 of the book) to show the average criteria values as a function of the hyperparameter. Unfortunately we would need to work with a larger number of trials to reduce the variance and make the kink really visible.</p>
<div id="cell-phase-transition-errors" class="cell" data-execution_count="13">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>alphas <span class="op">=</span> np.linspace(<span class="fl">0.25</span>, <span class="fl">2.5</span>, <span class="dv">10</span>)</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>n_trials <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>results_file <span class="op">=</span> <span class="st">"phase_transition_errors.npz"</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> os.path.exists(results_file):</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Loading saved results from disk..."</span>)</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>    data <span class="op">=</span> np.load(results_file)</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>    mean_ge <span class="op">=</span> data[<span class="st">'mean_ge'</span>]</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>    mean_waic <span class="op">=</span> data[<span class="st">'mean_waic'</span>]</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>    mean_ge <span class="op">=</span> []</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>    mean_waic <span class="op">=</span> []</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Using 20 trials with 10k MCMC steps smooths out the variance to reveal the kink.</span></span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Rendering this cell may take a minute or two.</span></span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> alpha <span class="kw">in</span> alphas:</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>        alpha_ge <span class="op">=</span> []</span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>        alpha_waic <span class="op">=</span> []</span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> trial <span class="kw">in</span> <span class="bu">range</span>(n_trials):</span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>            np.random.seed(<span class="dv">42</span> <span class="op">+</span> trial)</span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a>            X <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="dv">1</span>, n)</span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a>            X_test <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">5000</span>)</span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Empirical entropy of true distribution</span></span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a>            p0_train <span class="op">=</span> np.exp(<span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> X<span class="op">**</span><span class="dv">2</span>) <span class="op">/</span> np.sqrt(<span class="dv">2</span> <span class="op">*</span> np.pi)</span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a>            S_n <span class="op">=</span> <span class="op">-</span>np.mean(np.log(p0_train))</span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a>            <span class="co"># True entropy</span></span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a>            p0_test <span class="op">=</span> np.exp(<span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> X_test<span class="op">**</span><span class="dv">2</span>) <span class="op">/</span> np.sqrt(<span class="dv">2</span> <span class="op">*</span> np.pi)</span>
<span id="cb14-35"><a href="#cb14-35" aria-hidden="true" tabindex="-1"></a>            S <span class="op">=</span> <span class="op">-</span>np.mean(np.log(p0_test))</span>
<span id="cb14-36"><a href="#cb14-36" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb14-37"><a href="#cb14-37" aria-hidden="true" tabindex="-1"></a>            a <span class="op">=</span> <span class="fl">0.5</span><span class="op">;</span> mu <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb14-38"><a href="#cb14-38" aria-hidden="true" tabindex="-1"></a>            ll <span class="op">=</span> log_likelihood(X, a, mu)<span class="op">;</span> lp <span class="op">=</span> log_prior(a, mu, alpha)</span>
<span id="cb14-39"><a href="#cb14-39" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb14-40"><a href="#cb14-40" aria-hidden="true" tabindex="-1"></a>            samples_a <span class="op">=</span> []</span>
<span id="cb14-41"><a href="#cb14-41" aria-hidden="true" tabindex="-1"></a>            samples_mu <span class="op">=</span> []</span>
<span id="cb14-42"><a href="#cb14-42" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb14-43"><a href="#cb14-43" aria-hidden="true" tabindex="-1"></a>            <span class="co"># MCMC Sampling</span></span>
<span id="cb14-44"><a href="#cb14-44" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10000</span>):</span>
<span id="cb14-45"><a href="#cb14-45" aria-hidden="true" tabindex="-1"></a>                a_new <span class="op">=</span> a <span class="op">+</span> np.random.normal(<span class="dv">0</span>, <span class="fl">0.1</span>)</span>
<span id="cb14-46"><a href="#cb14-46" aria-hidden="true" tabindex="-1"></a>                mu_new <span class="op">=</span> mu <span class="op">+</span> np.random.normal(<span class="dv">0</span>, <span class="fl">0.5</span>)</span>
<span id="cb14-47"><a href="#cb14-47" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> <span class="dv">0</span> <span class="op">&lt;</span> a_new <span class="op">&lt;</span> <span class="dv">1</span>:</span>
<span id="cb14-48"><a href="#cb14-48" aria-hidden="true" tabindex="-1"></a>                    lp_new <span class="op">=</span> log_prior(a_new, mu_new, alpha)</span>
<span id="cb14-49"><a href="#cb14-49" aria-hidden="true" tabindex="-1"></a>                    ll_new <span class="op">=</span> log_likelihood(X, a_new, mu_new)</span>
<span id="cb14-50"><a href="#cb14-50" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">if</span> np.log(np.random.rand()) <span class="op">&lt;</span> (ll_new <span class="op">+</span> lp_new <span class="op">-</span> ll <span class="op">-</span> lp):</span>
<span id="cb14-51"><a href="#cb14-51" aria-hidden="true" tabindex="-1"></a>                        a, mu <span class="op">=</span> a_new, mu_new</span>
<span id="cb14-52"><a href="#cb14-52" aria-hidden="true" tabindex="-1"></a>                        ll, lp <span class="op">=</span> ll_new, lp_new</span>
<span id="cb14-53"><a href="#cb14-53" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> i <span class="op">&gt;=</span> <span class="dv">4000</span> <span class="kw">and</span> i <span class="op">%</span> <span class="dv">10</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb14-54"><a href="#cb14-54" aria-hidden="true" tabindex="-1"></a>                    samples_a.append(a)</span>
<span id="cb14-55"><a href="#cb14-55" aria-hidden="true" tabindex="-1"></a>                    samples_mu.append(mu)</span>
<span id="cb14-56"><a href="#cb14-56" aria-hidden="true" tabindex="-1"></a>                    </span>
<span id="cb14-57"><a href="#cb14-57" aria-hidden="true" tabindex="-1"></a>            samples_a <span class="op">=</span> np.array(samples_a)</span>
<span id="cb14-58"><a href="#cb14-58" aria-hidden="true" tabindex="-1"></a>            samples_mu <span class="op">=</span> np.array(samples_mu)</span>
<span id="cb14-59"><a href="#cb14-59" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb14-60"><a href="#cb14-60" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Vectorized WAIC Calculation</span></span>
<span id="cb14-61"><a href="#cb14-61" aria-hidden="true" tabindex="-1"></a>            diff_sq_tr <span class="op">=</span> (X[:, <span class="va">None</span>] <span class="op">-</span> samples_mu[<span class="va">None</span>, :])<span class="op">**</span><span class="dv">2</span></span>
<span id="cb14-62"><a href="#cb14-62" aria-hidden="true" tabindex="-1"></a>            pmu_tr <span class="op">=</span> np.exp(<span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> diff_sq_tr) <span class="op">/</span> np.sqrt(<span class="dv">2</span> <span class="op">*</span> np.pi)</span>
<span id="cb14-63"><a href="#cb14-63" aria-hidden="true" tabindex="-1"></a>            p_train <span class="op">=</span> (<span class="dv">1</span> <span class="op">-</span> samples_a[<span class="va">None</span>, :]) <span class="op">*</span> p0_train[:, <span class="va">None</span>] <span class="op">+</span> samples_a[<span class="va">None</span>, :] <span class="op">*</span> pmu_tr <span class="op">+</span> <span class="fl">1e-15</span></span>
<span id="cb14-64"><a href="#cb14-64" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb14-65"><a href="#cb14-65" aria-hidden="true" tabindex="-1"></a>            p_train_mean <span class="op">=</span> np.mean(p_train, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb14-66"><a href="#cb14-66" aria-hidden="true" tabindex="-1"></a>            T_n <span class="op">=</span> <span class="op">-</span>np.mean(np.log(p_train_mean))</span>
<span id="cb14-67"><a href="#cb14-67" aria-hidden="true" tabindex="-1"></a>            V_n <span class="op">=</span> np.mean(np.var(np.log(p_train), axis<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb14-68"><a href="#cb14-68" aria-hidden="true" tabindex="-1"></a>            waic <span class="op">=</span> T_n <span class="op">+</span> V_n <span class="op">-</span> S_n</span>
<span id="cb14-69"><a href="#cb14-69" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb14-70"><a href="#cb14-70" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Vectorized GE Calculation</span></span>
<span id="cb14-71"><a href="#cb14-71" aria-hidden="true" tabindex="-1"></a>            diff_sq_ts <span class="op">=</span> (X_test[:, <span class="va">None</span>] <span class="op">-</span> samples_mu[<span class="va">None</span>, :])<span class="op">**</span><span class="dv">2</span></span>
<span id="cb14-72"><a href="#cb14-72" aria-hidden="true" tabindex="-1"></a>            pmu_ts <span class="op">=</span> np.exp(<span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> diff_sq_ts) <span class="op">/</span> np.sqrt(<span class="dv">2</span> <span class="op">*</span> np.pi)</span>
<span id="cb14-73"><a href="#cb14-73" aria-hidden="true" tabindex="-1"></a>            p_test_all <span class="op">=</span> (<span class="dv">1</span> <span class="op">-</span> samples_a[<span class="va">None</span>, :]) <span class="op">*</span> p0_test[:, <span class="va">None</span>] <span class="op">+</span> samples_a[<span class="va">None</span>, :] <span class="op">*</span> pmu_ts <span class="op">+</span> <span class="fl">1e-15</span></span>
<span id="cb14-74"><a href="#cb14-74" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb14-75"><a href="#cb14-75" aria-hidden="true" tabindex="-1"></a>            p_test_mean <span class="op">=</span> np.mean(p_test_all, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb14-76"><a href="#cb14-76" aria-hidden="true" tabindex="-1"></a>            ge <span class="op">=</span> <span class="op">-</span>np.mean(np.log(p_test_mean)) <span class="op">-</span> S</span>
<span id="cb14-77"><a href="#cb14-77" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb14-78"><a href="#cb14-78" aria-hidden="true" tabindex="-1"></a>            alpha_ge.append(ge)</span>
<span id="cb14-79"><a href="#cb14-79" aria-hidden="true" tabindex="-1"></a>            alpha_waic.append(waic)</span>
<span id="cb14-80"><a href="#cb14-80" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb14-81"><a href="#cb14-81" aria-hidden="true" tabindex="-1"></a>        mean_ge.append(np.mean(alpha_ge))</span>
<span id="cb14-82"><a href="#cb14-82" aria-hidden="true" tabindex="-1"></a>        mean_waic.append(np.mean(alpha_waic))</span>
<span id="cb14-83"><a href="#cb14-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-84"><a href="#cb14-84" aria-hidden="true" tabindex="-1"></a>    np.savez(results_file, mean_ge<span class="op">=</span>mean_ge, mean_waic<span class="op">=</span>mean_waic)</span>
<span id="cb14-85"><a href="#cb14-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-86"><a href="#cb14-86" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">5</span>))</span>
<span id="cb14-87"><a href="#cb14-87" aria-hidden="true" tabindex="-1"></a>plt.plot(alphas, mean_ge, <span class="st">'o-'</span>, color<span class="op">=</span><span class="st">'blue'</span>, label<span class="op">=</span><span class="st">'Empirical GE - S'</span>)</span>
<span id="cb14-88"><a href="#cb14-88" aria-hidden="true" tabindex="-1"></a>plt.plot(alphas, mean_waic, <span class="st">'s--'</span>, color<span class="op">=</span><span class="st">'green'</span>, label<span class="op">=</span><span class="st">'Empirical WAIC - Sn'</span>)</span>
<span id="cb14-89"><a href="#cb14-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-90"><a href="#cb14-90" aria-hidden="true" tabindex="-1"></a><span class="co"># Theoretical curve: min(alpha/2, 1/2) / n</span></span>
<span id="cb14-91"><a href="#cb14-91" aria-hidden="true" tabindex="-1"></a>theoretical <span class="op">=</span> np.minimum(alphas <span class="op">/</span> <span class="dv">2</span>, <span class="fl">0.5</span>) <span class="op">/</span> n</span>
<span id="cb14-92"><a href="#cb14-92" aria-hidden="true" tabindex="-1"></a>plt.plot(alphas, theoretical, <span class="st">'k-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="vs">r'Theoretical </span><span class="dv">$</span><span class="er">\</span><span class="vs">lambda/n</span><span class="dv">$</span><span class="vs">'</span>)</span>
<span id="cb14-93"><a href="#cb14-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-94"><a href="#cb14-94" aria-hidden="true" tabindex="-1"></a>plt.axvline(<span class="fl">1.0</span>, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">':'</span>, label<span class="op">=</span><span class="st">'Critical Point $</span><span class="ch">\\</span><span class="st">alpha=1$'</span>)</span>
<span id="cb14-95"><a href="#cb14-95" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Phase Transition in Errors (Analogous to Fig 9.1 &amp; 9.2)"</span>)</span>
<span id="cb14-96"><a href="#cb14-96" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="vs">r"Dirichlet Hyperparameter </span><span class="dv">$</span><span class="ch">\a</span><span class="vs">lpha</span><span class="dv">$</span><span class="vs">"</span>)</span>
<span id="cb14-97"><a href="#cb14-97" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Error"</span>)</span>
<span id="cb14-98"><a href="#cb14-98" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb14-99"><a href="#cb14-99" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb14-100"><a href="#cb14-100" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb14-101"><a href="#cb14-101" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Loading saved results from disk...</code></pre>
</div>
<div class="cell-output cell-output-display">
<div id="phase-transition-errors" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="slt_gaussian_mixture_files/figure-html/phase-transition-errors-output-2.png" width="758" height="468" class="figure-img"></p>
<figcaption>Phase Transition in Average Errors. The Generalization Error and WAIC linearly increase until the critical point <span class="math inline">\(\alpha=1\)</span>, after which they plateau as the model switches to the generic regular-equivalent manifold.</figcaption>
</figure>
</div>
</div>
</div>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/gustavdelius\.github\.io\/Watanabe\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>