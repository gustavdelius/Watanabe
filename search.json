[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Watanabe’s Singular Learning Theory",
    "section": "",
    "text": "Welcome. This website contains some notes taken while exploring Sumio Watanabe’s Mathematical Theory of Bayesian Statistics (Singular Learning Theory) as well as simulations, mostly written by Gemini 3 Pro."
  },
  {
    "objectID": "index.html#contents",
    "href": "index.html#contents",
    "title": "Watanabe’s Singular Learning Theory",
    "section": "Contents",
    "text": "Contents\n\nCheat Sheet: Important Definitions and Relations\nRegular and Singular Models (Reproducing Figures 1.2 to 1.5)\nIllustrating Watanabe’s Singular Learning Theory with a Gaussian Mixture Model\nNeural Network (Reproducing Figure 2.7)"
  },
  {
    "objectID": "cheat_sheet.html",
    "href": "cheat_sheet.html",
    "title": "Cheat Sheet: Important Definitions",
    "section": "",
    "text": "This page provides a quick reference for the most important definitions and relations in Sumio Watanabe’s Mathematical Theory of Bayesian Statistics and Singular Learning Theory (SLT), together with pointers to where they are introduced in the book."
  },
  {
    "objectID": "cheat_sheet.html#core-concepts",
    "href": "cheat_sheet.html#core-concepts",
    "title": "Cheat Sheet: Important Definitions",
    "section": "Core Concepts",
    "text": "Core Concepts\nThroughout we will consider a triplet \\((q, p, \\varphi)\\), where\n\n\\(q(x)\\) is the true distribution,\n\\(p(x|w)\\) is the parametric model, and\n\\(\\varphi(w)\\) is the prior distribution over the parameter space \\(W \\subset \\mathbb{R}^d\\).\n\nThe Log Likelihood Ratio \\(K(w)\\) measures the discrepancy between the true distribution and the parametric model (See page 78): \\[ K(w) = \\int q(x) \\log \\frac{q(x)}{p(x|w)} dx \\] This is the Kullback-Leibler (KL) divergence between \\(q(x)\\) and \\(p(x|w)\\).\nThe average loss function is given by (See page 68): \\[ L(w) = \\int q(x) \\log \\frac{p(x|w)}{q(x)} dx = S + K(w)\\] where \\[ S = -\\int q(x) \\log q(x) dx \\] is the true entropy (See page 17).\nWe denote the set of optimal parameters by \\(W_0 = \\{w \\in W : K(w) = min_{w' \\in W} K(w') \\}\\) (See page 68)."
  },
  {
    "objectID": "cheat_sheet.html#regular-vs.-singular-models",
    "href": "cheat_sheet.html#regular-vs.-singular-models",
    "title": "Cheat Sheet: Important Definitions",
    "section": "Regular vs. Singular Models",
    "text": "Regular vs. Singular Models\n\nFisher Information Matrix\nThe Fisher information matrix \\(I(w)\\) evaluated at parameter \\(w\\) is a \\(d \\times d\\) matrix with entries (See page 32): \\[ I_{ij}(w) = \\int q(x) \\left( \\frac{\\partial \\log p(x|w)}{\\partial w_i} \\right) \\left( \\frac{\\partial \\log p(x|w)}{\\partial w_j} \\right) dx \\] Alternatively, under certain regularity conditions, it can be expressed as the expected negative Hessian of the log-likelihood (See page 32): \\[ I_{ij}(w) = -\\int q(x) \\frac{\\partial^2 \\log p(x|w)}{\\partial w_i \\partial w_j} dx \\] This is equivalent to the Hessian of the log-likelihood ratio \\(K(w)\\) (See page 107): \\[ I_{ij}(w) = \\frac{\\partial^2 K(w)}{\\partial w_i \\partial w_j} \\]\n\n\nRegular Model\nA statistical model is regular if the map from parameters to probability distributions (\\(w \\mapsto p(x|w)\\)) is one-to-one, and the Fisher information matrix is positive definite at the unique optimal parameter \\(w_0\\). Regular models satisfy classical asymptotic theories.\nIn a regular model, the Taylor expansion of \\(K(w)\\) around the optimal parameter \\(w_0\\) is dominated by the Fisher information matrix (See page 107): \\[ K(w) \\approx \\frac{1}{2} (w - w_0)^\\top I(w_0) (w - w_0) \\]\n\n\nSingular Model\nA statistical model is singular if it is not regular. In singular models, the Fisher information matrix is singular (not positive definite) at the true parameters, or the parameter-to-distribution mapping is many-to-one. Examples include neural networks, Gaussian mixture models, hidden Markov models, and Bayesian networks.\nIn a singular model, \\(I(w_0)\\) is singular, making the quadratic approximation to \\(K(w)\\) degenerate."
  },
  {
    "objectID": "cheat_sheet.html#bayesian-inference",
    "href": "cheat_sheet.html#bayesian-inference",
    "title": "Cheat Sheet: Important Definitions",
    "section": "Bayesian Inference",
    "text": "Bayesian Inference\nGiven \\(n\\) independent observations \\(X^n = (X_1, \\dots, X_n)\\) from \\(q(x)\\), the Partition Function (Marginal Likelihood) is the probability of observing the data \\(X^n = (X_1, \\dots, X_n)\\) given the model and the prior (See page 21): \\[ Z_n = \\int \\prod_{i=1}^n p(X_i|w) \\varphi(w) dw \\]\nThe Free Energy is the negative log of the marginal likelihood (See page 22): \\[ F_n = -\\log Z_n \\] In SLT, the asymptotic behavior of the free energy is a central object of study. For regular models, \\(F_n \\approx n T_n(\\hat{w}) + \\frac{d}{2} \\log n\\). For singular models, it requires algebraic geometry to analyze.\nThe Posterior Distribution is (See page 7): \\[ p(w|X^n) = \\frac{1}{Z_n} \\prod_{i=1}^n p(X_i|w) \\varphi(w) \\]\nThe Predictive Distribution is (See page 8): \\[ p(x|X^n) = \\int p(x|w) p(w|X^n) dw \\]"
  },
  {
    "objectID": "cheat_sheet.html#losses-and-errors",
    "href": "cheat_sheet.html#losses-and-errors",
    "title": "Cheat Sheet: Important Definitions",
    "section": "Losses and Errors",
    "text": "Losses and Errors\nThe Generalization Loss is the expected negative log-likelihood of a new data point \\(X\\) drawn from \\(q(x)\\), evaluated using the predictive distribution (See page 17): \\[ G_n = -\\int q(x) \\log p(x|X^n) dx \\] The expected generalization loss is exactly the difference in expected free energies (See page 23): \\[ \\mathbb{E}[G_n] = \\mathbb{E}[F_{n+1}] - \\mathbb{E}[F_n] \\]\nThe Generalization Error is the difference between the generalization loss and the true entropy (See page 17): \\[ \\text{Generalization Error} = G_n - S = \\int q(x) \\log \\frac{q(x)}{p(x|X^n)} dx = K(q(x)||p(x|X^n))\\] where \\(K(q(x)||p(x|X^n))\\) is the Kullback-Leibler divergence between \\(q(x)\\) and \\(p(x|X^n)\\).\nThe Training Loss is the empirical negative log-likelihood of the training data \\(X^n\\), evaluated using the predictive distribution (See page 17): \\[ T_n = -\\frac{1}{n} \\sum_{i=1}^n \\log p(X_i|X^n) \\]\nThe Training Error is the difference between the training loss and the empirical entropy (See page 17): \\[ \\text{Training Error} = T_n - S_n \\] where \\(S_n\\) is the empirical entropy (See page 17): \\[ S_n = -\\frac{1}{n} \\sum_{i=1}^n \\log q(X_i) \\]\nIn general, training loss underestimates generalization loss due to overfitting to the specific sample (See page 18): \\[ \\mathbb{E}[T_n] &lt; \\mathbb{E}[G_n] \\]\nThe Cross-Validation Loss estimates generalization loss by evaluating each training point on the predictive distribution formed by the remaining \\(n-1\\) points (See page 18): \\[ C_n = -\\frac{1}{n} \\sum_{i=1}^n \\log p(X_i|X^n \\setminus \\{X_i\\}) \\] Cross-validation estimates the generalization loss of a model trained on \\(n-1\\) samples (See page 18): \\[ \\mathbb{E}[C_n] = \\mathbb{E}[G_{n-1}] \\] The Cross-Validation Error is the difference between the cross-validation loss and the empirical entropy (See page 18): \\[ \\text{Cross-Validation Error} = C_n - S_n \\]"
  },
  {
    "objectID": "cheat_sheet.html#information-criteria",
    "href": "cheat_sheet.html#information-criteria",
    "title": "Cheat Sheet: Important Definitions",
    "section": "Information Criteria",
    "text": "Information Criteria\nThe WAIC (Widely Applicable Information Criterion) is an estimator of the generalization loss that relies on the log-posterior predictive density (training loss) and the empirical variance of the log-likelihood over the posterior. WAIC is asymptotically equivalent to the leave-one-out cross-validation loss and works well for both regular and singular models. \\[ WAIC_n = T_n + \\frac{V_n}{n} \\] where the functional variance \\(V_n\\) is (See page 22): \\[ V_n = \\sum_{i=1}^n \\left( \\mathbb{E}_{w|X^n}[(\\log p(X_i|w))^2] - (\\mathbb{E}_{w|X^n}[\\log p(X_i|w)])^2 \\right) \\]\nWAIC is an asymptotically unbiased estimator of the expected generalization Loss even in singular models (See page 22): \\[ \\mathbb{E}[WAIC_n] = \\mathbb{E}[G_n] + O\\left(\\frac{1}{n^2}\\right) \\] Additionally, WAIC and the Leave-One-Out Cross-Validation Loss are asymptotically equivalent (See page 23): \\[ WAIC_n = C_n + O_p\\left(\\frac{1}{n^2}\\right) \\]\nThe WBIC (Widely Applicable Bayesian Information Criterion) is an estimator of the free energy \\(F_n\\) that works for both regular and singular models. It is calculated by taking the average of the log-likelihood over a tempered posterior distribution with inverse temperature \\(\\beta = 1/\\log n\\). It generalizes the Bayesian Information Criterion (BIC) (See page 246).\nThe WBIC provides an asymptotically accurate approximation of the marginal likelihood / free energy (See page 247): \\[ WBIC_n = \\lambda \\log n + O_p(1) \\] This generalizes the Bayesian Information Criterion (BIC) to singular models, allowing for model selection by choosing the one that minimizes WBIC."
  },
  {
    "objectID": "cheat_sheet.html#singular-learning-theory-slt-quantities",
    "href": "cheat_sheet.html#singular-learning-theory-slt-quantities",
    "title": "Cheat Sheet: Important Definitions",
    "section": "Singular Learning Theory (SLT) Quantities",
    "text": "Singular Learning Theory (SLT) Quantities\nThe Real Log Canonical Threshold (RLCT) is denoted as \\(\\lambda\\). It is a positive rational number that measures the “singularity” of the optimal parameter set \\(W_0\\). It replaces the parameter dimension \\(d/2\\) in the asymptotic expansion of the free energy for singular models. Lower \\(\\lambda\\) implies a more severe singularity and a smaller effective dimension, often leading to better generalization and simpler representations (See page 147).\nThe Multiplicity is denoted as \\(m\\). It is the maximum power of the logarithmic term \\(\\log n\\) that appears alongside the RLCT in the asymptotic expansion of the free energy (See page 147).\nThe asymptotic expansion of the free energy in SLT is (See page 152): \\[ F_n = n S_n + \\lambda \\log n - (m - 1) \\log \\log n + O_p(1) \\]\nThe Resolution of Singularities / Blow-up is an algebraic geometry technique used in SLT. A “blow-up” is a coordinate transformation that resolves singularities in the parameter space, transforming the complex geometric structure of \\(K(w) = 0\\) into a simpler form with normal crossings. This analytic continuation allows the asymptotic evaluation of the marginal likelihood integral.\nThe Zeta Function of Statistical Learning is an analytic function of a complex variable \\(z\\) defined as (See page 151): \\[ \\zeta(z) = \\int K(w)^{-z} \\varphi(w) dw \\] The poles of this zeta function are deeply connected to the RLCT. The largest pole of the zeta function (which is negative) determines the RLCT \\(\\lambda\\).\nThe Singular Fluctuation is the variance of the log-likelihood ratio across the posterior distribution. In regular models, the singular fluctuation is \\(d/2\\), but in singular models, it varies depending on the geometry of the singularities. It governs the difference between training error and generalization error (See page 153)."
  },
  {
    "objectID": "slt_gaussian_mixture.html",
    "href": "slt_gaussian_mixture.html",
    "title": "Illustrating Watanabe’s Singular Learning Theory with a Gaussian Mixture Model",
    "section": "",
    "text": "This document provides a walk-through of Sumio Watanabe’s Singular Learning Theory, using a two-component Gaussian Mixture Model (GMM) as a concrete pedagogical example. We had already seen in Regular and Singular Models that this model is singular."
  },
  {
    "objectID": "slt_gaussian_mixture.html#introduction-the-model-and-the-true-distribution-chapter-1",
    "href": "slt_gaussian_mixture.html#introduction-the-model-and-the-true-distribution-chapter-1",
    "title": "Illustrating Watanabe’s Singular Learning Theory with a Gaussian Mixture Model",
    "section": "1. Introduction: The Model and the True Distribution (Chapter 1)",
    "text": "1. Introduction: The Model and the True Distribution (Chapter 1)\nIn statistical formulation, we consider a learning machine defined by a parametric statistical model \\(p(x|w)\\) and a true data-generating distribution \\(q(x)\\).\n\nThe Two-Component Gaussian Mixture\nLet’s define a simple 2-parameter Gaussian Mixture Model where one component is fixed at the origin: \\[ p(x|w) = (1-a) \\mathcal{N}(x|0, 1) + a \\mathcal{N}(x|\\mu, 1) \\] Here, the parameter vector is \\(w = (a, \\mu) \\in W = [0, 1] \\times [-c, c]\\).\nSuppose the true distribution is simply a standard normal distribution: \\[ q(x) = \\mathcal{N}(x|0, 1) \\]\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-4, 4, 400)\nq = np.exp(-x**2/2)/np.sqrt(2*np.pi)\np = 0.7 * np.exp(-x**2/2)/np.sqrt(2*np.pi) + 0.3 * np.exp(-(x-1.5)**2/2)/np.sqrt(2*np.pi)\n\nplt.figure(figsize=(8, 5))\nplt.plot(x, q, label=r'True Distribution $q(x) = \\mathcal{N}(0,1)$', color='blue', linewidth=2)\nplt.plot(x, p, label=r'Model $p(x|w)$ with $a=0.3, \\mu=1.5$', color='red', linestyle='--')\nplt.title(\"Gaussian Mixture Model vs True Distribution\")\nplt.xlabel(\"x\")\nplt.ylabel(\"Density\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: True Distribution vs Model\n\n\n\n\n\n\n\nThe Set of True Parameters\nThe true distribution is realized by the model whenever \\(p(x|w) = q(x)\\). By inspecting the equation: \\[ (1-a) \\mathcal{N}(x|0, 1) + a \\mathcal{N}(x|\\mu, 1) = \\mathcal{N}(x|0, 1) \\] \\[ a (\\mathcal{N}(x|\\mu, 1) - \\mathcal{N}(x|0, 1)) = 0 \\]\nThis holds true if and only if \\(a = 0\\) (the mixing proportion is zero) OR \\(\\mu = 0\\) (both components are identical). Thus, the set of true parameters \\(W_0\\) is: \\[ W_0 = \\{ (a, \\mu) \\in W : a = 0 \\text{ or } \\mu = 0 \\} \\]\nThis means that \\(W_0\\) is not a single point, but the union of two intersecting lines. In classical (regular) statistical theory, \\(W_0\\) is assumed to be a single point, and the Fisher Information Matrix is positive definite. Here, the Fisher Information Matrix degenerates on \\(W_0\\), making this a singular model.\n\n\nThe Kullback-Leibler Divergence\nThe log-likelihood ratio (empirical loss) is driven by the Kullback-Leibler (KL) divergence from \\(q(x)\\) to \\(p(x|w)\\): \\[ K(w) = \\int q(x) \\log \\frac{q(x)}{p(x|w)} dx \\]\nUsing a Taylor expansion for small \\(a\\) and \\(\\mu\\), we can approximate \\(p(x|w)\\): \\[ p(x|w) = \\mathcal{N}(x|0, 1) \\left[ 1 + a (e^{\\mu x - \\mu^2/2} - 1) \\right] \\approx \\mathcal{N}(x|0, 1) \\left[ 1 + a\\left(\\mu x + \\frac{1}{2}\\mu^2(x^2 - 1)\\right) \\right] \\]\nPlugging this into the KL divergence and using \\(-\\log(1+z) \\approx -z + z^2/2\\), the linear terms integrate to 0 under \\(q(x)\\), leaving the leading non-zero term: \\[ K(w) \\approx \\frac{1}{2} a^2 \\mu^2 \\]\n\n\nCode\nA = np.linspace(-1, 1, 400)\nM = np.linspace(-2, 2, 400)\nAA, MM = np.meshgrid(A, M)\nK = 0.5 * (AA**2) * (MM**2)\n\nplt.figure(figsize=(8, 6))\ncontour = plt.contourf(AA, MM, K, levels=20, cmap='viridis_r')\nplt.colorbar(contour, label=r'KL Divergence $K(a, \\mu)$')\nplt.axhline(0, color='red', linewidth=3, label='True Parameters ($a=0$)')\nplt.axvline(0, color='red', linewidth=3, label=r'True Parameters ($\\mu=0$)')\nplt.title(\"Parameter Space and KL Divergence\")\nplt.xlabel(\"Parameter $a$\")\nplt.ylabel(r\"Parameter $\\mu$\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2: Parameter Space and KL Divergence. Notice how the valley of \\(K(w)=0\\) forms a cross at \\(a=0\\) and \\(\\mu=0\\)."
  },
  {
    "objectID": "slt_gaussian_mixture.html#resolution-of-singularities-chapter-2",
    "href": "slt_gaussian_mixture.html#resolution-of-singularities-chapter-2",
    "title": "Illustrating Watanabe’s Singular Learning Theory with a Gaussian Mixture Model",
    "section": "2. Resolution of Singularities (Chapter 2)",
    "text": "2. Resolution of Singularities (Chapter 2)\nBecause the set of true parameters \\(W_0\\) has a singularity (an intersection forming a cross), standard asymptotic expansions (like the Laplace approximation) fail. Watanabe employs Hironaka’s Theorem on the Resolution of Singularities from algebraic geometry to resolve this.\nThe theorem states that there exists a real analytic manifold \\(\\mathcal{M}\\) and a proper analytic map \\(g: \\mathcal{M} \\to W\\) (a “blow-up”) such that the composition \\(K(g(u))\\) has a simple normal crossing form.\n\nThe Blow-Up Transformation\nFor the approximation \\(K(w) \\approx \\frac{1}{2} a^2 \\mu^2\\), the true parameters \\(W_0\\) correspond to the crossing lines \\(a=0\\) and \\(\\mu=0\\). To resolve this intersection, we apply a “blow-up” transformation. A blow-up geometrically replaces the problematic intersection point (the origin) with an entire line (called the exceptional divisor), separating the paths that cross there.\nWe can reparameterize the space by keeping track of the slope of lines passing through the origin. We define a local coordinate chart (a directional blow-up) as: \\[ a = u_1 \\] \\[ \\mu = u_1 u_2 \\]\nHere, \\(u_1\\) simply represents the original \\(a\\) coordinate, while \\(u_2 = \\mu / a\\) represents the slope of a line passing through the origin in the \\((a, \\mu)\\) parameter space.\n\nA single point in the original space—the origin \\((a=0, \\mu=0)\\)—now corresponds to the entire line \\(u_1 = 0\\) for any value of \\(u_2\\) in the new space.\nThe crossing lines in the \\((a, \\mu)\\) space have been pulled apart. Actually, our new coordinate system does not extend to the line \\(a=0\\), but we can get arbitrarily close to it. We would need another patch with coordinates \\(\\mu, a/\\mu\\) to represent the line \\(a=0\\).\n\nThe KL divergence in these new coordinates \\(u = (u_1, u_2)\\) is \\[ K(g(u)) \\approx \\frac{1}{2} (u_1)^2 (u_1 u_2)^2 = \\frac{1}{2} u_1^4 u_2^2 \\]\nWe must also account for the distortion of the volume measure, dictated by the Jacobian of \\(g\\): \\[ dw = |g'(u)| du = \\left| \\det \\begin{pmatrix} 1 & 0 \\\\ u_2 & u_1 \\end{pmatrix} \\right| du_1 du_2 = |u_1| du_1 du_2 \\]"
  },
  {
    "objectID": "slt_gaussian_mixture.html#standard-form-and-real-log-canonical-threshold-chapter-3",
    "href": "slt_gaussian_mixture.html#standard-form-and-real-log-canonical-threshold-chapter-3",
    "title": "Illustrating Watanabe’s Singular Learning Theory with a Gaussian Mixture Model",
    "section": "3. Standard Form and Real Log Canonical Threshold (Chapter 3)",
    "text": "3. Standard Form and Real Log Canonical Threshold (Chapter 3)\nIn Chapter 3, Watanabe introduces the concept of the Real Log Canonical Threshold (RLCT), denoted by \\(\\lambda\\), and its multiplicity \\(m\\). These two algebraic invariants completely govern the asymptotic behavior of the learning machine.\nTo understand why the blow-up was a strictly necessary algebraic maneuver, we have to look at how \\(\\lambda\\) is formally calculated. By definition, the RLCT is found by examining the analytic continuation of the zeta function of the statistical model, given by the integral: \\[ \\zeta(z) = \\int (K(w))^z \\varphi(w) dw \\] where \\(\\varphi(w)\\) is the prior distribution and \\(z \\in \\mathbb{C}\\) (\\(\\Re(z) &gt; 0\\)). The RLCT \\(\\lambda\\) is defined such that \\(-\\lambda\\) is the largest (closest to zero) real pole of this function, and its multiplicity \\(m\\) is the order of this pole.\nWithout the blow-up, evaluating this integral and finding its poles is mathematically intractable. The true KL divergence \\(K(w)\\) isn’t just a simple polynomial like \\(a^2\\mu^2\\); it contains an infinite series of higher-order terms from the Taylor expansion. Because the variables are coupled in a highly non-linear way at the singularity (the cross \\(a=0, \\mu=0\\)), you cannot separate the variables to evaluate the integral.\nThis is where the blow-up of the singularity resolves the integration problem. Hironaka’s Theorem guarantees that after passing to the resolved coordinates \\(u\\), the fully complex divergence \\(K(g(u))\\) perfectly factors into a single monomial multiplied by a non-vanishing positive analytic function \\(b(u) &gt; 0\\). The prior measure and Jacobian of the blow-up also become a simple monomial multiplied by a strictly positive function \\(c(u) &gt; 0\\).\nWhen we substitute this Standard Form into the zeta function integral using our resolved coordinates \\(u\\), the variables completely decouple near the origin: \\[ \\zeta(z) = \\int \\left( u_1^{2k_{1}} u_2^{2k_{2}} b(u) \\right)^z \\left( u_1^{h_1} u_2^{h_2} c(u) \\right) du_1 du_2 \\] \\[ \\zeta(z) \\approx C \\left( \\int u_1^{2k_1 z + h_1} du_1 \\right) \\left( \\int u_2^{2k_2 z + h_2} du_2 \\right) \\]\nEvaluating these independent 1D integrals yields formulas of the type: \\[ \\int_0^\\epsilon u^{2k z + h} du = \\frac{\\epsilon^{2k z + h + 1}}{2k z + h + 1} \\] This expression clearly has a pole exactly when the denominator is zero, i.e., at \\(z = -\\frac{h + 1}{2k}\\).\nBy separating the variables, the blow-up perfectly isolates the poles of the zeta function!\nUsing our specific factors from the GMM blow-up:\n\nDivergence function: \\(K(u) = u_1^4 u_2^2\\) (so \\(k_1 = 2\\), \\(k_2 = 1\\))\nPrior measure / Jacobian: \\(\\Phi(u) = u_1^1 u_2^0\\) (so \\(h_1 = 1\\), \\(h_2 = 0\\)). Note: This assumes a prior \\(\\varphi(w)\\) that does not vanish at the origin, such as a uniform prior \\(\\varphi(w) \\propto 1\\). For such a prior, the only algebraic zeros in the integral’s measure come from the Jacobian of the blow-up \\(|u_1|\\). This gives us \\(u_1^1 u_2^0\\), resulting in \\(h_1 = 1, h_2 = 0\\).\n\nThe candidate poles \\(z = -\\lambda_j\\) along each coordinate axis \\(j\\) give us: \\[ \\lambda_j = \\frac{h_j + 1}{2k_j} \\]\nFor our \\(u_1\\) and \\(u_2\\):\n\n\\(\\lambda_1 = \\frac{1 + 1}{4} = \\frac{1}{2}\\)\n\\(\\lambda_2 = \\frac{0 + 1}{2} = \\frac{1}{2}\\)\n\nThe overall RLCT \\(\\lambda\\) is dictated by the pole closest to zero, which is the minimum of these values: \\[ \\lambda = \\min(\\lambda_1, \\lambda_2) = \\min(1/2, 1/2) = \\frac{1}{2} \\]\nThe multiplicity \\(m\\) is the order of this leading pole, which equals the number of coordinate indices \\(j\\) that achieve this minimum. Here, both \\(\\lambda_1\\) and \\(\\lambda_2\\) equal \\(1/2\\). Thus: \\[ m = 2 \\]\nNote: In a regular model with parameter dimension \\(d=2\\), the RLCT is always \\(\\lambda = d/2 = 1\\). Our singular GMM has \\(\\lambda = 1/2 &lt; 1\\), showcasing the mathematical definition of a singular learning machine."
  },
  {
    "objectID": "slt_gaussian_mixture.html#singular-fluctuation-and-free-energy-chapter-4",
    "href": "slt_gaussian_mixture.html#singular-fluctuation-and-free-energy-chapter-4",
    "title": "Illustrating Watanabe’s Singular Learning Theory with a Gaussian Mixture Model",
    "section": "4. Singular Fluctuation and Free Energy (Chapter 4)",
    "text": "4. Singular Fluctuation and Free Energy (Chapter 4)\nIn Bayesian evaluation, the Stochastic Complexity or Free Energy \\(F_n\\) represents the negative log-marginal likelihood (evidence) of the data \\(X^n\\): \\[ F_n = -\\log Z_n = -\\log \\int e^{-n L_n(w)} \\varphi(w) dw \\] where \\(L_n(w) = -\\frac{1}{n} \\sum \\log p(X_i|w)\\) is the empirical loss.\nChapter 4 defines how the parameter posterior behaves under singular fluctuations. Using the algebraic invariants we just found, SLT proves that the free energy asymptotically expands as:\n\\[ F_n \\approx n L_n(w_0) + \\lambda \\log n - (m-1) \\log \\log n + O_p(1) \\]\nFor our two-component Gaussian Mixture Model: \\[ F_n \\approx n L_n(w_0) + \\frac{1}{2} \\log n - \\log \\log n + O_p(1) \\]\n\n\nCode\nn = np.logspace(1, 4, 100)\nF_reg = 1.0 * np.log(n)\nF_sing = 0.5 * np.log(n) - 1.0 * np.log(np.log(n + 1.1))\n\nplt.figure(figsize=(8, 5))\nplt.plot(n, F_reg, label=r'Regular Model ($d=2 \\Rightarrow \\lambda=1$)', color='red')\nplt.plot(n, F_sing, label=r'Singular Model (GMM, $\\lambda=0.5, m=2$)', color='blue')\nplt.title(\"Asymptotic Free Energy Penalty vs Sample Size\")\nplt.xlabel(\"Sample Size $n$ (log scale)\")\nplt.ylabel(\"Penalty Term\")\nplt.xscale('log')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 3: Asymptotic Free Energy. The penalty term for the singular model is significantly smaller than for a regular model, making singular models heavily preferred by the marginal likelihood.\n\n\n\n\n\nThis result breaks the classical Bayesian Information Criterion (BIC), which assumes a penalty of \\(\\frac{d}{2} \\log n\\). BIC would have penalized this 2-parameter model by \\(1.0 \\log n\\), entirely missing the true Bayesian Occam’s Razor effect on the singular manifold."
  },
  {
    "objectID": "slt_gaussian_mixture.html#generalization-and-training-errors-chapter-5",
    "href": "slt_gaussian_mixture.html#generalization-and-training-errors-chapter-5",
    "title": "Illustrating Watanabe’s Singular Learning Theory with a Gaussian Mixture Model",
    "section": "5. Generalization and Training Errors (Chapter 5)",
    "text": "5. Generalization and Training Errors (Chapter 5)\nChapter 5 links the geometry of the parameter space to the average errors. Let \\(G_n\\) be the Generalization Error (expected loss on new data) and \\(T_n\\) be the Training Error (empirical loss on the training set).\nIn traditional regular statistics (like AIC), the expectation of these errors relies on \\(d\\): \\[ \\mathbb{E}[G_n] = L(w_0) + \\frac{d}{2n} \\] \\[ \\mathbb{E}[T_n] = L(w_0) - \\frac{d}{2n} \\]\nIn Singular Learning Theory, Watanabe elegantly proves a symmetry relation using \\(\\lambda\\): \\[ \\mathbb{E}[G_n] = L(w_0) + \\frac{\\lambda}{n} \\] \\[ \\mathbb{E}[T_n] = L(w_0) - \\frac{\\lambda}{n} \\]\nFor our continuous GMM, \\(\\lambda = 1/2\\). Therefore:\n\nExpected Generalization Error converges with rate \\(\\frac{0.5}{n}\\).\nExpected Training Error converges with rate \\(-\\frac{0.5}{n}\\).\n\nThis explains why heavily overparameterized singular models (like deep neural networks and complex mixtures) often generalize better than their parameter count \\(d\\) would imply; their learning dynamics are constrained by the smaller geometric invariant \\(\\lambda &lt; d/2\\)."
  },
  {
    "objectID": "slt_gaussian_mixture.html#asymptotic-expansion-and-waic-chapter-6",
    "href": "slt_gaussian_mixture.html#asymptotic-expansion-and-waic-chapter-6",
    "title": "Illustrating Watanabe’s Singular Learning Theory with a Gaussian Mixture Model",
    "section": "6. Asymptotic Expansion and WAIC (Chapter 6)",
    "text": "6. Asymptotic Expansion and WAIC (Chapter 6)\nFinally, Chapter 6 resolves a crucial practical problem. Since calculating \\(\\lambda\\) analytically requires algebraic blow-ups (which is impossible for massive modern models like LLMs), we cannot use it directly to estimate the generalization error. Furthermore, cross-validation can be unstable in singular models.\nWatanabe introduces the Widely Applicable Information Criterion (WAIC): \\[ \\text{WAIC} = T_n + \\frac{1}{n} \\sum_{i=1}^n V_w \\left( \\log p(X_i | w) \\right) \\] Where \\(V_w\\) is the posterior variance of the log-likelihood for data point \\(X_i\\).\nThe core theorem of Chapter 6 proves that WAIC is an asymptotically unbiased estimator of the generalization error, even for singular models: \\[ \\mathbb{E}[\\text{WAIC}] \\approx \\mathbb{E}[G_n] \\]\nIn the context of our GMM, evaluating the posterior variance computationally via Markov Chain Monte Carlo (MCMC) allows us to estimate the true hold-out performance without needing to analytically find \\(\\lambda = 1/2\\) or \\(m=2\\), proving WAIC’s universal applicability in modern deep learning and mixture modeling.\n\nCalculating WAIC in Practice\nTo make this concrete, let’s generate a synthetic dataset from the true distribution \\(q(x) = \\mathcal{N}(0, 1)\\) and compute the WAIC for our toy GMM. By using a fine grid covering our parameter space \\(W = [0, 1] \\times [-2, 2]\\), we can exactly compute the posterior and WAIC without relying on intricate MCMC setups.\n\n\nCode\nimport numpy as np\n\n# 1. Generate synthetic data from true distribution q(x) = N(0,1)\nnp.random.seed(42)\nn = 100\nX = np.random.normal(0, 1, n)\n\n# 2. Define parameter grid for evaluating the posterior\na_vals = np.linspace(0, 1, 100)\nmu_vals = np.linspace(-2, 2, 100)\nA, MU = np.meshgrid(a_vals, mu_vals)\n\n# p(x|a, mu) = (1-a)*N(x|0,1) + a*N(x|mu,1)\ndef norm_pdf(x, m, s):\n    return np.exp(-0.5 * ((x - m) / s)**2) / (np.sqrt(2 * np.pi) * s)\n\n# 3. Compute log-likelihood for each parameter pair and each data point\nlog_p = np.zeros((n, len(mu_vals), len(a_vals)))\nfor i, x in enumerate(X):\n    p_x_w = (1 - A) * norm_pdf(x, 0, 1) + A * norm_pdf(x, MU, 1)\n    # Add a small epsilon to prevent log(0)\n    log_p[i] = np.log(p_x_w + 1e-12)\n\n# 4. Compute unnormalized posterior probabilities (assuming uniform prior)\n# Sum over all data points to get the full log-likelihood for each parameter pair\ntotal_log_likelihood = np.sum(log_p, axis=0) \n# Subtract maximum for numerical stability before exponentiating\nlog_posterior = total_log_likelihood - np.max(total_log_likelihood)\nposterior = np.exp(log_posterior)\n# Normalize to create a valid probability distribution over the grid\nposterior /= np.sum(posterior) \n\n# 5. Calculate WAIC\npredictive_density = np.zeros(n)\nV_w = np.zeros(n)\n\nfor i in range(n):\n    # Expected volume: E_w[p(X_i|w)]\n    predictive_density[i] = np.sum(np.exp(log_p[i]) * posterior)\n    \n    # Posterior variance of the log-likelihood: V_w(log p(X_i|w))\n    expected_log_p = np.sum(log_p[i] * posterior)\n    expected_log_p_sq = np.sum((log_p[i]**2) * posterior)\n    V_w[i] = expected_log_p_sq - expected_log_p**2\n\n# T_n: Empirical training loss of the Bayes predictive distribution\nT_n = -1.0 / n * np.sum(np.log(predictive_density + 1e-12))\n\n# Functional variance penalty term\n# This corresponds to (1/n) * sum(V_w)\npenalty = np.mean(V_w)\n\nwaic = T_n + penalty\n\n# 6. Calculate True Generalization Error on massive holdout\nn_test = 20000\nX_test = np.random.normal(0, 1, n_test)\n\npred_dens_test = np.zeros(n_test)\nbatch_size = 2000\nfor b in range(0, n_test, batch_size):\n    end_idx = min(b + batch_size, n_test)\n    X_batch = X_test[b:end_idx]\n    X_expanded = X_batch[:, np.newaxis, np.newaxis]\n    p_batch = (1 - A) * norm_pdf(X_expanded, 0, 1) + A * norm_pdf(X_expanded, MU, 1)\n    expected_p = np.sum(p_batch * posterior, axis=(1, 2))\n    pred_dens_test[b:end_idx] = expected_p\n\ngen_error = -1.0 / n_test * np.sum(np.log(pred_dens_test + 1e-12))\n\nprint(f\"Sample size n = {n}\")\nprint(f\"Training Error (T_n) = {T_n:.4f}\")\nprint(f\"Penalty Term (V / n) = {penalty:.4f}\")\nprint(f\"WAIC = {waic:.4f}\")\nprint(f\"Generalization Error = {gen_error:.4f}\")\n\n\nSample size n = 100\nTraining Error (T_n) = 1.3310\nPenalty Term (V / n) = 0.0057\nWAIC = 1.3367\nGeneralization Error = 1.4243\n\n\n\n\nVisualizing the Posterior\nWe can also visualize the posterior distribution over our parameter grid to see how it concentrates around the true parameters. Because this is a singular model, the asymptotic distribution does not look like a neat Gaussian blob—instead, it concentrates along the non-identifiable manifold (the cross at \\(a=0\\) or \\(\\mu=0\\)).\n\n\nCode\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(8, 6))\ncontour = plt.contourf(A, MU, posterior, levels=20, cmap='viridis')\nplt.colorbar(contour, label='Posterior Probability')\nplt.axhline(0, color='red', linestyle='--', alpha=0.5, label='True Parameters ($a=0$)')\nplt.axvline(0, color='red', linestyle='--', alpha=0.5, label=r'True Parameters ($\\mu=0$)')\nplt.title(\"Posterior Distribution of GMM Parameters\")\nplt.xlabel(\"Parameter $a$\")\nplt.ylabel(r\"Parameter $\\mu$\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 4: Posterior Distribution of GMM Parameters. Notice how the probability mass is drawn out along the axes representing the true parameters.\n\n\n\n\n\n\n\nContrast: Asymptotic Regularity vs. Finite-Sample Singularity\nWhat happens if the true distribution \\(q(x)\\) is actually drawn from a genuine two-component mixture with both \\(a \\neq 0\\) and \\(\\mu \\neq 0\\)? For example, setting \\(a=0.5\\) and \\(\\mu=0.3\\).\nIn this case, the true parameters \\((a_0, \\mu_0)\\) lie in the interior of the parameter space, and there is exactly one parameter combination that matches the distribution. The true parameter space shrinks to a single, unique point. Because the true parameters are uniquely identifiable, the log-likelihood function has a strict minimum at \\((a_0, \\mu_0)\\), and the Hessian (Fisher Information Matrix) becomes positive definite. Asymptotically (\\(n \\to \\infty\\)), the model behaves as a standard regular model.\nIn this theoretical asymptotic scenario: 1. The Real Log Canonical Threshold becomes \\(\\lambda = d/2 = 2/2 = 1.0\\). 2. The multiplicity is \\(m = 1\\). 3. The Bayesian Occam’s Razor penalty term in WAIC should converge to roughly \\(1.0\\times (\\log n)/n\\) for free energy, or \\(1.0/n\\) for generalization error. We expect the functional variance \\(V\\) to approximate this \\(1.0\\) factor (rather than \\(0.5\\)).\nLet’s test this by running our WAIC code again, but generating the data from \\(a=0.5\\) and \\(\\mu=0.3\\):\n\n\nCode\n# 1. Generate synthetic data from a true mixture model\nnp.random.seed(42)\nn_reg = 100\na_true = 0.5\nmu_true = 0.3\n\nz = np.random.binomial(1, a_true, n_reg)\nX_reg = np.zeros(n_reg)\nX_reg[z == 0] = np.random.normal(0, 1, np.sum(z == 0))\nX_reg[z == 1] = np.random.normal(mu_true, 1, np.sum(z == 1))\n\n# 2. Compute log-likelihood\nlog_p_reg = np.zeros((n_reg, len(mu_vals), len(a_vals)))\nfor i, x in enumerate(X_reg):\n    p_x_w = (1 - A) * norm_pdf(x, 0, 1) + A * norm_pdf(x, MU, 1)\n    log_p_reg[i] = np.log(p_x_w + 1e-12)\n\n# 3. Compute posterior\ntotal_log_likelihood_reg = np.sum(log_p_reg, axis=0) \nlog_posterior_reg = total_log_likelihood_reg - np.max(total_log_likelihood_reg)\nposterior_reg = np.exp(log_posterior_reg)\nposterior_reg /= np.sum(posterior_reg) \n\n# 4. Calculate WAIC penalty\npredictive_density_reg = np.zeros(n_reg)\nV_w_reg = np.zeros(n_reg)\nfor i in range(n_reg):\n    predictive_density_reg[i] = np.sum(np.exp(log_p_reg[i]) * posterior_reg)\n    expected_log_p = np.sum(log_p_reg[i] * posterior_reg)\n    expected_log_p_sq = np.sum((log_p_reg[i]**2) * posterior_reg)\n    V_w_reg[i] = expected_log_p_sq - expected_log_p**2\n\nT_n_reg = -1.0 / n_reg * np.sum(np.log(predictive_density_reg + 1e-12))\npenalty_reg = np.mean(V_w_reg)\nwaic_reg = T_n_reg + penalty_reg\n\n# 5. Calculate True Generalization Error\nn_test = 20000\nz_test = np.random.binomial(1, a_true, n_test)\nX_test_reg = np.zeros(n_test)\nX_test_reg[z_test == 0] = np.random.normal(0, 1, np.sum(z_test == 0))\nX_test_reg[z_test == 1] = np.random.normal(mu_true, 1, np.sum(z_test == 1))\n\npred_dens_test_reg = np.zeros(n_test)\nbatch_size = 2000\nfor b in range(0, n_test, batch_size):\n    end_idx = min(b + batch_size, n_test)\n    X_batch = X_test_reg[b:end_idx]\n    X_expanded = X_batch[:, np.newaxis, np.newaxis]\n    p_batch = (1 - A) * norm_pdf(X_expanded, 0, 1) + A * norm_pdf(X_expanded, MU, 1)\n    expected_p = np.sum(p_batch * posterior_reg, axis=(1, 2))\n    pred_dens_test_reg[b:end_idx] = expected_p\n\ngen_error_reg = -1.0 / n_test * np.sum(np.log(pred_dens_test_reg + 1e-12))\n\nprint(f\"Sample size n = {n_reg}\")\nprint(f\"Training Error (T_n) = {T_n_reg:.4f}\")\nprint(f\"Regular Penalty Term (V / n) = {penalty_reg:.4f}\")\nprint(f\"WAIC = {waic_reg:.4f}\")\nprint(f\"Generalization Error = {gen_error_reg:.4f}\")\n\n# 6. Plot the new regular posterior\nplt.figure(figsize=(7, 5))\ncontour3 = plt.contourf(A, MU, posterior_reg, levels=20, cmap='viridis')\nplt.colorbar(contour3, label='Posterior Probability')\nplt.scatter([a_true], [mu_true], color='red', marker='x', s=100, linewidths=3, label='True Parameters')\nplt.title(\"Posterior Distribution (Regular Model)\")\nplt.xlabel(\"Parameter $a$\")\nplt.ylabel(r\"Parameter $\\mu$\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\nSample size n = 100\nTraining Error (T_n) = 1.3483\nRegular Penalty Term (V / n) = 0.0073\nWAIC = 1.3556\nGeneralization Error = 1.4317\n\n\n\n\n\nPosterior Distribution for the ‘Regular’ Case with Finite Sample Size. Because the true parameter \\(\\mu=0.3\\) is very close to the singularity \\(\\mu=0\\), the sample size \\(n=100\\) is not large enough to resolve the parameters perfectly. The posterior is gravitationally pulled into the singularity, maintaining the shape and WAIC penalty of a singular model.\n\n\n\n\n\nThe Gravitational Pull of the Singularity\nNotice what happened in the plot and output! Despite the true model being theoretically regular, the penalty term \\(V \\approx 0.73\\) is far closer to the singular penalty \\(0.5\\) than the regular penalty \\(1.0\\). And the posterior clearly does not look like a neat Gaussian blob; it is smeared out along the \\(\\mu=0\\) axis exactly like the singular case.\nWhy? Because the true parameter \\(\\mu=0.3\\) is extremely close to the singular manifold (\\(\\mu=0\\)). Watanabe’s theory is asymptotic (\\(n \\to \\infty\\)). For a finite sample size of \\(n=100\\), the data simply does not have enough resolution to confidently distinguish the true distribution \\(\\mu=0.3\\) from the singularity at \\(\\mu=0\\). The likelihood spills over into the singularity, and the posterior is “gravitationally pulled” into the non-identifiable manifold, forcing the model to behave like a singular machine. It would take a much larger sample size (e.g., \\(n=10,000\\)) for the regular asymptotic theory to overcome the geometry of the singularity locally.\nThis is a profound realization from Singular Learning Theory: singularities govern the learning dynamics of models in practice, even when the true distribution is slightly off the singularity, because finite samples cannot perfectly resolve the identifiability.\nThe mathematical beauty of Chapter 6 is that WAIC works uniformly in both these cases without adjustment. The functional variance inherently adapts to the finite-sample geometry of the posterior, providing an accurate estimator of the true generalization error regardless of whether the model is trapped in a singularity or has broken free!\n\n\n\nIs WAIC Still Good for Small Samples?\nA natural follow-up question is whether WAIC remains a good estimator of generalization error when the sample size \\(n\\) is very small. The short answer is yes, and this is exactly where WAIC shines compared to classical criteria like the Deviance Information Criterion (DIC).\nClassical criteria like DIC rely on a point estimate of the parameter (usually the posterior mean or mode). In a singular model, the posterior is highly non-Gaussian and often stretched along a non-identifiable manifold (like the cross at \\(\\mu=0\\) or \\(a=0\\)). For a small sample, the posterior mean might land in a region of very low actual probability mass (e.g., in the empty space between two dense arms of the posterior). Because DIC evaluates the likelihood at this single mathematically awkward point, it can wildly misestimate the generalization error.\nWAIC, on the other hand, averages the likelihood over the entire posterior distribution and calculates the functional variance point-by-point. It entirely avoids point estimation."
  },
  {
    "objectID": "slt_gaussian_mixture.html#phase-transition-in-a-hierarchical-model-chapter-9",
    "href": "slt_gaussian_mixture.html#phase-transition-in-a-hierarchical-model-chapter-9",
    "title": "Illustrating Watanabe’s Singular Learning Theory with a Gaussian Mixture Model",
    "section": "7. Phase Transition in a Hierarchical Model (Chapter 9)",
    "text": "7. Phase Transition in a Hierarchical Model (Chapter 9)\nIn Chapter 9, Watanabe describes a Phase Transition in Bayesian statistics. A phase transition occurs when the structure of the posterior distribution changes drastically depending on a generalized hyperparameter. At this critical point, the free energy is often discontinuous or non-differentiable.\n\nExample 67: Normal Mixture Phase Transition\nLet’s adapt Example 67 from the book (pages 279-281) to our 1D model formulation. Consider the same normal mixture model for \\(x \\in \\mathbb{R}\\) with parameter \\(w=(a,\\mu)\\): \\[ p(x|w) = (1 - a) \\mathcal{N}(x|0, 1) + a \\mathcal{N}(x|\\mu, 1) \\]\nSuppose the true distribution is simply the standard normal: \\[ q(x) = \\mathcal{N}(x|0, 1) \\]\nWe place a Dirichlet prior on \\(a\\) with hyperparameter \\(\\alpha &gt; 0\\): \\[ \\varphi(a) \\propto (a(1 - a))^{\\alpha - 1} \\] And a wide Gaussian prior on \\(\\mu\\): \\[ \\varphi(\\mu) \\propto \\exp \\left( -\\frac{\\mu^2}{2\\sigma^2} \\right) \\] with \\(\\sigma = 10\\). We will investigate how the hyperparameter \\(\\alpha\\) induces a phase transition.\n\n\nThe Two Modes of the Model\nTo perfectly match the true distribution \\(q(x) = \\mathcal{N}(x|0, 1)\\), the model requires either:\n\n\\(a = 0\\): In this case, the first component perfectly matches \\(q(x)\\), and the parameter \\(\\mu\\) can be absolutely anything (it is completely “free” because its component has zero weight).\n\\(\\mu = 0\\): In this case, both components are centered at the origin, matching \\(q(x)\\), and the mixing proportion \\(a\\) can be absolutely anything (\\(a\\) is “free”).\n\nThese two regimes represent two intersecting singular manifolds in the parameter space. The Real Log Canonical Threshold (RLCT) is dictated by whichever singularity is “stronger”, which depends on the prior concentration \\(\\alpha\\).\n\n\nCalculating the RLCT\nThe true parameters are given by the union of two branches: \\(a=0\\) (with any \\(\\mu\\)) and \\(\\mu=0\\) (with any \\(a\\)). The overall RLCT is obtained by evaluating the poles along each branch and taking the minimum.\n\nAlong the branch \\(a=0\\) (where \\(\\mu \\neq 0\\) is free): Locally, \\(\\mu\\) is practically a non-zero constant \\(c\\). The KL divergence \\[K(a, \\mu) \\approx \\frac{1}{2} a^2 \\mu^2 \\approx \\frac{1}{2} c^2 a^2 \\propto a^2.\\] Thus, the polynomial power is \\(k=1\\). The prior contains \\((a(1-a))^{\\alpha-1}\\), which behaves like \\(a^{\\alpha-1}\\) as \\(a \\to 0\\). Thus, the measure has a polynomial power \\(h=\\alpha-1\\). The candidate RLCT for this branch is \\[\\lambda_1 = \\frac{h+1}{2k} = \\frac{(\\alpha-1)+1}{2(1)} = \\alpha/2\\].\nAlong the branch \\(\\mu=0\\) (where \\(a \\neq 0\\) is free): Locally, \\(a\\) is practically a non-zero constant \\(c\\). The KL divergence behaves like \\[K(a, \\mu) \\approx \\frac{1}{2} c^2 \\mu^2 \\propto \\mu^2,\\] so \\(k=1\\). The prior behaves like a constant locally with respect to \\(\\mu\\) (since \\(a\\) is near \\(c\\) and \\(\\mu \\approx 0\\)), so \\(h=0\\). The candidate RLCT for this branch is \\[\\lambda_2 = \\frac{h+1}{2k} = \\frac{0+1}{2(1)} = 1/2.\\]\n\nAt the intersection \\((a=0, \\mu=0)\\): A rigorous calculation using Hironaka’s blow-up at the origin produces additional candidate poles (specifically \\(\\frac{\\alpha+1}{4}\\)). However, this intersection pole is never strictly less than the minimum of the two individual branches for any \\(\\alpha &gt; 0\\).\nTherefore, the global RLCT is dictated purely by the minimum of the two branches: \\[ \\lambda = \\min(\\lambda_1, \\lambda_2) = \\min(\\alpha/2, 1/2) \\]\nThis reveals a profound structural shift exactly at \\(\\alpha = 1\\):\n\nFor \\(\\alpha &lt; 1\\) (\\(\\lambda = \\alpha/2\\)), the strongest singularity comes from the first branch. The posterior concentrates around \\(a \\approx 0\\), leaving the parameter \\(\\mu\\) completely free to wander across the wide prior space.\nFor \\(\\alpha &gt; 1\\) (\\(\\lambda = 1/2\\)), the strongest singularity comes from the second branch. The prior naturally pulls \\(a\\) away from the edges, closing off the first strategy. The posterior is forced to satisfy \\(\\mu \\approx 0\\), leaving \\(a\\) free to vary.\n\nThis abrupt change from “\\(\\mu\\) is free” to “\\(a\\) is free” at \\(\\alpha = 1\\) is the phase transition. Let’s visualize this drastic change in the shape of the posterior distribution.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef log_prior(a, mu, alpha):\n    if a &lt;= 0 or a &gt;= 1: return -np.inf\n    lp_a = (alpha - 1) * np.log(a * (1 - a))\n    lp_mu = -0.5 * (mu / 10.0)**2\n    return lp_a + lp_mu\n\ndef log_likelihood(X, a, mu):\n    p0 = np.exp(-0.5 * X**2) / np.sqrt(2 * np.pi)\n    pmu = np.exp(-0.5 * (X - mu)**2) / np.sqrt(2 * np.pi)\n    p = (1 - a) * p0 + a * pmu + 1e-15\n    return np.sum(np.log(p))\n\ndef sample_mcmc(alpha, n=100, iters=25000):\n    np.random.seed(42)\n    X = np.random.normal(0, 1, n)\n    \n    # Initialize near the regular manifold to avoid getting stuck early\n    a = 0.5\n    mu = 1.0\n    \n    ll = log_likelihood(X, a, mu)\n    lp = log_prior(a, mu, alpha)\n    \n    samples_a = np.zeros(iters)\n    samples_mu = np.zeros(iters)\n    \n    a_step, mu_step = 0.1, 0.5\n    for i in range(iters):\n        a_new = a + np.random.normal(0, a_step)\n        mu_new = mu + np.random.normal(0, mu_step)\n        \n        if 0 &lt; a_new &lt; 1:\n            lp_new = log_prior(a_new, mu_new, alpha)\n            ll_new = log_likelihood(X, a_new, mu_new)\n            \n            if np.log(np.random.rand()) &lt; (ll_new + lp_new - ll - lp):\n                a, mu = a_new, mu_new\n                ll, lp = ll_new, lp_new\n                \n        samples_a[i] = a\n        samples_mu[i] = mu\n        \n    # Discard burn-in\n    return samples_a[5000:], samples_mu[5000:]\n\nfig, axes = plt.subplots(2, 3, figsize=(10, 6), sharey='row')\nalphas_to_test = [0.2, 1.0, 3.0]\n\nfor i, alpha in enumerate(alphas_to_test):\n    sa, smu = sample_mcmc(alpha)\n    \n    axes[0, i].hist(sa, bins=40, density=True, color='skyblue', edgecolor='black')\n    axes[0, i].set_title(rf\"$\\alpha = {alpha}$\")\n    axes[0, i].set_xlim(0, 1)\n    if i == 0: axes[0, i].set_ylabel(\"Posterior of $a$\")\n    \n    axes[1, i].hist(np.abs(smu), bins=40, density=True, color='salmon', edgecolor='black')\n    axes[1, i].set_xlim(0, 6)\n    if i == 0: axes[1, i].set_ylabel(r\"Posterior of $|\\mu|$\")\n    axes[1, i].set_xlabel(r\"$|\\mu|$\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nPhase Transition in the Posterior Distribution. As \\(\\alpha\\) crosses the critical point \\(\\alpha=1\\), the posterior drastically flips from a regime where \\(\\mu\\) is free (large spread) to a regime where \\(a\\) is free (spread across [0,1]).\n\n\n\n\n\n\nReproducing Figures 9.1 and 9.2: Error Metric Phase Transition\nWatanabe shows that this geometric phase transition directly causes a kink in the generalization error and WAIC. The theoretical errors are: \\[ \\mathbb{E}[G_n - S] \\approx \\frac{\\min(\\alpha/2, 1/2)}{n} , \\quad \\mathbb{E}[\\text{WAIC} - S_n] \\approx \\frac{\\min(\\alpha/2, 1/2)}{n} \\] Since WAIC correctly estimates the generalization error, it will also exhibit a broken derivative at \\(\\alpha=1\\). The following code simulates this across multiple trials (analogous to Figures 9.1 and 9.2 of the book) to show the average criteria values as a function of the hyperparameter. Unfortunately we would need to work with a larger number of trials to reduce the variance and make the kink really visible.\n\n\nCode\nalphas = np.linspace(0.25, 2.5, 10)\nn_trials = 20\nn = 100\n\nmean_ge = []\nmean_waic = []\n\n# Using 20 trials with 10k MCMC steps smooths out the variance to reveal the kink.\n# Rendering this cell may take a minute or two, so it is cached.\nfor alpha in alphas:\n    alpha_ge = []\n    alpha_waic = []\n    \n    for trial in range(n_trials):\n        np.random.seed(42 + trial)\n        X = np.random.normal(0, 1, n)\n        X_test = np.random.normal(0, 1, 5000)\n        \n        # Empirical entropy of true distribution\n        p0_train = np.exp(-0.5 * X**2) / np.sqrt(2 * np.pi)\n        S_n = -np.mean(np.log(p0_train))\n        \n        # True entropy\n        p0_test = np.exp(-0.5 * X_test**2) / np.sqrt(2 * np.pi)\n        S = -np.mean(np.log(p0_test))\n        \n        a = 0.5; mu = 0.0\n        ll = log_likelihood(X, a, mu); lp = log_prior(a, mu, alpha)\n        \n        samples_a = []\n        samples_mu = []\n        \n        # MCMC Sampling\n        for i in range(10000):\n            a_new = a + np.random.normal(0, 0.1)\n            mu_new = mu + np.random.normal(0, 0.5)\n            if 0 &lt; a_new &lt; 1:\n                lp_new = log_prior(a_new, mu_new, alpha)\n                ll_new = log_likelihood(X, a_new, mu_new)\n                if np.log(np.random.rand()) &lt; (ll_new + lp_new - ll - lp):\n                    a, mu = a_new, mu_new\n                    ll, lp = ll_new, lp_new\n            if i &gt;= 4000 and i % 10 == 0:\n                samples_a.append(a)\n                samples_mu.append(mu)\n                \n        samples_a = np.array(samples_a)\n        samples_mu = np.array(samples_mu)\n        \n        # Vectorized WAIC Calculation\n        diff_sq_tr = (X[:, None] - samples_mu[None, :])**2\n        pmu_tr = np.exp(-0.5 * diff_sq_tr) / np.sqrt(2 * np.pi)\n        p_train = (1 - samples_a[None, :]) * p0_train[:, None] + samples_a[None, :] * pmu_tr + 1e-15\n        \n        p_train_mean = np.mean(p_train, axis=1)\n        T_n = -np.mean(np.log(p_train_mean))\n        V_n = np.mean(np.var(np.log(p_train), axis=1))\n        waic = T_n + V_n - S_n\n        \n        # Vectorized GE Calculation\n        diff_sq_ts = (X_test[:, None] - samples_mu[None, :])**2\n        pmu_ts = np.exp(-0.5 * diff_sq_ts) / np.sqrt(2 * np.pi)\n        p_test_all = (1 - samples_a[None, :]) * p0_test[:, None] + samples_a[None, :] * pmu_ts + 1e-15\n        \n        p_test_mean = np.mean(p_test_all, axis=1)\n        ge = -np.mean(np.log(p_test_mean)) - S\n        \n        alpha_ge.append(ge)\n        alpha_waic.append(waic)\n        \n    mean_ge.append(np.mean(alpha_ge))\n    mean_waic.append(np.mean(alpha_waic))\n\nplt.figure(figsize=(8, 5))\nplt.plot(alphas, mean_ge, 'o-', color='blue', label='Empirical GE - S')\nplt.plot(alphas, mean_waic, 's--', color='green', label='Empirical WAIC - Sn')\n\n# Theoretical curve: min(alpha/2, 1/2) / n\ntheoretical = np.minimum(alphas / 2, 0.5) / n\nplt.plot(alphas, theoretical, 'k-', linewidth=2, label=r'Theoretical $\\lambda/n$')\n\nplt.axvline(1.0, color='red', linestyle=':', label='Critical Point $\\\\alpha=1$')\nplt.title(\"Phase Transition in Errors (Analogous to Fig 9.1 & 9.2)\")\nplt.xlabel(r\"Dirichlet Hyperparameter $\\alpha$\")\nplt.ylabel(\"Error\")\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nPhase Transition in Average Errors. The Generalization Error and WAIC linearly increase until the critical point \\(\\alpha=1\\), after which they plateau as the model switches to the generic regular-equivalent manifold."
  },
  {
    "objectID": "neural_network_figure_2_7.html",
    "href": "neural_network_figure_2_7.html",
    "title": "Neural Networks and WAIC: Reproducing Figure 2.7",
    "section": "",
    "text": "This document explains and reproduces Figure 2.7 from Sumio Watanabe’s Mathematical Theory of Bayesian Statistics, which demonstrates the behavior of generalization errors and information criteria in Singular Statistical Models. We focus specifically on a two-layer neural network with varying numbers of hidden units."
  },
  {
    "objectID": "neural_network_figure_2_7.html#the-singular-nature-of-neural-networks",
    "href": "neural_network_figure_2_7.html#the-singular-nature-of-neural-networks",
    "title": "Neural Networks and WAIC: Reproducing Figure 2.7",
    "section": "The Singular Nature of Neural Networks",
    "text": "The Singular Nature of Neural Networks\nNeural networks are prime examples of singular statistical models. Consider a two-layer neural network with \\(H\\) hidden units mapping a 2D input \\(x \\in \\mathbb{R}^2\\) to an output probability \\(f(x;w_{out},w_{in}) \\in (0,1)\\) without bias terms, structured as:\n\\[\nf(x; w_{out}, w_{in}) = \\sigma\\left( \\sum_{h=1}^H w_{out, h} \\sigma(w_{in, h} \\cdot x) \\right)\n\\]\nwhere \\(w_{out} \\in \\mathbb{R}^H\\) are the output weights, \\(w_{in} \\in \\mathbb{R}^{H \\times 2}\\) are the input weights, and \\(\\sigma(t) = 1/(1 + e^{-t})\\) is the sigmoid activation function.\n\n\n\n\n\ngraph LR\n    %% Inputs\n    x1((x1)):::input_node\n    x2((x2)):::input_node\n    \n    %% Hidden units\n    h1((h1)):::hidden_node\n    h2((h2)):::hidden_node\n    hH((h_H)):::hidden_node\n    \n    %% Output\n    y((\"f(x)\")):::output_node\n    \n    %% Connections\n    x1 --&gt;|\"w_in\"| h1\n    x1 --&gt; h2\n    x1 -.-&gt; hH\n    \n    x2 --&gt; h1\n    x2 --&gt; h2\n    x2 -.-&gt; hH\n    \n    h1 --&gt;|\"w_out\"| y\n    h2 --&gt; y\n    hH -.-&gt; y\n    \n    %% Styling\n    classDef input_node fill:#e0f7fa,stroke:#006064,stroke-width:2px;\n    classDef hidden_node fill:#fff3e0,stroke:#e65100,stroke-width:2px;\n    classDef output_node fill:#e8f5e9,stroke:#1b5e20,stroke-width:2px;\n\n\n Architecture of the two-layer neural network mapping a 2D input to a single output through \\(H\\) hidden units. \n\n\n\nBecause these relationships use non-linear activations, true parameter values can be mapped to multiple distinct network configurations. For instance:\n\nNode Permutation Symmetry: Swapping two hidden nodes while swapping their inputs and outputs results in the same network mapping.\nNode Degeneracy: If the output weight to a hidden node is strictly zero, its input weights can take any value without affecting the output.\nActivation Symmetries: Properties of certain activation functions lead to flat parameter submanifolds.\n\nThese characteristics mean the mapping from model parameters to probability distributions (\\(w \\mapsto p(x|w)\\)) is not one-to-one, and the Fisher Information Matrix degenerates. In such singular regions, the Laplace approximation fails, and regular model theorems such as the fundamental AIC expansion (\\(G_n \\approx T_n + \\frac{d}{n}\\)) are radically invalidated."
  },
  {
    "objectID": "neural_network_figure_2_7.html#experimental-setup",
    "href": "neural_network_figure_2_7.html#experimental-setup",
    "title": "Neural Networks and WAIC: Reproducing Figure 2.7",
    "section": "Experimental Setup",
    "text": "Experimental Setup\nTo investigate this visually, we can replicate the experiment yielding Figure 2.7. The core idea is to establish a true data-generating process utilizing a Neural Network with 3 hidden units (\\(H = 3\\)), and then attempt to fit models of varying complexity (\\(H \\in \\{1, 2, 3, 4, 5\\}\\)).\nInstead of running MCMC dynamically (which takes considerable execution time for 20 trials), we present the annotated codebase and explain its structure in detail, along with the rendered figure.\n\nData Generation\nWe generate \\(n=200\\) points mapping a 2-dimensional space into a binomial classification (1D):\n# The true model is a neural net with 3 hidden units\nn = 200\nH_true = 3\nw_true = np.random.normal(0, 10, size=3*H_true)\n\nX = np.random.uniform(-2, 2, size=(n, 2))\nf_X = neural_net(X, w_true, H_true)\nY = np.random.binomial(1, f_X)\n\n\nMetrics Explored\nThe script executes 20 parallel trials, evaluating 4 vital measures for each hypothesis \\(H\\):\n\nGeneralization Error (GE) - The ground truth test error. Approximated by testing hypotheses on 10,000 hold-out samples over the Metropolis-Hastings posterior.\nAIC - The classic estimation using standard dimension theory: \\(AIC = T_n + \\frac{d}{n}\\) (where \\(d = 3H\\)).\nISCV (Leave-One-Out CV) - Importance Sampling Cross Validation. It consistently tracks Generalization.\nWAIC - Watanabe-Akaike Information Criterion evaluated via posterior predictive limits: \\(WAIC = T_n + V_n\\)."
  },
  {
    "objectID": "neural_network_figure_2_7.html#reproducing-the-figure",
    "href": "neural_network_figure_2_7.html#reproducing-the-figure",
    "title": "Neural Networks and WAIC: Reproducing Figure 2.7",
    "section": "Reproducing the Figure",
    "text": "Reproducing the Figure\nBelow is the output generated by the Python script reproduce_figure_2_7.py, aggregating these comparisons. Notice that the true generalization gap (\\(GE - S\\)) follows the same shape as WAIC:\n\n\n\n\n\n\nFigure 1: Figure 2.7: Comparison of GE, AIC, ISCV, and WAIC against the number of hidden units \\(H\\). Notice how AIC diverges, while WAIC closely shadows True Generalization.\n\n\n\n\nInterpretation of Results\nThe key takeaways from the resulting chart are:\n\nStandard AIC Fails Dramatically: Because the neural network is extremely singular, substituting the unadjusted parameter count \\(d/n\\) artificially penalizes complex models exponentially. AIC assumes all parameters contribute to the model’s actual power, severely demanding simpler networks when \\(H \\ge 3\\), and failing to acknowledge the dimensional reduction governed by the Real Log Canonical Threshold (\\(\\lambda\\)).\nWAIC Shadows True Generalization (GE): Irrespective of \\(H\\), WAIC gracefully evaluates the expected loss. It utilizes the model’s functional empirical variance \\(V_n\\) to uniquely recognize non-identifiable parameters without manual tuning.\nOverparameterization and Generalization: When the neural network is overparameterized (\\(H=4\\) or \\(H=5\\) vs \\(H_{true}=3\\)), it doesn’t hopelessly overfit. Due to singular model theory and Bayesian averaging, WAIC correctly identifies that these extra parameters do not damage out-of-sample performance, plateauing instead of continually diverging. This forms part of the mathematical proof for why modern deep neural networks often eschew tight pruning criteria without immediately overfitting data."
  },
  {
    "objectID": "neural_network_figure_2_7.html#full-python-implementation",
    "href": "neural_network_figure_2_7.html#full-python-implementation",
    "title": "Neural Networks and WAIC: Reproducing Figure 2.7",
    "section": "Full Python Implementation",
    "text": "Full Python Implementation\nThe complete sampling mechanism involves tracking Metropolis-Hastings trajectories over the singular manifold under a Gaussian Prior:\nimport numpy as np\nimport matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\nimport os\nimport time\nimport concurrent.futures\nimport pickle\n\n# Create a directory to save the generated figures\nos.makedirs(\"figures\", exist_ok=True)\n\ndef sigmoid(t):\n    \"\"\"Sigmoid activation function for the neural network.\"\"\"\n    # Clip values to prevent overflow in exp\n    return 1.0 / (1.0 + np.exp(np.clip(-t, -500, 500)))\n\ndef neural_net(x, w, H):\n    \"\"\"\n    Computes the output of a two-layer neural network with H hidden units.\n    Input x is 2D, hidden layer has H units, output is 1D.\n    Total parameters = H (output weights u) + 2*H (input weights W) = 3H.\n    \"\"\"\n    # Extract output weights (u) and input weights (W) from the flat weight vector w\n    u = w[:H]\n    W = w[H:].reshape(H, 2)\n    \n    # Forward pass: compute hidden layer activations then output\n    z = sigmoid(np.dot(x, W.T))\n    return sigmoid(np.dot(z, u))\n\ndef log_prior(w):\n    \"\"\"\n    Computes the log of the prior distribution.\n    The prior is a Gaussian with mean 0 and standard deviation 10 for all weights.\n    \"\"\"\n    return -0.5 * np.sum((w / 10.0)**2)\n\ndef metropolis(Y, X, H, iters=50000, burn_in=20000):\n    \"\"\"\n    Metropolis-Hastings MCMC algorithm to sample from the posterior distribution of the network weights.\n    \"\"\"\n    # Initialize weights randomly from a standard normal distribution\n    w = np.random.normal(0, 1, size=3*H)\n    \n    # Compute initial predictions and log-likelihood\n    f = neural_net(X, w, H)\n    f = np.clip(f, 1e-15, 1.0 - 1e-15) # Clip to avoid log(0)\n    ll = np.sum(Y * np.log(f) + (1 - Y) * np.log(1 - f))\n    lp = log_prior(w)\n    \n    samples = []\n    # Dynamic step size starting point, scaled by number of parameters\n    step_size = 0.1 / np.sqrt(3*H)\n    \n    # --- Burn-in phase with adaptive step size ---\n    for num_batch in range(int(burn_in / 100)):\n        acceptance = 0\n        for i in range(100):\n            # Propose new weights\n            w_new = w + np.random.normal(0, step_size, size=3*H)\n            f_new = neural_net(X, w_new, H)\n            f_new = np.clip(f_new, 1e-15, 1.0 - 1e-15)\n            \n            # Compute new log-likelihood and log-prior\n            ll_new = np.sum(Y * np.log(f_new) + (1 - Y) * np.log(1 - f_new))\n            lp_new = log_prior(w_new)\n            \n            # Acceptance probability calculation\n            if np.log(np.random.rand()) &lt; (ll_new + lp_new - ll - lp):\n                w = w_new\n                ll = ll_new\n                lp = lp_new\n                acceptance += 1\n                \n        # Tune step size based on acceptance rate to target 20-40% acceptance\n        acc_rate = acceptance / 100.0\n        if acc_rate &lt; 0.2:\n            step_size *= 0.9\n        elif acc_rate &gt; 0.4:\n            step_size *= 1.1\n\n    # --- Sampling phase ---\n    for i in range(iters):\n        w_new = w + np.random.normal(0, step_size, size=3*H)\n        f_new = neural_net(X, w_new, H)\n        f_new = np.clip(f_new, 1e-15, 1.0 - 1e-15)\n        \n        ll_new = np.sum(Y * np.log(f_new) + (1 - Y) * np.log(1 - f_new))\n        lp_new = log_prior(w_new)\n        \n        # Accept or reject\n        if np.log(np.random.rand()) &lt; (ll_new + lp_new - ll - lp):\n            w = w_new\n            ll = ll_new\n            lp = lp_new\n        \n        # Thinning: save every 10th sample to reduce autocorrelation\n        if i % 10 == 0:\n            samples.append(w.copy())\n            \n    return np.array(samples)\n\ndef run_trial(seed):\n    \"\"\"\n    Runs a single trial for an experiment comparing WAIC, LOO-CV (ISCV), AIC, \n    and Ground Truth Generalization (GE).\n    This function generates a dataset according to the true generative process and \n    fits models of varying complexity.\n    \"\"\"\n    trial_file = f'figures/trial_{seed}.pkl'\n    if os.path.exists(trial_file):\n        with open(trial_file, 'rb') as f:\n            results = pickle.load(f)\n        print(f\"Loaded cached Trial {seed} from {trial_file}.\", flush=True)\n        return results\n\n    t0 = time.time()\n    np.random.seed(seed)\n    \n    n = 200 # Training dataset size\n    H_true = 3 # The true synthetic model has 3 hidden units\n    # The true parameters: we use a fixed true network across all trials \n    # to see consistent generalization behavior (like in the book).\n    # Seed 100 with std=3.0 creates a robustly complex 3-unit network mapping.\n    rng_true = np.random.RandomState(100) \n    w_true = rng_true.normal(0, 3.0, size=3*H_true)\n    \n    # Generate training data inputs and outputs \n    X = np.random.uniform(-2, 2, size=(n, 2))\n    f_X = neural_net(X, w_true, H_true)\n    Y = np.random.binomial(1, f_X)\n    \n    # Generate large test set for evaluating True Generalization Error\n    N_test = 10000\n    X_test = np.random.uniform(-2, 2, size=(N_test, 2))\n    f_X_test = neural_net(X_test, w_true, H_true)\n    Y_test = np.random.binomial(1, f_X_test)\n    \n    # Calculate empirical entropy (S_n) on the training set\n    f_X_clipped = np.clip(f_X, 1e-15, 1.0 - 1e-15)\n    S_n = - np.mean(Y * np.log(f_X_clipped) + (1 - Y) * np.log(1 - f_X_clipped))\n    \n    # Calculate theoretical entropy (S) on the test set\n    f_X_test_clipped = np.clip(f_X_test, 1e-15, 1.0 - 1e-15)\n    S = - np.mean(f_X_test_clipped * np.log(f_X_test_clipped) + (1 - f_X_test_clipped) * np.log(1 - f_X_test_clipped))\n    \n    results = {}\n    \n    # We evaluate hypotheses models with H ranging from 1 to 5\n    for H in [1, 2, 3, 4, 5]:\n        # Perform MCMC sampling to obtain the posterior\n        samples = metropolis(Y, X, H, iters=50000, burn_in=20000)\n        \n        T = len(samples)\n        f_train_all = np.array([neural_net(X, w, H) for w in samples])\n        f_train_all = np.clip(f_train_all, 1e-15, 1.0 - 1e-15)\n        \n        # Calculate likelihoods for all training points over all MCMC samples\n        p_train = Y * f_train_all + (1 - Y) * (1 - f_train_all)\n        \n        # 1. Importance Sampling Cross-Validation (ISCV), estimating Leave-One-Out CV\n        inv_p_train_mean = np.mean(1.0 / p_train, axis=0) # Harmonic mean of likelihoods\n        ISCV = np.mean(np.log(inv_p_train_mean))\n        \n        # 2. Training Loss (T_n) from the predictive distribution\n        p_train_mean = np.mean(p_train, axis=0)\n        T_n = - np.mean(np.log(p_train_mean))\n        \n        # 3. Functional Variance (V_n) of the log likelihood, used for the WAIC penalty\n        log_p_train = np.log(p_train)\n        V_n = np.mean(np.var(log_p_train, axis=0))\n        \n        # WAIC is training loss plus the functional variance penalty\n        WAIC = T_n + V_n\n        \n        # 4. AIC using the theoretical dimension penalty (d / n)\n        d = 3 * H\n        AIC_b = T_n + d / n\n        \n        # 5. True Generalization Error (GE) on the test data evaluated using the predictive posterior\n        p_test_mean = np.zeros(N_test)\n        for w in samples:\n            f_test = neural_net(X_test, w, H)\n            f_test = np.clip(f_test, 1e-15, 1.0 - 1e-15)\n            p_test = Y_test * f_test + (1 - Y_test) * (1 - f_test)\n            p_test_mean += p_test\n        p_test_mean /= len(samples)\n        G = - np.mean(np.log(p_test_mean))\n        \n        # We store the errors normalized by subtracting the entropy \n        # (This aligns the baseline to 0 for the true distribution)\n        results[H] = {\n            'GE': G - S,\n            'AIC': AIC_b - S_n,\n            'ISCV': ISCV - S_n,\n            'WAIC': WAIC - S_n\n        }\n    \n    # Save the individual trial result to disk immediately\n    with open(trial_file, 'wb') as f:\n        pickle.dump(results, f)\n        \n    t1 = time.time()\n    print(f\"Trial {seed} finished in {t1-t0:.1f}s and saved to {trial_file}.\", flush=True)\n    return results\n\nif __name__ == '__main__':\n    # Watanabe's experiment typically uses 20 trials and 50000 iterations to build smooth boxplots\n    num_trials = 20\n    all_results = []\n    \n    print(f\"Evaluating {num_trials} trials (will run missing trials in parallel)...\", flush=True)\n    seeds_to_run = range(42, 42 + num_trials)\n    \n    # Execute trials in parallel utilizing ProcessPoolExecutor.\n    # The run_trial function handles both execution of new trials and loading of cached trials.\n    with concurrent.futures.ProcessPoolExecutor(max_workers=4) as executor:\n        for res in executor.map(run_trial, seeds_to_run):\n            all_results.append(res)\n            \n    # Save the combined results for convenience, though not strictly necessary anymore\n    results_file = 'figures/all_results.pkl'\n    with open(results_file, 'wb') as f:\n        pickle.dump(all_results, f)\n    print(f\"Saved combined trial results to {results_file}\")\n            \n    # --- Visualization to reproduce Figure 2.7 ---\n    fig, axes = plt.subplots(2, 2, figsize=(10, 8), sharex=True)\n    axes = axes.flatten()\n    \n    metrics = ['GE', 'AIC', 'ISCV', 'WAIC']\n    titles = ['Generalization (GE - S)', 'AIC (AICb - Sn)', 'ISCV (ISCV - Sn)', 'WAIC (WAIC - Sn)']\n    \n    # We tested Hidden Units from 1 to 5. The true model has H=3.\n    H_vals = [1, 2, 3, 4, 5]\n    \n    for i, metric in enumerate(metrics):\n        ax = axes[i]\n        # Aggregate data across all trials for the specific metric and complexity H\n        data = [[res[H][metric] for res in all_results] for H in H_vals]\n        # Boxplots highlight the variance and median across trials (similar to book figure)\n        ax.boxplot(data, labels=[1, 2, 3, 4, 5])\n        # Plot individual trajectories for each trial across H values (1 to 5)\n        # for res in all_results:\n        #     y_vals = [res[H][metric] for H in H_vals]\n        #     ax.plot(H_vals, y_vals, color='k', linewidth=1, alpha=0.8)\n            \n        ax.set_xlim(0.5, 5.5)\n        ax.set_xticks(H_vals)\n        ax.set_title(titles[i])\n        if i &gt;= 2:\n            ax.set_xlabel('Hidden Units')\n        ax.set_ylim(-0.02, 0.27)\n        ax.grid(True, linestyle='--', alpha=0.6)\n        \n    plt.tight_layout()\n    # Save the figure mimicking Figure 2.7 from Watanabe's work\n    plt.savefig('figures/Figure_2.7.png', dpi=150)\n    print(\"Saved to figures/Figure_2.7.png\")"
  },
  {
    "objectID": "regular_and_singular_models.html",
    "href": "regular_and_singular_models.html",
    "title": "Regular and Singular Models",
    "section": "",
    "text": "This document reproduces Figures 1.2 to 1.5 from Sumio Watanabe’s Mathematical Theory of Bayesian Statistics, demonstrating the fundamental differences between regular and singular statistical models."
  },
  {
    "objectID": "regular_and_singular_models.html#overview",
    "href": "regular_and_singular_models.html#overview",
    "title": "Regular and Singular Models",
    "section": "Overview",
    "text": "Overview\nIn Bayesian statistics, we observe how the posterior distribution behaves as the sample size \\(n\\) increases.\n\nRegular Models: Provide a one-to-one mapping from parameters to probability distributions with a strictly positive-definite Fisher information matrix. By the Bernstein-von Mises theorem, the posterior distribution asymptotically approaches a normal distribution, regardless of the specific sample realization.\nSingular Models: Feature unidentifiable parameters or a degenerate Fisher information matrix. The posterior distribution does not converge to a normal distribution, regardless of sample size, and its shape varies wildly depending on the specific realization of the data.\n\nWe generate multiple datasets for different sample sizes to visualize these properties."
  },
  {
    "objectID": "regular_and_singular_models.html#model-1-a-regular-statistical-model",
    "href": "regular_and_singular_models.html#model-1-a-regular-statistical-model",
    "title": "Regular and Singular Models",
    "section": "Model 1: A Regular Statistical Model",
    "text": "Model 1: A Regular Statistical Model\nIn our first model (Eq. 1.11), the data is drawn from a standard normal distribution \\(\\mathcal{N}(0, 1)\\). We model this with a normal distribution with an unknown mean \\(a\\) and standard deviation \\(\\sigma\\). \\[p(x \\mid a, \\sigma) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x - a)^2}{2\\sigma^2}\\right)\\] The true parameters are \\(a = 0\\) and \\(\\sigma = 1\\). The Fisher Information Matrix at the true parameters is positive definite, making this a regular model.\n\n\nCode\ndef generate_data_model1(n, num_datasets=12, seed=42):\n    np.random.seed(seed)\n    return np.random.normal(loc=0.0, scale=1.0, size=(num_datasets, n))\n\ndef posterior_model1(x, a_range, sigma_range):\n    A, Sigma = np.meshgrid(a_range, sigma_range)\n    x_reshaped = x.reshape(-1, 1, 1)\n    log_likelihood_all = -0.5 * np.log(2 * np.pi * Sigma**2) - 0.5 * ((x_reshaped - A) / Sigma)**2\n    log_likelihood = np.sum(log_likelihood_all, axis=0)\n    \n    log_posterior = log_likelihood - np.max(log_likelihood)\n    posterior = np.exp(log_posterior)\n    posterior /= np.sum(posterior)\n    return A, Sigma, posterior\n\ndef plot_model1(n, num_datasets=12):\n    data = generate_data_model1(n, num_datasets, seed=n)\n    a_vals = np.linspace(-1, 1, 100)\n    sigma_vals = np.linspace(0.01, 2.0, 100)\n    \n    fig, axes = plt.subplots(3, 4, figsize=(10, 7), sharex=True, sharey=True)\n    fig.subplots_adjust(wspace=0.1, hspace=0.1)\n    \n    for i in range(num_datasets):\n        ax = axes[i // 4, i % 4]\n        A, Sigma, post = posterior_model1(data[i], a_vals, sigma_vals)\n        ax.imshow(post, origin='lower', extent=[-1, 1, 0.01, 2.0], aspect='auto', cmap='gray_r')\n        ax.plot(0, 1, 'ws', markersize=6, markeredgecolor='k')\n        \n        ax.set_xticks([-1, 0, 1])\n        ax.set_yticks([0, 1, 2])\n        if i // 4 == 2:\n            ax.set_xlabel('a')\n        if i % 4 == 0:\n            ax.set_ylabel(r'$\\sigma$')\n            \n    plt.show()\n\n\n\nReproducing Figure 1.2 (\\(n = 10\\))\nHere we visualize the posterior distribution for 12 different generated datasets, each with \\(n=10\\) observations. The true parameter value \\((a=0, \\sigma=1)\\) is indicated by the white square. With a small sample size, the posteriors are broad and slightly irregular, heavily influenced by the specific data samples.\n\n\nCode\nplot_model1(10)\n\n\n\n\n\n\n\n\n\n\n\nReproducing Figure 1.3 (\\(n = 50\\))\nWhen we increase the sample size to \\(n=50\\), two things happen: 1. The posterior probability mass tightly concentrates around the true parameter \\((0, 1)\\). 2. The shape of the posterior takes on a consistent, isotropic ‘blob’ (a Gaussian profile), independent of the precise sampling variations. This is a hallmark of regular models.\n\n\nCode\nplot_model1(50)"
  },
  {
    "objectID": "regular_and_singular_models.html#sec:singular-model",
    "href": "regular_and_singular_models.html#sec:singular-model",
    "title": "Regular and Singular Models",
    "section": "Model 2: A Singular Statistical Model",
    "text": "Model 2: A Singular Statistical Model\nOur second model (Eq. 1.12) fits a two-component Gaussian mixture to data. The data is generated as a 50/50 mix of \\(\\mathcal{N}(0, 1)\\) and \\(\\mathcal{N}(0.3, 1)\\). Our model tries to learn the mixture proportions and the mean of the second component: \\[p(x \\mid a, b) = (1 - a) \\mathcal{N}(x \\mid 0, 1) + a \\mathcal{N}(x \\mid b, 1)\\]\nThe true distribution represents \\(a = 0.5\\) and \\(b = 0.3\\). This model is singular. The mapping from parameters to distribution is not identifiable or one-to-one, and the Fisher information matrix rank decreases at these singularities. Consequently, standard statistical theory (like AIC or the Laplace approximation) fails because the posterior is not normally distributed.\n\n\nCode\ndef generate_data_model2(n, num_datasets=12, seed=42):\n    np.random.seed(seed)\n    data = []\n    for _ in range(num_datasets):\n        components = np.random.choice([0, 1], size=n, p=[0.5, 0.5])\n        means = np.where(components == 0, 0.0, 0.3)\n        sample = np.random.normal(loc=means, scale=1.0)\n        data.append(sample)\n    return np.array(data)\n\ndef posterior_model2(x, a_range, b_range):\n    A, B = np.meshgrid(a_range, b_range)\n    x_reshaped = x.reshape(-1, 1, 1)\n    p1 = np.exp(-0.5 * x_reshaped**2) / np.sqrt(2 * np.pi)\n    p2 = np.exp(-0.5 * (x_reshaped - B)**2) / np.sqrt(2 * np.pi)\n    px = (1 - A) * p1 + A * p2\n    log_likelihood = np.sum(np.log(px + 1e-15), axis=0)\n        \n    log_posterior = log_likelihood - np.max(log_likelihood)\n    posterior = np.exp(log_posterior)\n    posterior /= np.sum(posterior)\n    return A, B, posterior\n\ndef plot_model2(n, num_datasets=12):\n    data = generate_data_model2(n, num_datasets, seed=n)\n    a_vals = np.linspace(0.01, 0.99, 100)\n    b_vals = np.linspace(0.01, 0.99, 100)\n    \n    fig, axes = plt.subplots(3, 4, figsize=(10, 7), sharex=True, sharey=True)\n    fig.subplots_adjust(wspace=0.1, hspace=0.1)\n    \n    for i in range(num_datasets):\n        ax = axes[i // 4, i % 4]\n        A, B, post = posterior_model2(data[i], a_vals, b_vals)\n        ax.imshow(post, origin='lower', extent=[0.01, 0.99, 0.01, 0.99], aspect='auto', cmap='gray_r')\n        ax.plot(0.5, 0.3, 'ws', markersize=6, markeredgecolor='k')\n        \n        ax.set_xticks([0, 0.5, 1])\n        ax.set_yticks([0, 0.5, 1])\n        if i // 4 == 2:\n            ax.set_xlabel('a')\n        if i % 4 == 0:\n            ax.set_ylabel('b')\n            \n    plt.show()\n\n\n\nReproducing Figure 1.4 (\\(n = 100\\))\nAlthough \\(n=100\\) is a relatively large sample size, the posterior surfaces (for 12 independent samples) display chaotic geometries. Rather than single circular peaks, the distributions exhibit long, extended ridges and multiple local maxima.\n\n\nCode\nplot_model2(100)\n\n\n\n\n\n\n\n\n\n\n\nReproducing Figure 1.5 (\\(n = 1000\\))\nEven at \\(n=1000\\) (a 10-fold increase), the posterior fails to converge to a neatly shaped Gaussian point cloud. The shape of the distribution remains idiosyncratic, fundamentally dependent on the exact sample drawn. The unidentifiability limits parameter collapse in specific directions—demonstrated by the ‘stretched’ variance seen uniquely across the twelve trials. This highlights the inherent flaw in trying to approximate singular posterior distributions with single-point estimates (e.g. maximum likelihood) or simple quadratics (like the Laplace approximation).\n\n\nCode\nplot_model2(1000)\n\n\n\n\n\n\n\n\n\nWe will use this example to illustrate the concepts of singular learning theory in SLT Gaussian Mixture."
  }
]